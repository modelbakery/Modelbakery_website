---
date: "2021-10-26T00:00:00Z"
external_link: ""
image:
  caption: Photo by rawpixel on Unsplash
  focal_point: Smart
links:
- icon: github 
  icon_pack: fab
  name: github material
  url: https://github.com/modelbakery/Modelbakery_website/tree/main/content/blog/internal-project
summary: Simple trick to segment your customers with only three customer information recency, frequency and monetary.
tags:
title: Retention Measure & Customer Segementation
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""
---

By nature of product or service sales businesses, there exists a sense of urgency to secure as many leads to generate more revenue with the notion that, with more of them, many data for conversion of sales such as levels of customers experience, desire, product categories, and the power of the leading score can be backtracked. 

In a given time and resource the businesses battle seeking for the what is called few "critical" lever to generate more impact and hence the ROAS (Return on advertisement spend) or optimizing investment of effort. 

A common pitfall for Marketers would be overlooking retention and focusing on acquisition. 

> Customer retention is the essential—but often overlooked—foundation for growth in subscription-based services. Though you'd probably agree that the concept of customer retention is important, you're probably not taking the time or effort to calculate it right and try to improve it.
-- By PROFITWELL


The revenue in its simplest form can be expressed as follow:

$$
Revenue\ = \# Customer\ * Returning\ Rates\ * Average\ Price\ Value
$$



***
## Menu 

### Appetizer
* What is RFM and why we do feature engineering 
* Why Campaign Response important feature for CRM?
* Pareto Principle 
* RFM Quantitative Scoring

### Main
* Data Preparation 
* Campaign Response Rate
* Performing RFM Segmentation
  + RFM Construction
  + RFM Analysis
  + RFM Visualisation

### Desert 
* Wrap up
  + Summary on Perspective Notes 
  + RFM Limitations
  + Future Development 
* Ready to receive feedback  

***


## 1. What is RFM and why we do feature engineering 


### 1.1 RFM


Past history, Marketers realised that targeting the right audience is just as important as what you send. That siad, the "right" customer will always vary depending on business objectives and business cycle. Having reliable KPI's for customer value and engagement score will sincerely accelerate business decisions. RFM sets a trick here; due to its high adaptability in various industries, it serves as a good starting point for customer segmentation journey. It is well known statistical model to develop a sense of company's customer base: __customer purchasing behaviour!__

>The main advantage of RFM analysis is that it provides meaningful information about customers while using fewer (three-dimensional) criterions as cluster attributes. This reduces the complexity of customer value analysis model __without compromising its accuracy__.

RFM is essentially a combined measure of customer's purchasing characteristics; consists three data points: recency, frequency and monetary. 

+ Recency  : __Length__ of a time period since the __last__ purchase 
+ Frequency: __Number__ of purchase within a customer life-span
+ Monetary : __Amount__ of money spent within customer transcations

All of these measures have been proven to be effective predictors of a customer's willingness to engage in marketing messages and construct their transcation values to the company. 


### 1.2 Benefit of RFM Analysis 

In-depth KYC (know your customer) on your customer base and framework for iterative campaigns to promising customer (targets) will undoubtfully benefit your organisation in many folds:

* Personalisation: By creating effective customer segments, highly relevant and cutomerised offer in personal level can be achieved. 

* Improve Conversion Rates: Personalised marketing will yield higher retention and conversion rate by aid of effective up-sell, cross-sell strategy. 

* ROI: Breaking customers into small buckets effectively reduce business's missing opportunity (low Risk) and enables objective-oriented target marketing (High return)


### 1.3 Feature Engineering 

Feature engineering is a process of applying domain knowledge to select and transform raw data into new definable features (characteristics, properties, attributes) that better represent the underlying problem to solve or predictive model. 

Depending on how you apply RFM analysis it is also a feature engineering as we are combining customer transcation historical data: latest purchase, how often, in amounts, to new __"Transcations History"__ feature. The important question is how well does new feature speaks of its own definition in business. In broad sense, this RFM feature has many usage for segmentation but also input feature in many predictive models (churn, campaign response, likelihood of future purchase). Simply put, in customer analytics, purchasing behaviour is not only dimension of company's interest. More so, in its best form, I believe the matrics should be flexible to include on various KPI's and modelling. We will discuss more later in this post.  

***

## 2.0 Why Campaign Response Tracking System Important?

Tracking responses is one of the important aspects of the campaign life cycle for a marketing analyst. This involves process of collecting data that identifies customers who have responded to a communication from a campaign program via specific channels. Any company would like to automate customer engagement and its campaign performance measure. 


Tracking campaign activities are vital measure for customer engagement management. Effective CRM system can leverage life time value of company's customers by effectively reducing their churn events. Data scientist and engineers are very well adapted in this field. Every businesses needs to optimise the balance between the cost retention and acquisition as they are essential track for ROI generation. 


The data I will be exploring below contains customer campaign response binary dataset. 

***

## 3.0 Pareto Principle 

> __For many events, roughly 80% of the effects coe from 20% of the causes.__
> Pareto Principle - The 80/20 Rule 

Similarly, 20% customers contribute to 80% of company's total revenue. The ideology is the people who spent once are more likely to spend again. Likewise, the people for have responded to campaign are likely to respond again. Pareto Principle is at the heart of RFM model; focusing target audience on critical segments of customers is likely to generate higher ROI. 


## 4.0 RFM Quantitative Scoring 

Many system rank search results by means of some form of scoring: each candidate item is given a score measuring in some sense how closely it fits with the __query__ and its items are ranked in descending score order. If indeed such a system ranks customers on the basis of transcation behaviour effect on revenue (better option: profitability), organisation will gain quick and actionable decisions to collect their targets by its priority order within the segments. This quantitative RFM scoring approach has multiple benefits:

* Gain clear picture on each RFM unique combination values for the company and how to strategically framework future customer journey analysis. 

* The ability distribute RFM combination metrics in size upon their revenue contribution. 

* Hierachical construction of RFM metrics by its importance. 

* The quantitative RFM scoring (Numeric: importance) and RFM metrics (Categorical: behaviour type) will bring more flexibility as part of input feature for further customer analytic prediction models.

In any form of scoring, the importance of its robustness will always be a challenge. For a obvious reasons, conversions within these scores or the segments without the internal explaintary, would not be a reliable system to conduct. 

Having this in mind, we will explore the transcation data in depth and construct RFM metrics and its scoring from a scratch on R. Thank you for reading this far and lets dig in!



## 1.0 library load

```{r, echo = FALSE, message=FALSE, warning=FALSE}

library(readr)
library(fs) 
library(tufte)
library(forecast)
library(GGally)
library(knitr)
library(kableExtra)

```


```{r, echo = TRUE, message=FALSE, warning=FALSE}

library(tidyverse)
library(dplyr)
library(lubridate)
library(skimr)
library(scales)
library(psych)
library(gmodels)

library(tidyquant)
library(timetk)
library(rsample) 
library(wesanderson)
library(formattable)
```


```{r, message=FALSE, warning=FALSE}
set.seed(42)
options(scipen = 100, digits = 3)

trans_df_raw <- read_csv("~/Dropbox/business_insight/df_trans_raw.csv", col_types = cols())

```


```{r, message=FALSE, warning=FALSE}
resp_df_raw <- read_csv("~/Dropbox/business_insight/df_resp_raw.csv")
resp_df_raw$X1 <- NULL
```



## 2.0 Transcational data 

```{r, warning=FALSE}
trans_df_raw %>% head()
 
resp_df_raw %>% head()

```


```{r}
trans_df <- trans_df_raw %>% 
  mutate(trans_date = lubridate::dmy(trans_date)) %>% 
  dplyr::rename(
    date   = trans_date,
    amount = tran_amount) 
```

```{r}
resp <- resp_df_raw %>% 
  select(customer_id, response) %>% 
  setNames(c("customer_id", "campaign"))

cust_df <- trans_df %>% 
  left_join(resp, by = "customer_id") %>% 
  mutate(campaign = case_when(campaign == 1 ~ 1,
                                TRUE ~ 0))
```



```{r}
cust_df %>% 
  mutate(year = year(date)) %>% 
  group_by(year) %>% 
  summarise(revenue = sum(amount)) %>% 
  mutate(revenue_text = scales::dollar(revenue, scale = 1e-6, suffix = "M")) %>% 
  ggplot(aes(year, revenue)) +
  geom_col(fill = "#2c3e50") +
  geom_text(aes(label = revenue_text), vjust = 1.5, colour = "white") + 
  theme_tq()
```


```{r}

camp_facet_rev_monthly_tbl <- cust_df %>% 
  mutate(monthly = floor_date(date, "month") -1) %>% 
  group_by(monthly, campaign) %>% 
  summarise(
    monthly_pur = n(),
    revenue = sum(amount)) %>%  ungroup() %>% 
  filter(!monthly == min(monthly) & !monthly == max(monthly)) %>% 
  arrange(campaign) %>% 
  mutate(campaign = ifelse(campaign == 0, "no_response", "response"))

p_camp_facet_rev_monthly <- camp_facet_rev_monthly_tbl %>% 
  ggplot(aes(monthly, revenue, colour = campaign)) +
  geom_point(aes(colour = campaign)) +
  geom_line() +
  # geom_smooth(method = 'loess', span = 0.2, colour = "black", alpha = 0.5) +
  facet_wrap(~campaign, scales = "free_y", nrow = 2) +
  scale_colour_tq() +
  theme_tq() +
  scale_y_continuous(labels = scales::dollar_format(scale = 1e-3, suffix = "K")) +
  labs(
    title = "Campaign Revenue Inspection Time Plot",
    subtitle = "Campaign Revenue Growth Activity Decline is Alarming!",
    x = "Time (in month)",
    y = "Revenue ($K)",
    colour = "Campaign",
    caption = "4 years revenue trends from customer campaign data base
              Estimated campaign start date : 2013/02
              Estimated campaign influene end date : 2014/07"
  ) +
  theme(legend.position = "none")

p_camp_facet_rev_monthly
```

* Has the marketing (advertising) strategy applied to right customers? 
  + Loyal or persuaders 
  + How can we define "Target" customer to drive revenue 
  + How is the acquisition rate? Business future potential 
  + The business health LTV/CVC
  + What is our current value offered to our existing customers - are we losing customer rentention and/or acqusition rate by the competitiors. 
  + Are we investing enough marketing 



## 3.0 Structuring Business Problem Framework 

#### 3.A 

Define Objectives: Campagin Target Customer 

Strategy: TBD 

* Acquisition Strategy

* 
Retailers tend to use advanced methods for calculating CAC because customer lifespans tend to be longer and the average spend tends to be shorter. If you imagine the number of times you'll pop into Target during your life (no matter where the location) and how much you'll spend there, you'll see how figuring out this amount of money can be crazy difficult.

Assess Target Customer: 

* Cohort Analsysis

* Customer acqusition cost (CAC)

* Customer life-time value (with and with-out marketing expansion)

* CAC:LTV ratio: internal business health metric



## 4.0 Cohort Analysis 

```{r}

set_cohort_period <- function(data, period = "month"){
  cust_df %>% 
    distinct(customer_id, date) %>% 
    group_by(customer_id) %>% 
    mutate(
      first_pur = min(date),
      last_pur  = max(date), 
      duration  = as.numeric(last_pur - first_pur),
      cohort    = floor_date(first_pur, period),
      cohort    = as.yearmon(cohort, '%Y-%m')) %>% 
    ungroup() 
}

monthly_cohort_retention_tbl <- cust_df %>% 
  set_cohort_period(period = "month") %>% 
  mutate(cohort_date = lubridate::my(cohort),
         cohort = as.character(cohort) %>% as_factor() %>% fct_reorder(cohort_date)) %>% 
  group_by(cohort) %>% 
  mutate(
     days_next_pur = as.numeric(difftime(date, first_pur, units = c("days"))),
     month_next_pur = floor(days_next_pur/30)
     ) %>% ungroup() %>% 
  group_by(cohort, month_next_pur) %>% 
  summarise(user = n()) %>% 
  group_by(cohort) %>% 
  mutate(first_no_user = first(user),
         cohort_retention = user/first_no_user) %>% 
  select(-first_no_user) %>% ungroup()


first_purchase_user_tbl <- monthly_cohort_retention_tbl %>% 
  filter(month_next_pur == 0) %>% 
  select(cohort, user) 


monthly_cohort_retention_metrics <- monthly_cohort_retention_tbl %>% 
  reshape2::dcast(cohort~month_next_pur,
                  value.var = "cohort_retention") %>% 
  left_join(
    first_purchase_user_tbl,
    by = "cohort") %>% 
  select(-"0") %>% 
  select(cohort, user, everything()) 

monthly_cohort_retention_metrics


monthly_cohort_retention_tbl %>% 
  filter(!month_next_pur == 0) %>% 
  group_by(cohort) %>% 
  summarise(avg_retention = mean(user),
            avg_retention_rate = mean(cohort_retention))
 
```


```{r}
breaks_cohort  <- quantile(monthly_cohort_retention_metrics[,3:46], probs = seq(.05, .95, .05), na.rm = TRUE)

monthly_cohort_retention_tbl %>% 
  mutate(pct = cohort_retention*100) %>% 
  ggplot(aes(month_next_pur, reorder(cohort, desc(cohort)), fill = cohort_retention)) +
  geom_raster() +
  scale_fill_continuous(type = "gradient", low = "deepskyblue", high = "darkblue") +
  scale_x_continuous(breaks = breaks_cohort, expand = c(0,0)) +
  geom_text(aes(label = scales::percent(formattable(pct), suffix = "%", prefix = " ")), size = 2, col = "white") +
  theme_tq()
```

Analysis framework for understanding customer loyalty: customer retention rate and churn rate 

The time frame I want to study: Annual, Quarterly, monthly 

Information aquired: 

* The number of exisitng customers at the start of the time period (S)

* The number of total customers at the end of the time period (E)

* The number of new customers added within the time period (N)

[(E-N)/S] * 100 = CRR (Customer retention rate)


Revenue churn: rate of losing MRR over customer experience in time. 

* Does our customer downgrades rather than churn? 

* Repeat purchase rate: Number of return customers/ Total number of customers 
  + Applies to companies without fixed contracts, such as retailers. 
  
Customer lifetime value: the profit generated the average customer contributes to a business over their entire lifecycle. 

* Cost of acquiring and retaining customers. (Average order value * Repeat purchase rate) - customer aquistioon cost 



```{r}
cust_monthly_tbl <- cust_df %>% 
  group_by(customer_id) %>% 
  summarise(monthly_date = ceiling_date(date, "month")-1,
         first_pur = min(monthly_date),
         last_pur = max(monthly_date),
         duration = as.numeric(last_pur - first_pur),
         rev = sum(amount),
         frequency = n(),
         camp_resp = campaign) %>% ungroup() 

cohort_avg_trans_tbl <- cust_monthly_tbl %>% 
  group_by(first_pur) %>% 
  summarise(users = n(),
            avg_duration = mean(duration),
            avg_rev = mean(rev),
            avg_freq = mean(frequency),
            avg_item_price_pur = avg_rev/avg_freq,
            avg_camp_resp = mean(camp_resp))  %>% 
  ungroup() 


cohort_avg_trans_tbl %>% 
  ggplot(aes(first_pur, avg_duration)) +
  geom_col() +
  geom_line(cohort_avg_trans_tbl, mapping = aes(first_pur, avg_rev), size = 1) +
  theme_tq()

cohort_avg_trans_tbl
  
month_based_cohort_tbl
quantile(month_based_cohort_tbl$duration)
quantile(month_based_cohort_tbl$duration)
```

```{r}
cust_monthly_tbl %>% 
  select(customer_id, last_pur) %>%
  arrange(last_pur) %>% distinct()
  
  


cust_holdout_period_tbl<- cust_monthly_tbl %>% 
  select(customer_id:last_pur) %>% 
  group_by(customer_id) %>% 
  mutate(days_since_first_pur = as.numeric(monthly_date - first_pur)) %>% 
  arrange(customer_id, desc(monthly_date)) %>% 
  mutate(lag_days_since_first_pur= lag(days_since_first_pur),
         holdout_period = lag_days_since_first_pur - days_since_first_pur) %>% 
  drop_na()


cust_holdout_period_tbl %>% 
  group_by(first_pur, customer_id) %>% 
  summarise(frequency = n(),
            avg_holdout_period = mean(holdout_period))
  
  
```



```{r}
q_cohort_monthly_retention_tbl <- cust_df %>% 
  set_cohort_period(period = "quarter") %>%
  group_by(cohort) %>% 
  mutate(
     days_next_pur = as.numeric(difftime(date, first_pur, units = c("days"))),
     month_next_pur = floor(days_next_pur/30)
     ) %>% ungroup() %>% 
  group_by(cohort, month_next_pur) %>% 
  summarise(users = n(),
            ) %>% ungroup() 

q_cohort_monthly_retention_tbl %>% 
  ggplot(aes(month_next_pur, users, fill = cohort)) +
  geom_area(colour = "white") +
  scale_fill_brewer("Blues") +
  theme_tq()

```


```{r}

# Monthly Cohort Table 
monthly_cohort_tbl <- cust_df %>% 
  # remove duplicated: same day purchase data -> can be a typo 
  distinct(customer_id, date) %>% 
  group_by(customer_id) %>% 
  mutate(first_pur = min(date),
         last_pur  = max(date), 
         duration  = as.numeric(last_pur - first_pur),
         cohort    = floor_date(first_pur, "month")) %>% 
  group_by(cohort) %>% 
  mutate(date_id = row_number(),
         cohort  = as.yearmon(cohort, '%Y-%m') %>% format(., "%m/%Y") %>% 
           as_factor() %>% fct_reorder(date_id)) %>% 
  ungroup() %>% 
  select(-date_id)

# LTV cohort Table
cohort_MAU_tbl <- monthly_cohort_tbl %>% 
  group_by(cohort) %>% 
  mutate(
     days_next_pur = as.numeric(difftime(date, first_pur, units = c("days"))),
     months_next_pur = floor(days_next_pur/30)
  ) %>% ungroup() 


MAU_retention_matrix <- monthly_cohort_tbl %>% group_by(cohort) %>% 
  summarise(users = n()) %>% 
  select(cohort, users) %>% 
  left_join(reshape2::dcast(cohort_MAU_tbl, cohort~months_next_pur,
                                 value.var="customer_id",
                                 fun.aggregate = length), "cohort")

MAU_revenue_tbl <- cohort_MAU_tbl %>% 
  left_join(
    cust_df %>% select(customer_id, amount),
    by = "customer_id"
    ) %>% 
  group_by(cohort, months_next_pur) %>% 
  summarise(revenue = sum(amount)) %>% ungroup() 

MAU_revenue_matrice <- MAU_revenue_tbl %>% 
  reshape2::dcast(cohort~months_next_pur,
                  value.var= "revenue") %>% 
  mutate(across(everything(), ~replace_na(.x, 0)))
```


```{r}
# Retention Rate Trend Analysis 
MAU_retention_prep <- cohort_MAU_tbl %>% 
  select(cohort, months_next_pur, customer_id) %>% 
  group_by(cohort, months_next_pur) %>% 
  summarise(active_user = n()) %>% ungroup() %>% 
  group_by(cohort) %>% 
  mutate(user = sum(active_user),
         retention_pct = active_user/user,
         cumsum_act_user = cumsum(active_user),
         retained = user-cumsum_act_user,
         fraction_retained = retained/user) %>% ungroup()

MAU_revenue_tbl %>% 
  group_by(cohort) %>% 
  mutate(total_revenue = cumsum(revenue)) %>% ungroup() %>% 
  ggplot(aes(months_next_pur, revenue, fill = cohort)) +
  geom_area() 
  

```

```{r}

  

MAU_retention_prep %>% 
  ggplot(aes(months_next_pur, fraction_retained, colour = cohort)) +
  geom_line(size = 1) +
  theme_tq() 


MAU_churn_prep <- cohort_MAU_tbl %>% 
  select(cohort, months_next_pur, customer_id) %>% 
  group_by(cohort, months_next_pur) %>% 
  summarise(active_user = n()) %>% 
  group_by(cohort) %>% 
  mutate(users = sum(active_user),
         lag_active_user = lag(active_user, default = NA_integer_),
         churn = (active_user-lag_active_user)/lag_active_user,
         churn = replace_na(churn, 0)) %>% ungroup() 

MAU_churn_prep %>% 
  select(cohort, months_next_pur, churn) %>% 
  reshape2::dcast(cohort~months_next_pur,
                  value.var="churn",
                  fun.aggregate = NULL) %>% 
  mutate(across(everything(), ~replace_na(.x, 0))) %>% 
  left_join(
        cohort_MAU_tbl %>% group_by(cohort) %>% 
        summarise(users = n()) %>% 
        select(cohort, users), 
      by = "cohort") %>% 
  select(cohort, users, everything())

MAU_churn_prep %>% 
  ggplot(aes(months_next_pur, churn)) +
  geom_line() +
  facet_wrap(~cohort)
```


```{r}
rev_active_user = rev(active_user),
         inactive_user = rev_active_user/user,
         churn = case_when(inactive_user != 0 ~ 1-inactive_user,
                           TRUE~ inactive_user))
MAU_retention_matrix

for (i in sort(3:ncol(MAU_retention_matrix))){
  MAU_retention_matrix[,i] <- round(MAU_retention_matrix[,i]/MAU_retention_matrix[,2],4)
}

# Verify: the Users proportions in the same cohort must equal to 1 (Total Users)
sum(rowSums(MAU_retention_matrix[1,3:49]))

MAU_retention_matrix %>% View()

cw_churn <- MAU_retention_matrix
  
for (i in rev(3:ncol(cw_churn))){
  cw_churn[,i] <- round(cw_churn[,i]/cw_churn[,2],6)
  cw_churn[,i] <- case_when(cw_churn[,i] !=0 ~ 1-cw_churn[,i],
                            TRUE ~ cw_churn[,i])
}

round(cw_churn[,41]/cw_churn[,2], 6) != 0
```


```{r}
cust_df %>% 
  group_by(customer_id) %>% 
  summarise(campaign = median(campaign)) %>% ungroup() %>% 
  group_by(campaign) %>% 
  summarise(n = n()) %>% ungroup() %>% 
  mutate(pct = n/sum(n))

```

Findings: 

* 9.3% campagin responding rate 
  + What are the potential benchmark for its percentage success rate? 
  + How can campaign ROI be measured? 
  + How are these customers and what they value to the company?
  
Ultimately, you will need to choose an email marketing budget that works for your company and goals. That being said, email marketing is an extremely cost effective strategy, and it can earn you an incredibly high ROI — $44 for every $1 you spend.So if you’re looking for a way to reach more customers for less money, email marketing is a great option.




Area to investigate to estimate the campaign ROI (valuation of the advertisiment project):

[1] Investigate Object: 9.3% campaign responders

[2] Investigate Objectives: 

* Duration of the campaign activity 

* Revenue growth generated by campaign responders 

[3] Hypothesise benchmark: non-campaign responders ROI 

[4] Collect information on customer data base: limited 





```{r}
camp_facet_rev_monthly_tbl %>% 
  ggplot(aes(monthly, revenue, fill = campaign)) +
  geom_col(position = "fill") +
  scale_fill_tq() +
  theme_tq() +
  labs(
    title = "Campaign Revenue Inspection Time Plot",
    subtitle = "Campaign Revenue Growth Activity Decline is Alarming!",
    x = "Time (in month)",
    y = "Revenue Index",
    colour = "Campaign"
  ) +
  theme(legend.position = "none")

```

```{r, echo=FALSE, warning=FALSE}
generator_hist_facet <- function(data, bins = 20, ncol = 5,
                                 fct_reorder = FALSE, 
                                 fct_rev = FALSE,
                                 fill = palette_light()[[3]],
                                 color = "white", scale = "free") {
      data_factored <- data %>% 
        gather(key = key, value = value, factor_key = TRUE) 
      
    if(fct_reorder) {
      data_factored <- data_factored %>% 
        # Trick to get variables in alphabetical order 
        mutate(key = as.character(key) %>% as.factor())
    }
      
    if(fct_rev) {
      data_factored <- data_factored %>% 
        mutate(key = fct_rev(key))
    }
      
    g <- data_factored %>% 
      ggplot(aes(x = value, group = key)) +
      geom_histogram(bins = bins, fill = fill, color = color) +
      facet_wrap(~key, ncol = ncol, scale = scale) +
      theme_tq()
    
    return(g)
}
```

```{r}

generator_rfm <- function(data){
  analy_day <- max(data$date) + 1

  data %>% 
  dplyr::group_by(customer_id) %>% 
  dplyr::summarise(recency_days = as.numeric(analy_day - max(date)),
                   frequency = n(), 
                   monetary = sum(amount)) %>%  ungroup()
}


rfm_pre_df <- cust_df %>% 
  mutate(year = year(date)) %>% 
  filter(year <= 2013) %>% 
  select(!year) %>% 
  generator_rfm()  

rfm_df <- rfm_pre_df %>% 
  mutate(log_recency_days = log(recency_days)) %>% 
  select(-recency_days)

rfm_df %>% 
  select(-customer_id) %>% 
  generator_hist_facet(bins = 40, ncol = 3, fct_rev = FALSE) + theme_ben()


```


```{r}

rfm_df

rfm_mapper_tbl <- tibble(rfm_matric = c("log_recency_days", "frequency", "monetary"))

rfm_mapper_tbl %>% 
  mutate(rfm_matric_df = rfm_matric %>% map(score_mapper()))

quantile(rfm_df$log_recency_days, probs = seq(0,1,0.2))

score_mapper <- function(rfm_matric){
  
    rfm_level <- quantile(rfm_df$rfm_matric, probs = seq(0,1,0.2))
    
    rfm_df %>% 
      mutate(score = case_when(.$rfm_matric >= rfm_level[1] ~ 1,
                               .$rfm_matric >= rfm_level[2] ~ 2,
                               .$rfm_matric >= rfm_level[3] ~ 3,
                               .$rfm_matric >= rfm_level[4] ~ 4,
                               TRUE ~ 5))
}

"log_recency_days" %>% map(score_mapper)


generator_rfm_metric <- function(data) {
  
  analy_day <- max(data$date) + 1
  
  step_1 <- data %>% 
        generator_rfm() 
  
  R_level <- quantile(step_1$recency_days,   probs = result)
  F_level <- quantile(step_1$frequency, probs = result)
  M_level <- quantile(step_1$monetary,  probs = result)
  
  out <- step_1 %>%
    mutate(R_score = case_when(.$recency_days >= R_level[1] ~ 1,
                               .$recency_days >= R_level[2] ~ 2,
                               .$recency_days >= R_level[3] ~ 3,
                               .$recency_days >= R_level[4] ~ 4,
                               TRUE ~ 5),
           F_score = case_when(.$frequency >= F_level[1] ~ 5,
                               .$frequency >= F_level[2] ~ 4,
                               .$frequency >= F_level[3] ~ 3,
                               .$frequency >= F_level[4] ~ 2,
                               TRUE ~ 1),
           M_score = case_when(.$monetary >= M_level[1] ~ 5,
                               .$monetary >= M_level[2] ~ 4,
                               .$monetary >= M_level[3] ~ 3,
                               .$monetary >= M_level[4] ~ 2,
                               TRUE ~ 1))
  return(out)
}


generator_rev_per_metric <- function(pre_score_data, ...) {
  
  pre_score_data %>% 
    dplyr::group_by(...) %>% 
    dplyr::summarise(cust = n_distinct(customer_id),
                     total_monetary = sum(monetary)) %>% ungroup() %>% 
    
    dplyr::mutate(rev_per_cus = total_monetary/cust,
                  prop_cus = cust/sum(cust),
                  prop_rev = total_monetary/sum(total_monetary),
                  rev_effect = prop_rev/prop_cus)
}


```

#### 3.B Develope Adertising KPIs

Average retail industry KPIs: 12.7% email open rate & 6.8% click-to-open rate (2013)

Cost per email campaign action (list of factors): 

* Self managed or agency supported: unknown 

* Frequency of campaign: unknown 

* Platform used: unknown 

* quality of current email list: TBD 

For the analysis purpose I will consider 12.7% email as a conservative KPI that indicates successful conversion rate. Also, email marketing benchmark from __WebFX__ website will be accessed in order to calculate cost per action (highly subjective but the purpose is to access target amongst the customer data base).


Findings:

* Clear revenue growth (leveling effect) observed by the marketing campagin activity. 
  + 


```{r}
calculate_advertising_cost <- function(
  
  n = 1,
  
  # Cost per email campaign action
  # direct cost 
  cost_per_volume = 0.008,
  verification_cost = 0.003,
  
  frequency_of_campaign = 
)
```




## 4.0 Measure the Campaign Response Rate

The ability to retain engagement customers is a major concern for many businesses. However, their retention is not the only dimension of our interest; the revenue stream associated with these active engagement or persuadable customer is another factor influencing customer profitability. 

What is a good response rate? 

Typically in the field of ecommerce, in average, a good response rate are within 10~13% boundary. 

The average email benchmark by retail industry: 12.6% (open rate), 8.5% (click-to-open rate)

```{r}
format_dollar  <- function(x, ...) {
  paste0("$", formatC(as.numeric(x), format="f", digits = 0, big.mark=","))
}

se <- function(x) sqrt(var(x)/length(x))

```


The average campaign response rate is 9.4%; with the assumption that the company is either retail or ecommerce business this value sits on a good hand. This value could also mean the percentage of persuaders.  

Questions of ask:
* Campaign responders contribute higher value to the company than the non-responders?
* Is there a revenue stream by engaging customers? 
  + When has the promotion began? 
  + What is the average expiration rate of campaign effect?

```{r}
cust_df %>% 
  group_by(customer_id) %>% 
  summarise(campaign = median(campaign)) %>% ungroup() %>% 
  count(campaign) %>% 
  mutate(pct_response = n/n_distinct(cust_df$customer_id)*100)
```


### 4.1 Quick t-test and visualisation 

```{r}

campaign_tbl <- cust_df %>% 
  group_by(customer_id) %>% 
  summarise(monetary = sum(amount),
            frequency = n(),
            campaign = median(campaign) %>% as.factor()) %>% ungroup()

t.test(monetary~campaign, data = campaign_tbl)

```


```{r}
P4_C_dup <- campaign_tbl %>% ggplot(aes(x = campaign, y = monetary, fill = campaign)) +
  geom_col(alpha = 0.5) +
  scale_y_continuous(labels = scales::dollar_format()) +
  scale_fill_tq() +
  theme_tq()

P4_B_dup <- campaign_tbl %>% ggplot(aes(x = campaign, y = monetary, fill = campaign)) +
  geom_boxplot(alpha = 0.5) +
  scale_y_continuous(labels = scales::dollar_format()) +
  scale_fill_tq() +
  theme_tq()

P4_V_dup <- campaign_tbl %>% ggplot(aes(x = campaign, y = monetary, fill = campaign)) +
  geom_violin(alpha = 0.5) +
  scale_y_continuous(labels = scales::dollar_format()) +
  scale_fill_tq() +
  theme_tq()

P4_D_dup <- campaign_tbl %>% 
  ggplot(aes(x = monetary, fill = campaign)) +
  geom_density(alpha = 0.5) +
  scale_fill_tq() +
  theme_tq()

gridExtra::grid.arrange(P4_C_dup, P4_B_dup, P4_V_dup, P4_D_dup, ncol = 2)
```

### 4.2 Perspective Notes 

* The monetary value distribution of campaign respond customer is symmetric with low variance; hence desirable to work with.

* Time series benchmark comparison between campaign respond customer and non-responded customer (benchmark) is needed to assign two things:
  + Trend analysis: this variance in mean value (between the group) has fostered by campaign activity.  
  + If TRUE, where in phase are they with the revenue upstream? -> Different engagement strategy needed?
  + If NO, can irregular term (Unexpected Revenue Fluctuation) in STL decomposition analysis explain such fortune?
  
* Consult the expert on how to measure these revenue contribution and the KPI's for their worth before iterative engagement strategy. 
  + Current churn rate?
  + Their progressive impact on MRR or ARR?


***
## 5.0 Performing RFM Segmentation: RFM Construction 

I like to note that with aid of package development and software. There are RFM segmentation and k-means built in library available which are quick, sophisticated and with accurate results. That said, building any models or simple matric from scratch is great source and opportunity for anyone's learning curve - right?. More importantly, I believe it will deliver the ability to understand descriptive statistics in a more practical way. Some of the codes here may not be so easy to interpret as most of them are built from scratch and by me. I will really appreciate any feedback or mistakes I have made. 


Aim: 
* Feature RFM score with acceptable density distribution (0 ~ 100 score in range): Robust Scoring 

Key Task: 

* Handling the skewness of the data set 
* Weight control for individual RFM components -> consult the expert
* Conduct code workflow for practice. 


### 5.1 Quick EDA 

Often, in most of the quantitative analysis and toward predictions, having too many zero values can effect on the transformation and its distribution. For this reason, I have added 1 day to all customers which takes account of customer has purchased the day we compute this metric; named it as recency days. This is great trick in case we would like to do log transform later on the analysis. 

recency = date of analysis - latest purchase date 

recency days =  tomorrow date - latest purchase date
 
```{r}
generator_rfm <- function(data){
  analy_day <- max(data$date) + 1

  data %>% 
  dplyr::group_by(customer_id) %>% 
  dplyr::summarise(recency_days = as.numeric(analy_day - max(date)),
                   frequency = n(), 
                   monetary = sum(amount)) %>%  ungroup()
}

cust_df %>% 
  generator_rfm() %>% select(-customer_id) %>% 
  psych::describe()

```


```{r}
cust_df %>% 
  generator_rfm() %>% head(10) %>% 
  mutate(monetary = sum(monetary) %>% format_dollar()) %>% 
  set_names(c("Customer ID", "Recency Days", "Frequency", "Monetary")) %>%
  kbl() %>% 
  kable_material_dark()

```


```{r, echo = FALSE}

generator_hist_facet <- function(data, bins = 20, ncol = 5,
                                 fct_reorder = FALSE, 
                                 fct_rev = FALSE,
                                 fill = palette_light()[[3]],
                                 color = "white", scale = "free") {
      data_factored <- data %>% 
        gather(key = key, value = value, factor_key = TRUE) 
      
    if(fct_reorder) {
      data_factored <- data_factored %>% 
        # Trick to get variables in alphabetical order 
        mutate(key = as.character(key) %>% as.factor())
    }
      
    if(fct_rev) {
      data_factored <- data_factored %>% 
        mutate(key = fct_rev(key))
    }
      
    g <- data_factored %>% 
      ggplot(aes(x = value, group = key)) +
      geom_histogram(bins = bins, fill = fill, color = color) +
      facet_wrap(~key, ncol = ncol, scale = scale) +
      theme_tq()
    
    return(g)
}

theme_ben <- function(base_size = 14) {theme_bw(base_size = base_size) %+replace%
    theme(
      # L'ensemble de la figure
      plot.title = element_text(size = rel(1), face = "bold", margin = margin(0,0,5,0), hjust = 0),
      # Zone où se situe le graphique
      panel.grid.minor = element_blank(),
      panel.border = element_blank(),
      # Les axes
      axis.title = element_text(size = rel(0.85), face = "bold"),
      axis.text = element_text(size = rel(0.70), face = "bold"),
      axis.line = element_line(color = "black", arrow = arrow(length = unit(0.3, "lines"), type = "closed")),
      # La légende
      legend.title = element_text(size = rel(0.85), face = "bold"),
      legend.text = element_text(size = rel(0.70), face = "bold"),
      legend.key = element_rect(fill = "transparent", colour = NA),
      legend.key.size = unit(1.5, "lines"),
      legend.background = element_rect(fill = "transparent", colour = NA),
      # Les étiquettes dans le cas d'un facetting
      strip.background = element_rect(fill = "#17252D", color = "#17252D"),
      strip.text = element_text(size = rel(0.85), face = "bold", color = "white", margin = margin(5,0,5,0))
    )
}

```

### 5.2 RFM distribution and skewness 

```{r}

skimr::partition(skim(cust_df %>% 
  generator_rfm()))$numeric %>% data.frame %>%
   kbl() %>% 
   kable_material_dark()

cust_df %>% 
  generator_rfm() %>% 
  select(monetary, frequency:recency_days) %>% 
  generator_hist_facet(bins = 40, ncol = 3, fct_rev = FALSE) + theme_ben()
  
```

_generator_hist_facet() and theme_ben() are customised facet histogram function. It is uploaded on my GitHub. Anyone who are interested in customerised visualisation, Business Science 201 Course organised by __Matt Dancho__ _is one of the the option to look at. He regularly uploads free tutorials on Youtube as well_

The recency days dataset is highly right skewed. There are many transformations to handle such skewed distribution, e.g. log transformation for instance. However, inhere, I will like to demonstrate small trick using quantiles binning to effectively split range of customers by organisational profit. - Yes Pareto Principle embeds here. 

Quantile ranges are one of descriptive statistics describing the skewness of the data. The range between Q3-Q2 is larger in magnitude than the Q2-Q1 point in range. The concept of the quantiles (any family of quantiles) are the values located in the dataset in a way that it can split the data uniformly. 


Small example of quantile:

```{r}
rfm_example <- cust_df %>% 
    generator_rfm()

R_quantile <- quantile(rfm_example$recency_days, probs = seq(0,1,0.25))
F_quantile <- quantile(rfm_example$monetary, probs = seq(0,1,0.25))
M_quantile <- quantile(rfm_example$frequency, probs = seq(0,1,0.25))

cbind(R_quantile, F_quantile, M_quantile)
```

This quantile measure would most likely to preserve the actual customer behaviour distribution; thus the robustness of its score. However, due to the wide range between Q3 (858 recency days) and Q2 (112 recency days), the scope of valuable customers are squeezed making it difficult quantify their differences in their Recency score. 

Hence we will do an opposite approach squeezing the Q3 and Q2 and exposing the range at low recency region. This can be done by relocating the quantiles in data set. 


```{r}
result <- c() 
j <- 0
for(i in 1:5){
  j = j + i
  result[i] = 1 -(j/(1+2+3+4+5))
  print(result)
}


R_result <- quantile(rfm_example$recency_days, probs = result)
F_result <- quantile(rfm_example$frequency, probs = result)
M_result <- quantile(rfm_example$monetary, probs = result)

cbind(R_result, F_result, M_result)

```

This is no longer a quantiles; instead of 100% (tip of the iceberg) there is a 93.3%. This new splitting point wraps customers > {recency days over 221} into a single, low scored R group. 

There is one other small problem. The direction of scoreboard for these matrics are opposite: between the R and F,M, which I will take care of in the code below.  


```{r}

generator_rfm_metric <- function(data) {
  
  analy_day <- max(data$date) + 1
  
  step_1 <- data %>% 
        generator_rfm() 
  
  R_level <- quantile(step_1$recency_days,   probs = result)
  F_level <- quantile(step_1$frequency, probs = result)
  M_level <- quantile(step_1$monetary,  probs = result)
  
  out <- step_1 %>%
    mutate(R_score = case_when(.$recency_days >= R_level[1] ~ 1,
                               .$recency_days >= R_level[2] ~ 2,
                               .$recency_days >= R_level[3] ~ 3,
                               .$recency_days >= R_level[4] ~ 4,
                               TRUE ~ 5),
           F_score = case_when(.$frequency >= F_level[1] ~ 5,
                               .$frequency >= F_level[2] ~ 4,
                               .$frequency >= F_level[3] ~ 3,
                               .$frequency >= F_level[4] ~ 2,
                               TRUE ~ 1),
           M_score = case_when(.$monetary >= M_level[1] ~ 5,
                               .$monetary >= M_level[2] ~ 4,
                               .$monetary >= M_level[3] ~ 3,
                               .$monetary >= M_level[4] ~ 2,
                               TRUE ~ 1))
  return(out)
}


generator_rev_per_metric <- function(pre_score_data, ...) {
  
  pre_score_data %>% 
    dplyr::group_by(...) %>% 
    dplyr::summarise(cust = n_distinct(customer_id),
                     total_monetary = sum(monetary)) %>% ungroup() %>% 
    
    dplyr::mutate(rev_per_cus = total_monetary/cust,
                  prop_cus = cust/sum(cust),
                  prop_rev = total_monetary/sum(total_monetary),
                  rev_effect = prop_rev/prop_cus)
}

```


### 5.3 RFM components Weight control 

The next question that arises is: is it fair to average out the individual recency, frequency, and monetary scores for each customers and assign them to RFM score, as per their purchase or engagement behaviour?
Depending on the nature of the businesses, the relative importance for individual matrics can be adjusted to arrive at their final score. 

For example, for retail business selling customer goods. A customer who searches and purchases product in monthly bases will have higher recency and frequency score than monetary score. Hence the weight can be calculated by giving more importance to the recency and frequency scores. 

This is the area the data analyst would need to contact this domain of an expert. In prior to jumping into conclusions without any certainty. 


Educated Approach: 

* Calculate the revenue contribution for each score in individual R, F, M components. 
  + revenue contribution/ no. customer  = Revenue contributed per customer
  + {Rev(group)/Rev(total)} / {No. of cust(group)/No. of cust(total)} = Revenue contribution by group
  + The sum of revenue contribution by group (score) is revenue effect by its component
  + R(Rev effect) + F(Rev effect) + M(Rev effect) = Total Revenue effect 
  + Weight{R(Rev effect)} + Weight{F(Rev effect)} + Weight{M(Rev effect)} = Weight{Total Revenue effect} = 1


```{r}

R_score_table <- cust_df %>% 
  generator_rfm_metric() %>% 
  generator_rev_per_metric(R_score); R_score_table %>% kbl() %>% kable_material_dark()

F_score_table <- cust_df %>% 
  generator_rfm_metric() %>% 
  generator_rev_per_metric(F_score); F_score_table %>% kbl() %>% kable_material_dark() 

M_score_table <- cust_df %>% 
  generator_rfm_metric() %>% 
  generator_rev_per_metric(M_score); M_score_table %>% kbl() %>% kable_material_dark()
  
```




```{r}

rfm_effect <- sapply(list(R_score_table$rev_effect, F_score_table$rev_effect, M_score_table$rev_effect), function(x) sum(x))

sum_effect <- sum(rfm_effect)
weight <- c(rfm_effect[1]/sum_effect, rfm_effect[2]/sum_effect, rfm_effect[3]/sum_effect)

RFM_function <- function(x, y, z, w){
  RFM_Score <- x*w[1] + y*w[2] + z*w[3]
  return(RFM_Score)
}

min_max_normalization <- function(x) {
  return((x-min(x))/(max(x)-min(x)))
}

generator_RFM_score <- function(pre_score_data) {
  pre_score_data %>% 
  mutate(RFM_Score_norm = RFM_function(.$R_score,
                                       .$F_score,
                                       .$M_score,
                                       w = weight),
         RFM_Score_norm = min_max_normalization(RFM_Score_norm),
         RFM_Score      = (RFM_Score_norm)*100,
         RFM_Score_gr   = ifelse(RFM_Score >= 90, 10,
                               ifelse(RFM_Score >= 80, 9,
                                      ifelse(RFM_Score >= 70, 8,
                                             ifelse(RFM_Score >= 60, 7,
                                                    ifelse(RFM_Score >= 50, 6,
                                                           ifelse(RFM_Score >= 40, 5,
                                                                  ifelse(RFM_Score >= 30, 4,
                                                                         ifelse(RFM_Score >= 20, 3,
                                                                                ifelse(RFM_Score >= 10, 2,
                                                                                       1)))))))))) 
}
  

```


## 6.0 Performing RFM Segmentation: RFM Analysis 



```{r}
rfm_score_df <- cust_df %>% 
  generator_rfm_metric() %>% 
  generator_RFM_score() %>% 
  left_join(resp, by = "customer_id") %>% 
  mutate(campaign = case_when(campaign == 1 ~ 1,
                                TRUE ~ 0) %>% as.factor()) %>% 
  mutate(RFM_Score_gr = RFM_Score_gr %>% as.factor) %>%
  unite("RFM", R_score:M_score, sep = ":", remove = FALSE)

```

```{r}
rfm_score_df %>% 
  select(R_score:M_score) %>% 
  generator_hist_facet(bins = 5, ncol = 1, fct_rev = FALSE) + theme_ben()

rfm_score_df %>% 
  select(RFM_Score, RFM_Score_gr) %>% 
  mutate(RFM_Score_gr = as.numeric(RFM_Score_gr)) %>% 
  generator_hist_facet(bins = 10, ncol = 1, fct_rev = FALSE) + theme_ben()

```


```{r}
summary_rfm_score_table <- rfm_score_df %>% 
  select(RFM_Score_gr, recency_days:monetary, campaign, RFM_Score) %>% 
  dplyr::group_by(RFM_Score_gr) %>% 
  dplyr::summarise(no_cust           = n(),
                   campaign          = sum(as.numeric(as.character(campaign))),
                   avg_RFM_Score     = mean(RFM_Score),
                   avg_freq          = mean(frequency),
                   avg_sales         = mean(monetary),
                   avg_recency_days  = mean(recency_days),
                   tot_sales         = sum(monetary),
                   rev_per_cust      = tot_sales/no_cust) %>% ungroup() %>% 
  arrange(desc(avg_RFM_Score)) %>% 
  mutate(RFM_Score_gr = RFM_Score_gr %>% fct_reorder(desc(avg_RFM_Score))) %>% 
  select(avg_RFM_Score, everything()) 
  

summary_rfm_score_table %>% 
  mutate(tot_sales = format_dollar(tot_sales),
         rev_per_cust = format_dollar(rev_per_cust)) %>% 
  setNames(c("Avg. RFM Score", "RFM", "No. Customer", "No. Campaign","Avg. Frequency", "Avg. Monetary", "Avg. Recency", "Total Revenue", "Revenue Per Customer")) %>% kbl() %>% kable_material_dark() 
  
```


```{r}

ggplot(rfm_score_df, aes(x = RFM_Score)) +
  geom_density(aes(x = RFM_Score, y = ..density..), fill = palette_light()[1], alpha = 0.7) +
  stat_function(fun=dnorm, 
                args = list(mean=mean(rfm_score_df$RFM_Score),
                              sd=sd(rfm_score_df$RFM_Score)),
                color="red", lwd = 1) +
  geom_density(color="black", lwd =1.5) +
  theme_tq()

```

## 7.0 Performing RFM Segmentation: RFM Visualisation  

```{r}

generator_plot_rfm <- function(data, var1, var2, cmpg, point_size =0.7,
                               colour = wesanderson::wes_palette(name="Royal1", 2)){
  
  var1 <- enquo(var1)
  var2 <- enquo(var2)
  var1_name<- quo_name(var1)
  var2_name<- quo_name(var2)
  
  data %>% 
    ggplot(aes(x = {{var1}}, y = {{var2}}, color = {{cmpg}})) + 
    geom_point(position = "jitter", size = point_size) +
    theme_bw() + 
    scale_color_manual(values=colour) +
    scale_y_continuous(label = scales::dollar) +
    ggtitle(paste0("Customer Purchasing Behaviour Plot: ", str_to_upper(var1_name), " and ", str_to_upper(var2_name))) +
  labs(
     x        = var1_name %>% str_to_upper(),
     y        = var2_name %>% str_to_upper(),
     color    = "Campaign Response",
     caption  = "Any prior knowledge to this cause from +2 years of historical data"
  ) +
  theme_tq()

}

generator_plot_rfm(rfm_score_df, RFM_Score, monetary, campaign)
generator_plot_rfm(rfm_score_df, recency_days, monetary, campaign)
p100 <-  generator_plot_rfm(rfm_score_df, frequency, monetary, campaign)

save.image(p100,)
```


```{r}

g_plot <- ggplot(data = rfm_score_df, aes(x = frequency, y = monetary, color = RFM_Score_gr)) + 
  geom_point(position = "stack", size = 3) +
  theme_bw()+ 
    labs(
     title    = "Customer Purchasing Behaviour Plot: Frequency and Monetary",
     subtitle = "Uniform Stack Distribution of RFM Score under F and M dimension",
     x        = "Frequency",
     y        = "Monetary",
     color    = "RFM Score"
  ) + 
  theme_tq()
g_plot


h_plot <- ggplot(data = rfm_score_df, aes(x = frequency, y = monetary, color = RFM_Score_gr)) +
  geom_point(position = "jitter", size = 0.7) +
  theme_bw() +  
  labs(
     title    = "Customer Purchasing Behaviour Plot: Frequency and Monetary",
     subtitle = "Two Distinctive Customer Groups visually identified",
     x        = "Frequency",
     y        = "Monetary",
     color    = "RFM Score"
  ) +
  theme_tq()
h_plot

f_plot <- ggplot(data = rfm_score_df, aes(x = log(monetary/recency_days), y = frequency, color = RFM_Score_gr)) +
  geom_point(position = "jitter", size = 1.5) +
  theme_bw()+   
  labs(
     title    = "Customer Purchasing Behaviour Plot: log(M/R) and Frequency",
     x        = "log(M/R)",
     y        = "Frequency",
     color    = "RFM Score"
  ) +
  theme_tq()
f_plot

```



## 8.0 Performing RFM Segmentation: Business Use

Having individual R, F, and M scores and these unqiuw combination, customers can be clustered in groups based on their purchase behaviour. On top of that, the RFM score we have generated assigns the profitable priorities of these groups. This is bring an advantage for quick and actionable decision makings in comparison to having just the purchasing behaviour analysis. 

Clustering these groups exceeds my goal in this post hence I would not explore more than illustrating some of the classical examples below. 


At Risk

* Customers with high revenue contribution but risk of churn. 

* R_score of 1 means +221 days (8 months) of last purchase -> More likely to be lost customers

* 27.7% of customers in "At Risk" group had previous campaign subscription: Very High (Urgent Action needed)  

```{r}
risk_cust <- rfm_score_df %>%
  select(-c(recency_days:monetary, RFM_Score_norm)) %>% 
  filter(RFM_Score >= 60 & R_score <= 2 & R_score > 1) %>% 
  arrange(desc(RFM_Score)); risk_cust %>% head(10) %>% kbl() %>% kable_material_dark()

prop.table(table(risk_cust$campaign))[2]*100
```

Up-sell

* Customers with frequent purchase history but does not spend much 

* Personalised Up-sell Recommendations needed 

* 20% of customers in "Up-sell" group had previous campaign subscription: High

```{r}
upsell_cust <- rfm_score_df %>%
  select(-c(recency_days:monetary, RFM_Score_norm)) %>% 
  filter(F_score >= 4 & M_score <= 2) %>% 
  arrange(desc(RFM_Score)); upsell_cust %>% head(10) %>% kbl() %>% kable_material_dark()

prop.table(table(upsell_cust$campaign))[2]*100
```

Cross-Sell 

* Customers with good spending amount but less frequent

* Good R_score needed for cross-selling strategy: recommending products related to their recent purchase. 

* 9.7% of customers in "Cross-Sell" group had previous campaign subscription: Average

```{r}
crosssell_cust <- rfm_score_df %>%
  select(-c(recency_days:monetary, RFM_Score_norm)) %>% 
  filter(M_score >= 3 & F_score <= 2 & R_score >= 4) %>% 
  arrange(desc(RFM_Score)); crosssell_cust %>% head(10) %>% kbl() %>% kable_material_dark()

prop.table(table(crosssell_cust$campaign))[2]*100
```




## Wrap up

### Summary on Perspective note

* 
* Quantiles: Stick to Quantile rules! for better distribution 

### Limitations of RFM analysis

Using RFM modeling can provide valuable insights about customers. But it does not take into account many other factors about the customer.

In-depth targeted marketing may also use type of item purchased or customer campaign responses as factors. Customer demographics such as age, sex and ethnicity are not covered in RFM analysis either.

Additionally, RFM only uses historical data about customers and may not predict future customer activity. Predictive methods may be able to identify future customer behavior that RFM analysis cannot.

### Future development 

* Average time span on expirary of campaign 

* Cohort Analysis 

* Time Series: How these RFM Score varies with quartery 


## Ready to receive feedback 




