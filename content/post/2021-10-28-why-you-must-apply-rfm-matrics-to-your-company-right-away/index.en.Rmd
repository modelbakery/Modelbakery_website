---
title: K-Nearest Neighbour Diabetes Prediction 
author: Package Build
date: '2021-10-26'
slug: K-Nearest Neighbour Diabetes Prediction
categories:
  - R
tags:
  - Classification 
  - Health Care
subtitle: ''
summary: ''
authors: []
lastmod: '2021-10-26T03:10:43+09:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
draft: FALSE
---







Contents:

Study 

* What is K-Nearest Neighbour function 
  + What are K-NN assumptions?
  + Parameters
  + Variety of distance criteria 
  + Optimal Number of Neighbors 

* Cons 
  + Homogeneous feature 
  + Curse of Dimensionality 
  + Imbalanced data problems 
  + Outlier Sensitivity 
  
* Advanced topics 
  + Kd tree algorithm


## 


$$
\hat{C}_k(x) =  \underset{g}{\mathrm{argmax}} \ \ \hat{p}_{kg}(x)
$$


## Variable Explaination 

[1] npreg : number of pregnancies.

[2] glu : plasma glucose concentration in an oral glucose tolerance test.

[3] bp : diastolic blood pressure (mm Hg).

[4] skin : triceps skin fold thickness (mm).

[5] bmi : body mass index (weight in kg/(height in m)\^2). 

[6] ped : diabetes pedigree function. 

[7] age : age in years

[8] type : Yes or No, for diabetic according to WHO criteria.


## Feature Engineering IDeas or 

Biological Age: f(BMI) 

```{r}
setwd("/Users/seunghyunsung/Desktop/rdata/ML_basic/SVM_RF/diabetes_prediction")

suppressMessages(library(class))
suppressMessages(library(kknn))
suppressMessages(library(randomForest))
suppressMessages(library(e1071))
suppressMessages(library(caret))
suppressMessages(library(reshape2))
suppressMessages(library(ggplot2))
suppressMessages(library(kernlab))
suppressMessages(library(MASS))
suppressMessages(library(skimr))

library(ggplot2)
library(GGally)
```


```{r}

# conditions (options & set.seed)
options(digits = 3, scipen = 100)

set.seed(42)
# load the data
data(Pima.tr)
data(Pima.te)

dim(Pima.tr) # [1] 200   8
dim(Pima.te) # [1] 332   8

```

### Combine Train and Teat Data Set 

```{r}

# Train dataset 
Pima.tr$type <- ifelse(Pima.tr$type == "No", 0, 1)
prop.table(table(Pima.tr$type))

# Test dataset 
Pima.te$type <- ifelse(Pima.te$type == "No", 0, 1)
prop.table(table(Pima.te$type))

# Combine  
Pima = rbind(Pima.tr, Pima.te)

```

## EDA 

K-NN classifier algorithm 

To do:

* Feature Scaling: K-NN and many ML algorithm relies on the distance (norm) of the matrix. 

* Outlier detection: K-NN algorithm is highly sensitive to outliers 

```{r}
partition(skim(Pima))
```




```{r}
ggpairs(Pima[ ,c(1:8)], aes(color = type %>% as.factor(), alpha =0.75), lower = list(continuous = "smooth")) + theme_bw() + 
  labs(title = "Diabetes") +
  theme(plot.title = element_text(face ='bold', color ='black', hjust=0.5, size =12)) 
```
```{r}
Pima_melt <- reshape2::melt(Pima, id.var ="type")

ggplot2::ggplot(data = Pima_melt,
                aes(x = type, y = value, group = type)) +
  geom_boxplot() +
  facet_wrap(~variable, ncol=2) 
```

### Eucliden Distance Calculation 

```{r}
euclideanDist <- function(a, b){
  d = 0
  for(i in c(1:(length(a)-1) ))
  {
    d = d + (a[[i]]-b[[i]])^2
  }
  d = sqrt(d)
  return(d)
}
```



### KNN Prediction Function

```{r}
knn_predict <- function(test_data, train_data, k_value){
  pred <- c()  #empty pred vector 
  #LOOP-1
  for(i in c(1:nrow(test_data))){   #looping over each record of test data
    eu_dist =c()          #eu_dist & eu_char empty  vector
    eu_char = c()
    good = 0              #good & bad variable initialization with 0 value
    bad = 0
    
    #LOOP-2-looping over train data 
    for(j in c(1:nrow(train_data))){
 
      #adding euclidean distance b/w test data point and train data to eu_dist vector
      eu_dist <- c(eu_dist, euclideanDist(test_data[i,], train_data[j,]))
 
      #adding class variable of training data in eu_char
      eu_char <- c(eu_char, as.character(train_data[j,][[6]]))
    }
    
    eu <- data.frame(eu_char, eu_dist) #eu dataframe created with eu_char & eu_dist columns
 
    eu <- eu[order(eu$eu_dist),]       #sorting eu dataframe to gettop K neighbors
    eu <- eu[1:k_value,]               #eu dataframe with top K neighbors
 
    #Loop 3: loops over eu and counts classes of neibhors.
    for(k in c(1:nrow(eu))){
      if(as.character(eu[k,"eu_char"]) == "g"){
        good = good + 1
      }
      else
        bad = bad + 1
    }
 
    # Compares the no. of neighbors with class label good or bad
    if(good > bad){          #if majority of neighbors are good then put "g" in pred vector
 
      pred <- c(pred, "g")
    }
    else if(good < bad){
                   #if majority of neighbors are bad then put "b" in pred vector
      pred <- c(pred, "b")
    }
    
  }
  return(pred) #return pred vector
}

```



### Accuracy Calculation 

```{r}

accuracy <- function(test_data){
  correct = 0
  for(i in c(1:nrow(test_data))){
    if(test_data[i,6] == test_data[i,7]){ 
      correct = correct+1
    }
  }
  accu = correct/nrow(test_data) * 100  
  return(accu)
}
```


```{r, echo = FALSE}
# calling knn_predict()
# K = 5
# predictions <- knn_predict(test.df, train.df, K) 
 
#Adding predictions in test data as 7th column
# test.df[,7] <- predictions 
# print(accuracy(test.df))
```







