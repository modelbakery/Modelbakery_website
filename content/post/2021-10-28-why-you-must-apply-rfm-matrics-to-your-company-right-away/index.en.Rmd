---
title: Why you Must apply RFM matrics to your company right away!
author: Package Build
date: '2021-10-26'
slug: why-you-must-apply-rfm-matrics-to-your-company-right-away
categories:
  - R
tags:
  - Marketing
subtitle: ''
summary: ''
authors: []
lastmod: '2021-10-26T03:10:43+09:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
draft: FALSE
---


You may have heard of the Pareto Principle, but knowing how would you utilize its advantages, in reality, can seriously save your time. By nature of sales businesses, in general, have an urgency to secure as many leads as quickly as possible due to the market competition. Therefore, depending on the number of leads, levels of customers' demographics,  desire, and product categories, the power of the leading score can ever so be significant. In this post, I like to introduce a simple statistical RFM metrics and further discuss the essence of scoring. There are many excellent books and course materials with technical details on statistical inference, but knowing when and how to apply it in the real world is another muscle we all need to train ourselves.

***
## Menu 

### Appetizer
+ Motivation: Segmentation is "Divide and Conquer"
+ What is RFM and why we do feature engineering 
+ Why Campaign Response important feature for CRM?
+ Power of Quantile and Pareto Principle 
+ Robust Scoring Rules 

### Main
+ Time Series Visualisation + seasonal effect
+ Data splitting 
+ RFM Scoring 
+ Visualisation for business report 

### Desert 
+ Perspective note 
+ To take action
+ Wrap up: What I have learned
+ Ready to receive feedback  

***

## Motivation: Segmentation is "Divide and Conquer"

As many consulting companies, e.g. McKenzie and PwC, have claimed, well-defined customer segmentation has the ability to cut through to the heart of business strategy. Perhaps, segmentation can be simply understood by a picture of a top-down mechanism. I like to bring the concept of the decision tree algorithm in two folds. First, the "complexity" of function depends on the depth and second, more splits provide a finer-grained partitioning; hence more homogeneous groups would result. Despite many caveats to this model and its concept (e.g. robustness of these segments), the model cannot avoid that this is an opportunity-seeking model. It is breaking through defined groups and uncovering potential leading customers with unmet needs, the one of era analytic marketers should strive and provide solution to effectively reduce such Type 1 error. This took time to sink into me. I hope the message is not about what model we should consider. It is the necessity to plan for both actionable short-term and robust long-term bulk scoring ... leverage CRM system. 

In this post I would like to illustrate example on actionable target marketing with RFM model, emphasizing on the Pareto Principle. Now lets break down its components in details. 


***


## What is RFM and why we do feature engineering 

Past history, Marketers realised that targeting the right audience is just as important as what you send. The phrase "right" customer is too general. The RFM sets a trick here. It is well known statistical model to develope a sense of company's customer base. Top of this, due to its high adaptibility in various industries it serves as good starting point of your customer segmentation journey. 

RFM is essentially a combined measure of customer's purchasing characteristics; a method that lets business to pin down their most valuable customers and in what degree in differs as the score board progressively descends. RFM consists three customer behviour data points: recency, frequency and monetary. 

+ Recency  : __Length__ of a time period since the __last__ purchase 
+ Frequency: __Number__ of purchase within a customer life-span
+ Monetary : __Amount__ of money spent within customer transcations

There are various approaches to these weight control and customerised metrics but at its basic, 
In data science world RFM is a type of feature engineering.

>The main advantage of RFM analysis is that it provides meaningful information about customers while using fewer (three-dimensional) criterions as cluster attributes. This reduces the complexity of customer value analysis model __without compromising its accuracy__.

is a process of applying domain knowledge to select and transform raw data into new definable features (characterisitcs, properties, attributes) that better represent the underlying problem to solve or predict with improve model performance. 

Decision makings starts with assumptions: hypothesis. When I make a strong assumptions, I bring that idea and paint it on 2D canvas; this is a model. Yes - there will be restrictions and some of the excluded information can be important. It is almost too scary to see what may be behind that canvas.

Feature engineering is a trick that brings some of this hidden layers of information. Because I force them wear on a similar colour. Hence a prior knowledge at this specific domain gains more opportunity to be creative and ... upon the assumptions made. 

***



RFM metrics traditional marketing trick to segment customer based on their purchasing behaviours. Past history, Marketers realised that targeting the right audience is just as important as what you send. The idea behind a target marketing can be defined as "anyone interested in our service". Short understandings and certainity on their values for the future put more risk on your marketing decisions. 

$$ 
\ E(Sales) = \frac{Sales + Cost\ of\ goods\ sold}{Risk} 
$$


Having said, well-defined customer-life value is not easy calculations, in fact, it should not be ran by company's operational convience. In my opinion, the "Target" will ever be general and the key will always be a simplifications. 


So why are we doing RFM? To find customer priorities? for better customer organisation? - Yes these may all remain true. But, I would like to interpret differently, the heart of scoring and segmentation is to 

There is no doubt having expert knowlegde in the domain of business has a capability to reveal tangible insights and priorties business initiative. 

In opinion its message is clear. There are no one-way scoring mechanism 


In-depth KYC (know your customer) ensures  only can leverage ROI from  
it may give the feel of understanding your customer better, as you are almost personalising their uniquness. However, this holds two caveats, 


Small Segment of customers -> reduce costs 


## Why Campaign Response important feature for CRM?

A campaign has variety of strategy and some examples are as follow:

+ A time bound offer
+ To specific set of target group e.g. location, age and payment methods etc. 
+ For a specific purchasing behaviour
+ A specific reward for the same

Tracking campaign activities are vital measure for customer engagement management. Effective CRM system can leverage life time value of company's customers by effectively reducing their churn events. Data scientist and engineers are very well adapted in this field. Every businesses needs to optimise the balance between the cost retention and acquisition as they are essential track for ROI generation. 


The data I will be exploring below contains customer campaign response binary dataset. 

***

## 1.0 library load

```{r, echo = FALSE, message=FALSE, warning=FALSE}

library(readr)
library(fs)
library(tufte)

library(forecast)

library(GGally)
library(knitr)
library(kableExtra)

```


```{r, echo = TRUE, message=FALSE, warning=FALSE}

library(tidyverse)
library(dplyr)
library(lubridate)
library(skimr)
library(scales)
library(psych)
library(gmodels)

library(tidyquant)
library(timetk)
library(rsample) 
library(wesanderson)
```


```{r, message=FALSE, warning=FALSE}
set.seed(42)
options(scipen = 100, digits = 3)

trans_df_raw <- read_csv("~/Dropbox/business_insight/df_trans_raw.csv", col_types = cols())

```


```{r, message=FALSE, warning=FALSE}
resp_df_raw <- read_csv("~/Dropbox/business_insight/df_resp_raw.csv")
resp_df_raw$X1 <- NULL
```


## 2.0 Transcational data 

```{r, warning=FALSE}
trans_df_raw %>% head()

resp_df_raw %>% head()

```


```{r}
trans_df <- trans_df_raw %>% 
  mutate(trans_date = lubridate::dmy(trans_date)) %>% 
  dplyr::rename(
    date   = trans_date,
    amount = tran_amount) 
```


## 3.0 Data Preparation 


### 3.1 Checking for duplicates 

* No duplicate data 
* Diagnosed same-day repeated purchase 
  + 10.7% from total customers falls in this category 
  +  0.6% of occurrence within transaction history 

Knowing that the level of occurance on sameday repeated purchase 
```{r}
format_dollar  <- function(x, ...) {
  paste0("$", formatC(as.numeric(x), format="f", digits = 0, big.mark=","))
}

se <- function(x) sqrt(var(x)/length(x))

```

```{r}

dup_pur_customer <- which(duplicated(trans_df[, c("customer_id", "date")]))
y_dup_df <- trans_df[dup_pur_customer,]
n_dup_df <- trans_df[-dup_pur_customer,]

y_dup_df$dup <- TRUE
n_dup_df$dup <- FALSE


cust_dup_tbl <- rbind(y_dup_df, n_dup_df) %>% 
  group_by(customer_id) %>% 
  summarise(monetary = sum(amount),
            frequency = n(),
            dup = mean(dup) %>% ceiling() %>% as.factor()) %>% ungroup()

```

A quick statistics representations by these groups are not significantly different. It is not easy to make scientific decisions when the sample sizes differ in high degree. We all know often visualisation can be a great help. 

```{r}
cust_dup_tbl %>% 
  group_by(dup) %>% 
  summarise(count            = n_distinct(customer_id),
            med_pur_freq     = median(frequency),
            avg_pur_freq     = mean(frequency),
            sd_freq          = sd(frequency),
            se_freq          = se(frequency),
            med_pur_monetary = median(monetary) %>% format_dollar,
            avg_pur_monetary = mean(monetary)   %>% format_dollar,
            sd_monetary      = sd(monetary),
            se_monetary      = se(monetary)) %>% 
    kbl() %>% 
    kable_material_dark()
```

This has become more interesting. Customer monetary value, by nature, has two noticeable binary distributions (there can always be more) overlapped from one and another. It may be valid to make assumption that there is a threshold from its low-density region to high-density region in purchasing characteristic cluster in amount. Thorough analysis on their purchasing pattern can add valuable information for cross-sell, offer, pricing strategy. However, this is not my primary focus for now. I like to make quick, actionable marketing decisions on 'target' customers.

```{r}

cust_dup_tbl %>% 
  ggplot(aes(x = monetary)) +
  geom_density(alpha = 0.5, fill = palette_light()[3]) +
  scale_fill_tq() +
  theme_tq()
```
The mean, median and other sample statistics had trouble to explain this difference in 

```{r}
cust_dup_tbl %>% 
  ggplot(aes(x = monetary, fill = dup)) +
  geom_density(alpha = 0.5) +
  scale_fill_tq() +
  theme_tq()

```


### 4.0 Quick overview on time series visualisation 

Like I have mentioned above, having __prior knowledge__ is a key to successful feature engineering. RFM metrics is not an exception. This data unfortunately does not hold background information (supplied by Regi found on Kaggle community). Instead, there are time and sales components, and additionally campaign customer response flag data. From what is given lets pull some insights and write in it on our perspective note.

Key questions

* What phase are we in with revenue growth? (Recession, Recovery, Expansion)
  + Is there noticeable trend?
  + Periodic revenue fluctuation found in the customer activity
    + Seasonal?
    + Cycle? 
  + How the campaign responded consumption patterns are evolving? 
    + Yes - How we should fine-tune campaign strategies accordingly?
    + No  - What could be the key influential factor (e.g. competitors, lack of recent customer engagement) 
  

### 4.1 Starting with periodic revenue 

Depending on where we are with business cycle, of course, the companies priorities and spending cost needs to be adjusted accordingly. Depending the phase the scope of the target customer may regulate stricter or lesser policy - risk measurement. 

What's more, visualising time series supplies valuable boundary to the company's goal. The ability to scale and formulate a value where the business falls within a High <--> Low risk spectrum is spectacularly important before scoring any business/finance matrices. I will illustrate this on my next blog using credit loan default risk measure. 

```{r}
timespan_day <- max(trans_df$date) - min(trans_df$date)

timespan_year <- format(as.numeric(timespan_day/365.24), digits = 3)

cat("There is",paste0(timespan_year), "annual cycle worth of data")

```

Broadly speaking, there are two types of time series data: stationary and non-stationary time series. These are defined as whether their statistical properties, in general, mean and variance is independent (stationary) or dependent (non-stationary) with time. Any human interactive data wont be stationary, there always be some trend or seasonality that enables us to decompose from its series. In other words, understanding these components will aid tracking down their purchasing patterns.

```{r}
daily_pur <- trans_df %>%
  dplyr::group_by(date) %>%
  dplyr::summarise(frequency = n(),
                   sales     = sum(amount)) %>% ungroup()

week_pur <- trans_df %>%
  mutate(week = ceiling_date(date, unit = "week")) %>% 
    filter_by_time(.start_date = "2011-07", 
                   .end_date = "2015-03-01",
                   .date_var = week) %>% 
  dplyr::group_by(week) %>%
  dplyr::summarise(frequency = n(),
                   sales     = sum(amount)) %>% ungroup()


monthly_pur <- trans_df %>%
  mutate(month = ceiling_date(date, unit = "month")) %>% 
    filter_by_time(.start_date = "2011-07", 
                   .end_date = "2015-03-01",
                   .date_var = month) %>% 
  dplyr::group_by(month) %>%
  dplyr::summarise(frequency = n(),
                   sales     = sum(amount)) %>% ungroup()
```

The volatile daily and weekly transcations data can bury many useful information such as trend or activity variance.

```{r, warning = FALSE, message=FALSE}

daily_pur %>% 
  mutate_by_time(.by = "week",
                 sales_mean = mean(sales)) %>% 
  pivot_longer(contains("sales")) %>% 
  plot_time_series(date, value, name,
                   .smooth = FALSE,
                   .interactive = FALSE) 

week_pur %>%
  mutate_by_time(.by = "month",
                 sales_mean = mean(sales)) %>% 
  pivot_longer(contains("sales")) %>% 
  plot_time_series(week, value, name,
                   .smooth = FALSE,
                   .interactive = FALSE) 

```


```{r, warning = FALSE, message=FALSE}

P4_monthly_plot <- monthly_pur %>% 
  mutate_by_time(.by = "3 month",
                 sales_mean = mean(sales)) %>% 
  pivot_longer(contains("sales")) %>% 
  plot_time_series(month, value, name,
                   .smooth      = FALSE,
                   .legend_show = FALSE,
                   .title       = "The monthly revenue time series with 3-month moving average",
                   .interactive = FALSE) 

mon_sd_tbl <- monthly_pur %>% 
  mutate(revenue = log(sales)) %>% 
  mutate(revenue = standardize_vec(revenue)) 

P4_monthlysd_plot <- mon_sd_tbl %>% 
  plot_time_series(month, revenue,
                   .title       = "The monthly revenue time series with its standard deviation",
                   .interactive = FALSE) 

gridExtra::grid.arrange(P4_monthly_plot, P4_monthlysd_plot, ncol = 2)


```


Perspective Notes : 
[1] The historical data shows there are hardly any correlation between date and customer behaviour, likely to be a random walk model 
[2] There is a noticeable correlation between customer id and their purchasing behaviour. I anticipate could be due to number of reasons, perhaps, the primary key was organised by geographical manner, time, or other possible useful information held behind. 



```{r}

```

```{r}

```


```{r}

trans_df %>%
  mutate(month = month(date, label = TRUE, abbr = FALSE)) %>% 
  dplyr::group_by(month) %>%
  dplyr::summarise(frequency = n(),
                   sales     = sum(amount) %>%  format_dollar()) %>% 
  set_names(c("Month", "Frequency", "Revenue")) %>%
  kbl() %>% 
  kable_material_dark()

trans_df %>%
  mutate(quarter = quarter(date)) %>% 
  dplyr::group_by(quarter) %>%
  dplyr::summarise(frequency = n(),
                   sales     = sum(amount) %>% format_dollar()) %>% 
  set_names(c("Quarter", "Frequency", "Revenue")) %>%
  kbl() %>% 
  kable_material_dark()

```



```{r}
resp <- resp_df_raw %>% 
  select(customer_id, response) %>% 
  setNames(c("customer_id", "campaign"))

pre_rfm_df <- trans_df %>% 
  left_join(resp, by = "customer_id") %>% 
  mutate(campaign = case_when(campaign == 1 ~ 1,
                                TRUE ~ 0))
  

```


```{r}
colSums(is.na(pre_rfm_df))

pre_rfm_df %>% 
  mutate(month = month(date)) %>%
  group_by(campaign) %>% 
  summarise(no_pur  = n(),
            avg_pur = mean(amount)   %>% format_dollar(),
            med_pur = median(amount) %>% format_dollar(),
            sd_pur  = sd(amount)     %>% format_dollar()) %>% 
  ungroup() %>% 
  set_names(c("Campaign", "No Purchase", "Averge Purchase", "Median Purchase", "Std Purchase")) %>%
  kbl() %>% 
  kable_material_dark()

```


```{r}
library(reshape2)
cmpg_df <- pre_rfm_df %>% 
  mutate(campaign = case_when(campaign == 1 ~ 1,
                                TRUE ~ 0),
         week = ceiling_date(date, unit = "week")) %>%
  group_by(week, campaign) %>% 
  summarise(no_pur = n(),
            revenue = sum(amount))


cmpg_pur_df <- cmpg_df %>% 
  dcast(week ~ campaign, value.var = "no_pur") 
cmpg_rev_df <- cmpg_df %>% 
  dcast(week ~ campaign, value.var = "revenue") 

cmpg_vis_df <- left_join(cmpg_pur_df, cmpg_rev_df, by = "week") %>% 
  set_names(c("week", "no_pur_N", "no_pur_Y", "sales_N", "sales_Y")) %>% 
  filter_by_time(.start_date = "2011-07", 
                   .end_date = "2015-03-01",
                   .date_var = week) %>% 
  
  group_by(week) %>%
  summarise(no_pur_N_cmpg  = sum(no_pur_N),
            no_pur_Y_cmpg  = sum(no_pur_Y),
            revenue_N_cmpg = sum(sales_N),
            revenue_Y_cmpg = sum(sales_Y)) %>% ungroup()
  
cmpg_vis_df %>% plot_time_series(week, 
                                 revenue_Y_cmpg,
                                 .smooth = FALSE,
                                 .interactive = FALSE) 

cmpg_vis_df %>% plot_time_series(week, 
                                 revenue_N_cmpg,
                                 .smooth = FALSE,
                                 .interactive = FALSE) 

cmpg_vis_df %>% plot_time_series(week, 
                                 no_pur_Y_cmpg,
                                 .smooth = FALSE,
                                 .interactive = FALSE) 

cmpg_vis_df %>% plot_time_series(week, 
                                 no_pur_N_cmpg,
                                 .smooth = FALSE,
                                 .interactive = FALSE) 

  
```


the RFM scoring possess less weight on customer next purchasing prediction value. By means, although RFM metrics yet remain its purpose of segmenting superior to sleeping customer but it would be a good predictor index for estimating who would be most likely to purchase in given time period. As the stationary means it occurs by chance. 
