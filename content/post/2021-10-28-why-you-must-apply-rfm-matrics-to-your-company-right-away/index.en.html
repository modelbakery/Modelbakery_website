---
title: Why you Must apply RFM matrics to your company right away!
author: Package Build
date: '2021-10-26'
slug: why-you-must-apply-rfm-matrics-to-your-company-right-away
categories:
  - R
tags:
  - Marketing
subtitle: ''
summary: ''
authors: []
lastmod: '2021-10-26T03:10:43+09:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
draft: FALSE
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>
<script src="{{< blogdown/postref >}}index.en_files/kePrint/kePrint.js"></script>
<link href="{{< blogdown/postref >}}index.en_files/lightable/lightable.css" rel="stylesheet" />


<p>You may have heard of the Pareto Principle, but knowing how would you utilize its advantages, in reality, can seriously save your time. By nature of sales businesses, in general, have an urgency to secure as many leads as quickly as possible due to the market competition. Therefore, depending on the number of leads, levels of customers’ demographics, desire, and product categories, the power of the leading score can ever so be significant. In this post, I like to introduce a simple statistical RFM metrics and further discuss the essence of scoring. There are many excellent books and course materials with technical details on statistical inference, but knowing when and how to apply it in the real world is another muscle we all need to train ourselves.</p>
<hr />
<div id="menu" class="section level2">
<h2>Menu</h2>
<div id="appetizer" class="section level3">
<h3>Appetizer</h3>
<ul>
<li>Motivation: Segmentation is “Divide and Conquer”</li>
<li>What is RFM and why we do feature engineering</li>
<li>Why Campaign Response important feature for CRM?</li>
<li>Power of Quantile and Pareto Principle</li>
<li>Robust Scoring Rules</li>
</ul>
</div>
<div id="main" class="section level3">
<h3>Main</h3>
<ul>
<li>Time Series Visualisation + seasonal effect</li>
<li>Data splitting</li>
<li>RFM Scoring</li>
<li>Visualisation for business report</li>
</ul>
</div>
<div id="desert" class="section level3">
<h3>Desert</h3>
<ul>
<li>Perspective note</li>
<li>To take action</li>
<li>Wrap up: What I have learned</li>
<li>Ready to receive feedback</li>
</ul>
<hr />
</div>
</div>
<div id="motivation-segmentation-is-divide-and-conquer" class="section level2">
<h2>Motivation: Segmentation is “Divide and Conquer”</h2>
<p>As many consulting companies, e.g. McKenzie and PwC, have claimed, well-defined customer segmentation has the ability to cut through to the heart of business strategy. Perhaps, segmentation can be simply understood by a picture of a top-down mechanism. I like to bring the concept of the decision tree algorithm in two folds. First, the “complexity” of function depends on the depth and second, more splits provide a finer-grained partitioning; hence more homogeneous groups would result. Despite many caveats to this model and its concept (e.g. robustness of these segments), the model cannot avoid that this is an opportunity-seeking model. It is breaking through defined groups and uncovering potential leading customers with unmet needs, the one of era analytic marketers should strive and provide solution to effectively reduce such Type 1 error. This took time to sink into me. I hope the message is not about what model we should consider. It is the necessity to plan for both actionable short-term and robust long-term bulk scoring … leverage CRM system.</p>
<p>In this post I would like to illustrate example on actionable target marketing with RFM model, emphasizing on the Pareto Principle. Now lets break down its components in details.</p>
<hr />
</div>
<div id="what-is-rfm-and-why-we-do-feature-engineering" class="section level2">
<h2>What is RFM and why we do feature engineering</h2>
<p>Past history, Marketers realised that targeting the right audience is just as important as what you send. The phrase “right” customer is too general. The RFM sets a trick here. It is well known statistical model to develope a sense of company’s customer base. Top of this, due to its high adaptibility in various industries it serves as good starting point of your customer segmentation journey.</p>
<p>RFM is essentially a combined measure of customer’s purchasing characteristics; a method that lets business to pin down their most valuable customers and in what degree in differs as the score board progressively descends. RFM consists three customer behviour data points: recency, frequency and monetary.</p>
<ul>
<li>Recency : <strong>Length</strong> of a time period since the <strong>last</strong> purchase</li>
<li>Frequency: <strong>Number</strong> of purchase within a customer life-span</li>
<li>Monetary : <strong>Amount</strong> of money spent within customer transcations</li>
</ul>
<p>There are various approaches to these weight control and customerised metrics but at its basic,
In data science world RFM is a type of feature engineering.</p>
<blockquote>
<p>The main advantage of RFM analysis is that it provides meaningful information about customers while using fewer (three-dimensional) criterions as cluster attributes. This reduces the complexity of customer value analysis model <strong>without compromising its accuracy</strong>.</p>
</blockquote>
<p>is a process of applying domain knowledge to select and transform raw data into new definable features (characterisitcs, properties, attributes) that better represent the underlying problem to solve or predict with improve model performance.</p>
<p>Decision makings starts with assumptions: hypothesis. When I make a strong assumptions, I bring that idea and paint it on 2D canvas; this is a model. Yes - there will be restrictions and some of the excluded information can be important. It is almost too scary to see what may be behind that canvas.</p>
<p>Feature engineering is a trick that brings some of this hidden layers of information. Because I force them wear on a similar colour. Hence a prior knowledge at this specific domain gains more opportunity to be creative and … upon the assumptions made.</p>
<hr />
<p>RFM metrics traditional marketing trick to segment customer based on their purchasing behaviours. Past history, Marketers realised that targeting the right audience is just as important as what you send. The idea behind a target marketing can be defined as “anyone interested in our service”. Short understandings and certainity on their values for the future put more risk on your marketing decisions.</p>
<p><span class="math display">\[ 
\ E(Sales) = \frac{Sales + Cost\ of\ goods\ sold}{Risk} 
\]</span></p>
<p>Having said, well-defined customer-life value is not easy calculations, in fact, it should not be ran by company’s operational convience. In my opinion, the “Target” will ever be general and the key will always be a simplifications.</p>
<p>So why are we doing RFM? To find customer priorities? for better customer organisation? - Yes these may all remain true. But, I would like to interpret differently, the heart of scoring and segmentation is to</p>
<p>There is no doubt having expert knowlegde in the domain of business has a capability to reveal tangible insights and priorties business initiative.</p>
<p>In opinion its message is clear. There are no one-way scoring mechanism</p>
<p>In-depth KYC (know your customer) ensures only can leverage ROI from<br />
it may give the feel of understanding your customer better, as you are almost personalising their uniquness. However, this holds two caveats,</p>
<p>Small Segment of customers -&gt; reduce costs</p>
</div>
<div id="why-campaign-response-important-feature-for-crm" class="section level2">
<h2>Why Campaign Response important feature for CRM?</h2>
<p>A campaign has variety of strategy and some examples are as follow:</p>
<ul>
<li>A time bound offer</li>
<li>To specific set of target group e.g. location, age and payment methods etc.</li>
<li>For a specific purchasing behaviour</li>
<li>A specific reward for the same</li>
</ul>
<p>Tracking campaign activities are vital measure for customer engagement management. Effective CRM system can leverage life time value of company’s customers by effectively reducing their churn events. Data scientist and engineers are very well adapted in this field. Every businesses needs to optimise the balance between the cost retention and acquisition as they are essential track for ROI generation.</p>
<p>The data I will be exploring below contains customer campaign response binary dataset.</p>
<hr />
</div>
<div id="library-load" class="section level2">
<h2>1.0 library load</h2>
<pre class="r"><code>library(tidyverse)
library(dplyr)
library(lubridate)
library(skimr)
library(scales)
library(psych)
library(gmodels)

library(tidyquant)
library(timetk)
library(rsample) 
library(wesanderson)</code></pre>
<pre class="r"><code>set.seed(42)
options(scipen = 100, digits = 3)

trans_df_raw &lt;- read_csv(&quot;~/Dropbox/business_insight/df_trans_raw.csv&quot;, col_types = cols())</code></pre>
<pre class="r"><code>resp_df_raw &lt;- read_csv(&quot;~/Dropbox/business_insight/df_resp_raw.csv&quot;)
resp_df_raw$X1 &lt;- NULL</code></pre>
</div>
<div id="transcational-data" class="section level2">
<h2>2.0 Transcational data</h2>
<pre class="r"><code>trans_df_raw %&gt;% head()</code></pre>
<pre><code>## # A tibble: 6 × 3
##   customer_id trans_date tran_amount
##   &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;
## 1 CS5295      11-Feb-13           35
## 2 CS4768      15-Mar-15           39
## 3 CS2122      26-Feb-13           52
## 4 CS1217      16-Nov-11           99
## 5 CS1850      20-Nov-13           78
## 6 CS5539      26-Mar-14           81</code></pre>
<pre class="r"><code>resp_df_raw %&gt;% head()</code></pre>
<pre><code>## # A tibble: 6 × 2
##   customer_id response
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 CS1112             0
## 2 CS1113             0
## 3 CS1114             1
## 4 CS1115             1
## 5 CS1116             1
## 6 CS1117             0</code></pre>
<pre class="r"><code>trans_df &lt;- trans_df_raw %&gt;% 
  mutate(trans_date = lubridate::dmy(trans_date)) %&gt;% 
  dplyr::rename(
    date   = trans_date,
    amount = tran_amount) </code></pre>
</div>
<div id="data-preparation" class="section level2">
<h2>3.0 Data Preparation</h2>
<div id="checking-for-duplicates" class="section level3">
<h3>3.1 Checking for duplicates</h3>
<ul>
<li>No duplicated data</li>
<li>Diagnosed same-day repeated purchase
<ul>
<li>10.7% from total customers falls in this category</li>
<li>0.6% of occurrence within transaction history</li>
</ul></li>
</ul>
<p>Important question to ask:</p>
<ul>
<li>Are these group statistically significant in respect to their revenue?</li>
<li>Does same-day repeated purchase represent any meaning?</li>
</ul>
<pre class="r"><code>format_dollar  &lt;- function(x, ...) {
  paste0(&quot;$&quot;, formatC(as.numeric(x), format=&quot;f&quot;, digits = 0, big.mark=&quot;,&quot;))
}

se &lt;- function(x) sqrt(var(x)/length(x))</code></pre>
<pre class="r"><code>dup_pur_customer &lt;- which(duplicated(trans_df[, c(&quot;customer_id&quot;, &quot;date&quot;)]))
y_dup_df &lt;- trans_df[dup_pur_customer,]
n_dup_df &lt;- trans_df[-dup_pur_customer,]

y_dup_df$samday_purchase &lt;- TRUE
n_dup_df$samday_purchase &lt;- FALSE


cust_dup_tbl &lt;- rbind(y_dup_df, n_dup_df) %&gt;% 
  group_by(customer_id) %&gt;% 
  summarise(monetary = sum(amount),
            frequency = n(),
            samday_purchase = mean(samday_purchase) %&gt;% ceiling() %&gt;% as.factor()) %&gt;% ungroup()</code></pre>
<pre class="r"><code>cust_dup_tbl %&gt;% 
  group_by(samday_purchase) %&gt;% 
  summarise(count            = n_distinct(customer_id),
            med_pur_freq     = median(frequency),
            avg_pur_freq     = mean(frequency),
            sd_freq          = sd(frequency),
            se_freq          = se(frequency),
            med_pur_monetary = median(monetary) %&gt;% format_dollar,
            avg_pur_monetary = mean(monetary)   %&gt;% format_dollar,
            sd_monetary      = sd(monetary),
            se_monetary      = se(monetary)) %&gt;% 
    kbl() %&gt;% 
    kable_material_dark()</code></pre>
<table class=" lightable-material-dark" style="font-family: &quot;Source Sans Pro&quot;, helvetica, sans-serif; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
samday_purchase
</th>
<th style="text-align:right;">
count
</th>
<th style="text-align:right;">
med_pur_freq
</th>
<th style="text-align:right;">
avg_pur_freq
</th>
<th style="text-align:right;">
sd_freq
</th>
<th style="text-align:right;">
se_freq
</th>
<th style="text-align:left;">
med_pur_monetary
</th>
<th style="text-align:left;">
avg_pur_monetary
</th>
<th style="text-align:right;">
sd_monetary
</th>
<th style="text-align:right;">
se_monetary
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
0
</td>
<td style="text-align:right;">
6152
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
17.8
</td>
<td style="text-align:right;">
5.12
</td>
<td style="text-align:right;">
0.065
</td>
<td style="text-align:left;">
$1,194
</td>
<td style="text-align:left;">
$1,149
</td>
<td style="text-align:right;">
461
</td>
<td style="text-align:right;">
5.88
</td>
</tr>
<tr>
<td style="text-align:left;">
1
</td>
<td style="text-align:right;">
737
</td>
<td style="text-align:right;">
21
</td>
<td style="text-align:right;">
21.2
</td>
<td style="text-align:right;">
4.76
</td>
<td style="text-align:right;">
0.175
</td>
<td style="text-align:left;">
$1,486
</td>
<td style="text-align:left;">
$1,436
</td>
<td style="text-align:right;">
426
</td>
<td style="text-align:right;">
15.69
</td>
</tr>
</tbody>
</table>
</div>
<div id="hypothesis-test-on-same-day-repeated-and-non-repeated-customer" class="section level3">
<h3>3.2 Hypothesis Test on same-day repeated and non-repeated customer</h3>
<ul>
<li>Unbalanced Welch Two Sample t-test</li>
<li>Balanced Welch Two Sample t-test</li>
</ul>
<p>Both the hypothesis tests rejects null hypothesis (p-value &lt; 0.05) that the difference in mean of customer “same-day duplicate” and “non-duplicate” groups are not significant.</p>
<blockquote>
<p>p &lt; .05 bar is too easy to pass and sometimes serves as an excuse for lower quality research."
- Traflmow, editor of the Basic and Applied Social Psychology journal
- Mentioned by Ashutosh Nandeshwar</p>
</blockquote>
<p>A small p-value does not indicate that the null hypothesis is false, in fact it is commonly understood that the extent of p-value should not footnote “statistically significant”. However, we can invoke the result is not by chance.</p>
<pre class="r"><code>random_n_dup_df &lt;- sample_n(n_dup_df, 737) 
random_n_dup_df$samday_purchase &lt;- FALSE

sample_dup_tbl &lt;- rbind(y_dup_df, random_n_dup_df) %&gt;% 
  group_by(customer_id) %&gt;% 
  summarise(monetary = sum(amount),
            frequency = n(),
            samday_purchase = mean(samday_purchase) %&gt;% ceiling() %&gt;% as.factor()) %&gt;% ungroup()

t.test(monetary~samday_purchase, data = cust_dup_tbl)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  monetary by samday_purchase
## t = -17, df = 955, p-value &lt;0.0000000000000002
## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0
## 95 percent confidence interval:
##  -320 -255
## sample estimates:
## mean in group 0 mean in group 1 
##            1149            1436</code></pre>
<pre class="r"><code>t.test(monetary~samday_purchase, data = sample_dup_tbl)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  monetary by samday_purchase
## t = 6, df = 1326, p-value = 0.00000002
## alternative hypothesis: true difference in means between group 1 and group 0 is not equal to 0
## 95 percent confidence interval:
##   7.08 14.64
## sample estimates:
## mean in group 1 mean in group 0 
##            80.3            69.5</code></pre>
</div>
<div id="visualisation-on-same-day-repeated-and-non-repeated-customer" class="section level3">
<h3>3.3 Visualisation on same-day repeated and non-repeated customer</h3>
<p>The visualisation here provides more insights.</p>
<ul>
<li><p>Customer monetary value, by nature, has two noticeable binary distributions (there always can be more) overlapped from one and another. It would be valid to make a weak assumptions in the threshold from its low-monetary value region to high-monetary value region.</p></li>
<li><p>The difference in low-monetary region (approx. first quantile) to low-monetary region (approx. third quantile) proportions between these groups is indeed notable.</p></li>
<li><p>Given the same weight on number of customers to these groups, their first quantile density shrinks by factor of ~2.</p></li>
</ul>
<p>The mean, median and other sample statistics had trouble to explain this proportional difference of these two regions. I believe this is where it gets quite tricky. Repeated same-day purchase records themselves are not a meaningful feature, but due to “confonding” effects it false appearence as if it has some predictive power between these two groups.</p>
<p>Perhaps it is the <strong>accessibility</strong> upon purchase act as a confounder between monetary value and same-day purchase customer behaviour. With given customer geographical data or payment methods it would be worth to investigate further but from data what is given the hypothesis remains too strong of an assumptions to define any insightful meanings.</p>
<pre class="r"><code>P3_C_dup &lt;- cust_dup_tbl %&gt;% ggplot(aes(x = samday_purchase, y = monetary, fill = samday_purchase)) +
  geom_col(alpha = 0.5) +
  scale_y_continuous(labels = scales::dollar_format()) +
  scale_fill_tq() +
  theme_tq()

P3_B_dup &lt;- cust_dup_tbl %&gt;% ggplot(aes(x = samday_purchase, y = monetary, fill = samday_purchase)) +
  geom_boxplot(alpha = 0.5) +
  scale_y_continuous(labels = scales::dollar_format()) +
  scale_fill_tq() +
  theme_tq()

P3_V_dup &lt;- cust_dup_tbl %&gt;% ggplot(aes(x = samday_purchase, y = monetary, fill = samday_purchase)) +
  geom_violin(alpha = 0.5) +
  scale_y_continuous(labels = scales::dollar_format()) +
  scale_fill_tq() +
  theme_tq()

P3_D_dup &lt;- cust_dup_tbl %&gt;% 
  ggplot(aes(x = monetary, fill = samday_purchase)) +
  geom_density(alpha = 0.5) +
  scale_fill_tq() +
  theme_tq()

gridExtra::grid.arrange(P3_C_dup, P3_B_dup, P3_V_dup, P3_D_dup, ncol = 2)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="perspective-notes" class="section level3">
<h3>3.4 Perspective Notes</h3>
<ul>
<li><p>Comparative analysis of potential aspects on “what is the behind the scene” of customers at low and high monetary regions are needed.</p>
<ul>
<li>Their revenue contributes (MRR or ARR) and churn rate needs to be modeled in order to measure their worth (risk measure) before the engagement strategy.</li>
</ul></li>
<li><p>In order to identify common characteristics among customers, the “hard” variables such as age, gender, place of residence are needed.</p></li>
<li><p>Inquire customer information on geographical and payment method from database in order to approach on defining “soft” variables such as “service accessibility” and its scoring board.</p></li>
</ul>
<hr />
</div>
</div>
<div id="quick-overview-on-time-series-visualisation" class="section level2">
<h2>4.0 Quick overview on time series visualisation</h2>
<p>In this section I will explore and study on revenue time series visualisation and TSL (Trend, Seasonal, Irregular) components. Unfortunately, I would not be able to throw any punch line of my task here. Especially I wish not to deliver any miss interpreted knowledge to anyone. Hence I will be only clarifying the techniques and quotes supplied by professionals and codes with real thoughts.</p>
<p>Key questions to ask:</p>
<ul>
<li>What phase are we in with revenue growth?
<ul>
<li>Is there noticeable trend?</li>
<li>Periodic revenue fluctuation found in the customer activity
<ul>
<li>Seasonal?</li>
<li>Cycle?</li>
</ul></li>
<li>How the campaign responded consumption patterns are evolving?
<ul>
<li>Positive - How we should fine-tune campaign strategies accordingly?</li>
<li>Negative - What could be the key influential factor (e.g. competitors, lack of recent customer engagement)?</li>
</ul></li>
</ul></li>
</ul>
<div id="why-time-series" class="section level3">
<h3>4.1 Why time series?</h3>
<p>Depending on where businesses are with their business cycle, of course, the companies priorities and spending cost needs to be adjusted accordingly. Depending on the phase, the scope of the target customer may regulate stricter or lesser policy (risk measurement).</p>
<p>What’s more, visualising time series supplies valuable boundary to the company’s goal. The ability to scale and formulate a value where the business falls within a High &lt;–&gt; Low risk spectrum is spectacularly important before scoring any business/finance matrices. I can not wait to study this more for future blog post on credit loan default risk prediction modelling.</p>
<pre class="r"><code>timespan_day &lt;- max(trans_df$date) - min(trans_df$date)

timespan_year &lt;- format(as.numeric(timespan_day/365.24), digits = 3)

cat(&quot;There is&quot;,paste0(timespan_year),&quot;annual cycle worth of data&quot;)</code></pre>
<pre><code>## There is 3.83 annual cycle worth of data</code></pre>
</div>
<div id="data-frame-of-daily-weekly-monthly-time-stamps-time-series-visualisation" class="section level3">
<h3>4.2 Data frame of daily, weekly, monthly time stamps + time series visualisation</h3>
<p>Above three years of revenue annual cycle is sufficient collection of data to predict its future revenue and sales. According to Amazon’s time series forecasting principles, also mentioned by Lucas Soares on <em>towards data science blog post</em>, forecasting is a hard problem for two reasons:</p>
<ul>
<li><p>Incorporating large volumes of historical data, which can lead to missing important information about the past of the target data dynamics.</p></li>
<li><p>Incorporating related yet independent data (holidays/event, locations, marketing promotions)</p></li>
</ul>
<p>Beside that, sales forecasting is likely to depend on the economic situation, the competition, consumer behaviour and many other external factors. For these reasons the concept of relaxing assumption will remain a key for quantitatively measuring the issues influencing on the revenue metrics. Again, too generic.</p>
<pre class="r"><code># Tidy eval + time series visualisation function 
start_date &lt;- &quot;2011-07&quot;
end_date   &lt;- &quot;2015-03-01&quot;

generator_periodic_tbl &lt;- function(data, 
                                   period, 
                                   start_date, 
                                   end_date, 
                                   var_x1, 
                                   var_x2){
  
  var_x1 &lt;- enquo(var_x1)
  var_x2 &lt;- enquo(var_x2)
  start_date &lt;- enquo(start_date)
  end_date   &lt;- enquo(end_date)
  
  period_list   &lt;- enquo(period)
  period_quote  &lt;- quo_name(period_list)
  
  out_tbl &lt;- data %&gt;%
    mutate(foo = ceiling_date(!!var_x2, unit = !!period_quote)) %&gt;% 
    filter_by_time(.start_date = !!start_date, 
                   .end_date = !!end_date,
                   .date_var = foo)  %&gt;% 
    dplyr::group_by(foo) %&gt;%
    dplyr::summarise(frequency = n(),
                     sales     = sum(!!var_x1)) %&gt;% ungroup()
  
  names(out_tbl)[1] &lt;- &quot;date&quot;
  out_tbl
}

generator_plot_ts &lt;- function(.data, 
                              .period, 
                              .var_x1){
  
  period_list &lt;- enquo(.period)
  period_name &lt;- quo_name(period_list)
  var_x1_list  &lt;- enquo(.var_x1)
  var_x1_name  &lt;- quo_name(var_x1_list)
  
  .data %&gt;% 
  mutate_by_time(.by = period_name,
                 sales_mean = mean(!!var_x1_list)) %&gt;% 
  pivot_longer(contains(!!var_x1_name))  %&gt;%  
  plot_time_series(date, value, name,
                   .smooth = FALSE,
                   .interactive = FALSE)
}</code></pre>
<p>In here I will like to illustrate traditional statisical method for decomposing time series. STL decomposition</p>
<p>uncover step-by-step how to</p>
<p>For these reasons, we first need to remove highly volatile and predictable</p>
<p>to generalise and interpret evidence for sales quantitative measure.</p>
<p>The volatility of daily and weekly transcation data can bury many useful information such as trend or activity variance.</p>
<pre class="r"><code>trans_df %&gt;% 
  generator_periodic_tbl(day, start_date, end_date, amount, date) %&gt;% 
  generator_plot_ts(week, sales)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code>trans_df %&gt;% 
  generator_periodic_tbl(week, start_date, end_date, amount, date) %&gt;% 
  generator_plot_ts(month, sales)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-14-2.png" width="672" /></p>
<pre class="r"><code>trans_df %&gt;% 
  generator_periodic_tbl(month, start_date, end_date, amount, date) %&gt;% 
  generator_plot_ts(&quot;3 month&quot;, sales)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-14-3.png" width="672" /></p>
<p>Now the monthly revenue time series reveals some useful information</p>
<ul>
<li>revenues with its worst performance. shows +3 years overall descending trend.</li>
</ul>
<pre class="r"><code>outlier_detection &lt;- function(x) {
  x[x &gt; quantile(x, .95, na.rm = T)] 
  x[x &lt; quantile(x, .05, na.rm = T)] 
}

trans_df %&gt;% 
  generator_periodic_tbl(month, start_date, end_date, amount, date) %&gt;% 
  mutate(revenue       = sales) %&gt;% 
  mutate(revenue_log   = log(sales)) %&gt;% 
  mutate(revenue_sd    = standardize_vec(revenue_log),
         outlier       = ifelse(revenue %in% outlier_detection(revenue) == TRUE, 1, NA),
         revenue_text  = case_when(outlier == 1 ~ revenue, 
                                   TRUE~ outlier),
         revenue_text  = scales::dollar(revenue_text, scale = 1e-3, suffix = &quot;K&quot;)) %&gt;% 
  ggplot(aes(x = date, y = (revenue_sd))) +
  geom_line(size = 0.5, linetype = 1) +
  geom_smooth(method   = &quot;lm&quot;, se = FALSE) +
  geom_label(aes(label = revenue_text,
             frontface = &quot;bold&quot;,
             vjust     = 0.5,
             hjust     = 1.2,
             color     = &quot;red&quot;)) +
  labs(
     title    = &quot;Monthly Revenue Time Series: We are at the end of seasonal downturn!&quot;,
     x        = &quot;Year&quot;,
     y        = &quot;Revenue&quot;
  ) +
  guides(fill = FALSE, color = FALSE, linetype = FALSE, shape = FALSE) +
  theme_tq()</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="stl-time-series-decomposition" class="section level3">
<h3>4.3 STL time series decomposition</h3>
<pre class="r"><code>generator_stl_monthly &lt;- function(data, start_date, end_date){
  
  start &lt;- start_date %&gt;% str_split(&quot;-&quot;) %&gt;% unlist()
  
  monthly_ts &lt;- data %&gt;% 
  generator_periodic_tbl(month, start_date, end_date, amount, date) %&gt;% 
  select(sales) %&gt;% 
  ts(start = c(start[1], start[2]), f = 12)
  
  t &lt;- 1:length(monthly_ts)
  
  fit_monthly_lm &lt;- lm(monthly_ts ~ t)
  trend &lt;- fitted(fit_monthly_lm)
  
  adjtrend = monthly_ts/trend
  y = factor(cycle(adjtrend))
  fit_monthly_1 &lt;-auto.arima(adjtrend,
                           max.p = 2,
                           xreg=model.matrix(~0 + y)[, -12],
                           seasonal = F, max.d = 0, max.q = 0)
  
  seasonal = fit_monthly_1$fitted
  pred = trend*seasonal
  irregular = monthly_ts/pred

  date &lt;- ym(start_date) + months(1:length(monthly_ts)-1)
  table4 &lt;- data.frame(date, monthly_ts, pred, trend, seasonal, irregular) %&gt;% 
    setNames(c(&quot;date&quot;, &quot;revenue&quot;, &quot;exp_rev&quot;,&quot;trend&quot;, &quot;seasonal&quot;, &quot;irregular&quot;))
  
  return(table4)
}

trans_df %&gt;% 
  generator_stl_monthly(start_date, end_date) </code></pre>
<pre><code>##          date revenue exp_rev  trend seasonal irregular
## 1  2011-07-01  174527  175463 177720    0.987     0.995
## 2  2011-08-01  178097  180555 177652    1.016     0.986
## 3  2011-09-01  188631  182978 177584    1.030     1.031
## 4  2011-10-01  169173  174824 177516    0.985     0.968
## 5  2011-11-01  182634  182586 177449    1.029     1.000
## 6  2011-12-01  166921  175796 177381    0.991     0.950
## 7  2012-01-01  181405  178718 177313    1.008     1.015
## 8  2012-02-01  177987  182293 177245    1.028     0.976
## 9  2012-03-01  170135  162392 177177    0.917     1.048
## 10 2012-04-01  180453  181418 177109    1.024     0.995
## 11 2012-05-01  168000  172737 177041    0.976     0.973
## 12 2012-06-01  178880  178898 176973    1.011     1.000
## 13 2012-07-01  172933  174659 176906    0.987     0.990
## 14 2012-08-01  179117  179727 176838    1.016     0.997
## 15 2012-09-01  179284  182138 176770    1.030     0.984
## 16 2012-10-01  175045  174022 176702    0.985     1.006
## 17 2012-11-01  181862  181748 176634    1.029     1.001
## 18 2012-12-01  179156  174989 176566    0.991     1.024
## 19 2013-01-01  173747  177897 176498    1.008     0.977
## 20 2013-02-01  181729  181455 176430    1.028     1.002
## 21 2013-03-01  160359  161646 176363    0.917     0.992
## 22 2013-04-01  181491  180584 176295    1.024     1.005
## 23 2013-05-01  173642  171942 176227    0.976     1.010
## 24 2013-06-01  185826  178075 176159    1.011     1.044
## 25 2013-07-01  176813  173855 176091    0.987     1.017
## 26 2013-08-01  180983  178899 176023    1.016     1.012
## 27 2013-09-01  180031  181299 175955    1.030     0.993
## 28 2013-10-01  176830  173220 175888    0.985     1.021
## 29 2013-11-01  181521  180910 175820    1.029     1.003
## 30 2013-12-01  177341  174182 175752    0.991     1.018
## 31 2014-01-01  180802  177077 175684    1.008     1.021
## 32 2014-02-01  184554  180618 175616    1.028     1.022
## 33 2014-03-01  154151  160899 175548    0.917     0.958
## 34 2014-04-01  179804  179750 175480    1.024     1.000
## 35 2014-05-01  174149  171148 175412    0.976     1.018
## 36 2014-06-01  169555  177252 175345    1.011     0.957
## 37 2014-07-01  172741  173051 175277    0.987     0.998
## 38 2014-08-01  179026  178072 175209    1.016     1.005
## 39 2014-09-01  178975  180460 175141    1.030     0.992
## 40 2014-10-01  173385  172418 175073    0.985     1.006
## 41 2014-11-01  179303  180072 175005    1.029     0.996
## 42 2014-12-01  174855  173375 174937    0.991     1.009
## 43 2015-01-01  174010  176256 174869    1.008     0.987
## 44 2015-02-01  179837  179780 174802    1.028     1.000
## 45 2015-03-01  160509  160153 174734    0.917     1.002</code></pre>
<pre class="r"><code>trans_df %&gt;% 
  generator_stl_monthly(start_date, end_date) %&gt;% 
  select(date, revenue, exp_rev) %&gt;% 
  gather(key = &quot;type_rev&quot;, value = &quot;value&quot;, -date) %&gt;% 
  ggplot(aes(date, value, group = type_rev)) +
  geom_line(aes(linetype = type_rev, colour = type_rev)) +
  scale_color_brewer(palette = &quot;Dark2&quot;) +
  theme_tq()</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<pre class="r"><code>trans_df %&gt;% 
  generator_stl_monthly(start_date, end_date) %&gt;%
  mutate(unexp_revenue  = revenue - exp_rev,
         risk_rev_index = unexp_revenue/revenue*100) %&gt;% 
  ggplot(aes(date, risk_rev_index)) +
  geom_line() +
  theme_tq()</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-17-2.png" width="672" /></p>
<p>Perspective Notes :
[1] The historical data shows there are hardly any correlation between date and customer behaviour, likely to be a random walk model
[2] There is a noticeable correlation between customer id and their purchasing behaviour. I anticipate could be due to number of reasons, perhaps, the primary key was organised by geographical manner, time, or other possible useful information held behind.</p>
</div>
<div id="comonents-for-time-series-analysis" class="section level3">
<h3>4.3 Comonents for Time Series Analysis</h3>
<pre class="r"><code>trans_df %&gt;%
  mutate(quarter = quarter(date)) %&gt;% 
  dplyr::group_by(quarter) %&gt;%
  dplyr::summarise(frequency = n(),
                   sales     = sum(amount) %&gt;% format_dollar()) %&gt;% 
  set_names(c(&quot;Quarter&quot;, &quot;Frequency&quot;, &quot;Revenue&quot;)) %&gt;%
  kbl() %&gt;% 
  kable_material_dark()</code></pre>
<table class=" lightable-material-dark" style="font-family: &quot;Source Sans Pro&quot;, helvetica, sans-serif; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
Quarter
</th>
<th style="text-align:right;">
Frequency
</th>
<th style="text-align:left;">
Revenue
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
30837
</td>
<td style="text-align:left;">
$2,005,838
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
28419
</td>
<td style="text-align:left;">
$1,846,017
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
32923
</td>
<td style="text-align:left;">
$2,138,577
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
32821
</td>
<td style="text-align:left;">
$2,133,557
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>trans_df %&gt;%
  mutate(month = month(date, label = TRUE, abbr = FALSE)) %&gt;% 
  dplyr::group_by(month) %&gt;%
  dplyr::summarise(frequency   = n(),
                   tot_revenue = sum(amount) %&gt;% format_dollar()) %&gt;%  
  set_names(c(&quot;Month&quot;, &quot;Total No Purchase&quot;, &quot;Total Revenue&quot;)) %&gt;%
  kbl() %&gt;% 
  kable_material_dark()</code></pre>
<table class=" lightable-material-dark" style="font-family: &quot;Source Sans Pro&quot;, helvetica, sans-serif; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Month
</th>
<th style="text-align:right;">
Total No Purchase
</th>
<th style="text-align:left;">
Total Revenue
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
January
</td>
<td style="text-align:right;">
11145
</td>
<td style="text-align:left;">
$724,107
</td>
</tr>
<tr>
<td style="text-align:left;">
February
</td>
<td style="text-align:right;">
9939
</td>
<td style="text-align:left;">
$645,154
</td>
</tr>
<tr>
<td style="text-align:left;">
March
</td>
<td style="text-align:right;">
9753
</td>
<td style="text-align:left;">
$636,577
</td>
</tr>
<tr>
<td style="text-align:left;">
April
</td>
<td style="text-align:right;">
7974
</td>
<td style="text-align:left;">
$515,791
</td>
</tr>
<tr>
<td style="text-align:left;">
May
</td>
<td style="text-align:right;">
9689
</td>
<td style="text-align:left;">
$633,212
</td>
</tr>
<tr>
<td style="text-align:left;">
June
</td>
<td style="text-align:right;">
10756
</td>
<td style="text-align:left;">
$697,014
</td>
</tr>
<tr>
<td style="text-align:left;">
July
</td>
<td style="text-align:right;">
11036
</td>
<td style="text-align:left;">
$717,223
</td>
</tr>
<tr>
<td style="text-align:left;">
August
</td>
<td style="text-align:right;">
11198
</td>
<td style="text-align:left;">
$726,921
</td>
</tr>
<tr>
<td style="text-align:left;">
September
</td>
<td style="text-align:right;">
10689
</td>
<td style="text-align:left;">
$694,433
</td>
</tr>
<tr>
<td style="text-align:left;">
October
</td>
<td style="text-align:right;">
11181
</td>
<td style="text-align:left;">
$725,320
</td>
</tr>
<tr>
<td style="text-align:left;">
November
</td>
<td style="text-align:right;">
10674
</td>
<td style="text-align:left;">
$698,273
</td>
</tr>
<tr>
<td style="text-align:left;">
December
</td>
<td style="text-align:right;">
10966
</td>
<td style="text-align:left;">
$709,964
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>resp &lt;- resp_df_raw %&gt;% 
  select(customer_id, response) %&gt;% 
  setNames(c(&quot;customer_id&quot;, &quot;campaign&quot;))

pre_rfm_df &lt;- trans_df %&gt;% 
  left_join(resp, by = &quot;customer_id&quot;) %&gt;% 
  mutate(campaign = case_when(campaign == 1 ~ 1,
                                TRUE ~ 0))</code></pre>
<pre class="r"><code>pre_rfm_df %&gt;% 
  mutate(month = month(date)) %&gt;%
  group_by(campaign) %&gt;% 
  summarise(no_pur  = n(),
            avg_pur = mean(amount)   %&gt;% format_dollar(),
            med_pur = median(amount) %&gt;% format_dollar(),
            sd_pur  = sd(amount)     %&gt;% format_dollar()) %&gt;% 
  ungroup() %&gt;% 
  set_names(c(&quot;Campaign&quot;, &quot;No Purchase&quot;, &quot;Averge Purchase&quot;, &quot;Median Purchase&quot;, &quot;Std Purchase&quot;)) %&gt;%
  kbl() %&gt;% 
  kable_material_dark()</code></pre>
<table class=" lightable-material-dark" style="font-family: &quot;Source Sans Pro&quot;, helvetica, sans-serif; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
Campaign
</th>
<th style="text-align:right;">
No Purchase
</th>
<th style="text-align:left;">
Averge Purchase
</th>
<th style="text-align:left;">
Median Purchase
</th>
<th style="text-align:left;">
Std Purchase
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
111158
</td>
<td style="text-align:left;">
$64
</td>
<td style="text-align:left;">
$64
</td>
<td style="text-align:left;">
$23
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
13842
</td>
<td style="text-align:left;">
$69
</td>
<td style="text-align:left;">
$69
</td>
<td style="text-align:left;">
$21
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>library(reshape2)</code></pre>
<pre><code>## 
## Attaching package: &#39;reshape2&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:tidyr&#39;:
## 
##     smiths</code></pre>
<pre class="r"><code>cmpg_df &lt;- pre_rfm_df %&gt;% 
  mutate(campaign = case_when(campaign == 1 ~ 1,
                                TRUE ~ 0),
         week = ceiling_date(date, unit = &quot;week&quot;)) %&gt;%
  group_by(week, campaign) %&gt;% 
  summarise(no_pur = n(),
            revenue = sum(amount))</code></pre>
<pre><code>## `summarise()` has grouped output by &#39;week&#39;. You can override using the `.groups` argument.</code></pre>
<pre class="r"><code>cmpg_pur_df &lt;- cmpg_df %&gt;% 
  dcast(week ~ campaign, value.var = &quot;no_pur&quot;) 
cmpg_rev_df &lt;- cmpg_df %&gt;% 
  dcast(week ~ campaign, value.var = &quot;revenue&quot;) 

cmpg_vis_df &lt;- left_join(cmpg_pur_df, cmpg_rev_df, by = &quot;week&quot;) %&gt;% 
  set_names(c(&quot;week&quot;, &quot;no_pur_N&quot;, &quot;no_pur_Y&quot;, &quot;sales_N&quot;, &quot;sales_Y&quot;)) %&gt;% 
  filter_by_time(.start_date = &quot;2011-07&quot;, 
                   .end_date = &quot;2015-03-01&quot;,
                   .date_var = week) %&gt;% 
  
  group_by(week) %&gt;%
  summarise(no_pur_N_cmpg  = sum(no_pur_N),
            no_pur_Y_cmpg  = sum(no_pur_Y),
            revenue_N_cmpg = sum(sales_N),
            revenue_Y_cmpg = sum(sales_Y)) %&gt;% ungroup()
  
cmpg_vis_df %&gt;% plot_time_series(week, 
                                 revenue_Y_cmpg,
                                 .smooth = FALSE,
                                 .interactive = FALSE) </code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>cmpg_vis_df %&gt;% plot_time_series(week, 
                                 revenue_N_cmpg,
                                 .smooth = FALSE,
                                 .interactive = FALSE) </code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-22-2.png" width="672" /></p>
<pre class="r"><code>cmpg_vis_df %&gt;% plot_time_series(week, 
                                 no_pur_Y_cmpg,
                                 .smooth = FALSE,
                                 .interactive = FALSE) </code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-22-3.png" width="672" /></p>
<pre class="r"><code>cmpg_vis_df %&gt;% plot_time_series(week, 
                                 no_pur_N_cmpg,
                                 .smooth = FALSE,
                                 .interactive = FALSE) </code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-22-4.png" width="672" /></p>
<p>the RFM scoring possess less weight on customer next purchasing prediction value. By means, although RFM metrics yet remain its purpose of segmenting superior to sleeping customer but it would be a good predictor index for estimating who would be most likely to purchase in given time period. As the stationary means it occurs by chance.</p>
<hr />
</div>
</div>
