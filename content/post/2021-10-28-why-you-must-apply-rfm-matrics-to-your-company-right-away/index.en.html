---
title: K-Nearest Neighbour Diabetes Prediction 
author: Package Build
date: '2021-10-26'
slug: K-Nearest Neighbour Diabetes Prediction
categories:
  - R
tags:
  - Classification 
  - Health Care
subtitle: ''
summary: ''
authors: []
lastmod: '2021-10-26T03:10:43+09:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
draft: FALSE
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<p>In this post, we will learn and absorb k-nearest neighbour classification Machine Learning with the diabetes prediction case study. Despite the simplicity, the k-nearest neighbour classification is very power technique to have in our tool bag. To recall from Prof. Steven from Standford University - it is known that 1/3 of the classification problems the best tool will be k-NN classification. My aim here is to investigate and deliver why that is and, more importantly, what aspects of data that can make kNN classifiers more challenging and unattractive in comparison to other ML classifiers.</p>
<p>Contents:</p>
<p>Study</p>
<ul>
<li>What is K-Nearest Neighbour function
<ul>
<li>What are K-NN assumptions?</li>
<li>Parameters</li>
<li>Variety of distance criteria</li>
<li>Optimal Number of Neighbors</li>
</ul></li>
<li>Cons
<ul>
<li>Homogeneous feature</li>
<li>Curse of Dimensionality</li>
<li>Imbalanced data problems</li>
<li>Outlier Sensitivity</li>
</ul></li>
<li>Advanced topics
<ul>
<li>Kd tree algorithm</li>
</ul></li>
</ul>
<p>Objective of Analysis:</p>
<div id="what-is-k-nearest-neighbour-function" class="section level2">
<h2>1.0 What is K-Nearest Neighbour function</h2>
<p><strong>Description:</strong> For given test point, the k-NN algorithm identifies the <strong>k</strong> most similar training points and finds the most common label amongst them. This label is used as a prediction for the test point.</p>
<p><span class="math display">\[
(x_1, y_1), ..., (x_n, y_n)
\]</span></p>
<p><span class="math display">\[
x_i \in R^d, y_i \in \{0, 1\}
\]</span></p>
<p>k-NN is called a lazy learner algorithm. Instead of function learning from the training set, it stores all the available data and at the time of new data appears, it performs a classification into a category label.</p>
<p>When performing classification, the new data points are classified based on the similarity measure (i.e closeness, proximity, or distance) of neighbouring classes within the radius of vector space given as k parameter.</p>
<p>*Majority Vote of k nearest neighbours so the algorithm will unanimously decide bases on the neighbouring classes.</p>
<p>For every training data points within the local boundary set by k (size or number of nearest neighbour), their distance to<br />
The distance from test data can be lower or equal to distance of<br />
The data points within the boundary of selected k parameter,</p>
</div>
<div id="application-of-k-nn" class="section level2">
<h2>Application of k-NN</h2>
<p>k-NN can be used in both regression and classification predictive problems. However, when it comes to industrial problems, the classification usage is highly advised.</p>
<p>k-NN, as well as perceptron, is often used as a benchmark for more complex classifiers like the SVM and the ANN.</p>
<div id="recommnedation-system" class="section level3">
<h3>Recommnedation System</h3>
<p>Companies like Amazon or Netflix use KNN when recommending books or movies for us to read and watch.</p>
</div>
<div id="complex-image-classification" class="section level3">
<h3>Complex Image Classification</h3>
<p>Beside one thing that is lately becoming very fashionable is to learn a distance matrix. Use of deep convolutional neural networks to learn representations for images, then from what is learned k-NN algorithm is applied. This is how face classification algorithms work at Facebook, Google and more.</p>
<p><span class="math display">\[
\hat{Y}(x) = \frac{1}{k} \sum_{x_i \in N_{k(x)}} y_i 
\]</span></p>
<p><span class="math display">\[
d(x_i,x_j)^2 = ||x_i - x_j||^2 = \sum_{k=1}^d (x_{ik} - x_{jk})^2
\]</span></p>
</div>
<div id="what-are-k-nn-assumptions" class="section level3">
<h3>What are K-NN assumptions?</h3>
<p>“Similar inputs have similar labels”; k-NN assumes that the user has a way to compute distances that reflect meaningful dissimilarities.</p>
<p>In its basic level,</p>
<blockquote>
<p>When Choosing kNN algorithm, the distance metric using to identify the k-Nearest Neighbour</p>
</blockquote>
</div>
<div id="parameters" class="section level3">
<h3>Parameters</h3>
<p>The k-NN algorthum has one parameter, <strong>k</strong>, the number of the local nearest neighbour (i.e. size of the neighbourhood). Changing this parameter will affect the algorithm’s classification accuracy and can also help to mange noisy data.</p>
<ul>
<li><p>k is a measure of complexity</p></li>
<li><p>Typical choice of k is an odd numbers: ranges from 1 upto 15 or about.</p></li>
<li><p>Directly influence the decision boundary:</p>
<ul>
<li>k = 1, can be jiggly as it picks up on any kind of little movement in the data set.</li>
<li>Higher the k, the smoother the decision boundary will be.</li>
</ul></li>
</ul>
<p>Key point to notate</p>
<ul>
<li>k is a measure of complexity
<ul>
<li>Lower the k -&gt; more complexed decision boundary; many islands may appear</li>
<li>Higher the k -&gt; smoother decision boundary; more susceptible to noise</li>
</ul></li>
</ul>
</div>
<div id="variety-of-distance-criteria" class="section level3">
<h3>Variety of distance criteria</h3>
<p>he key behind the k-NN is to use appropriate distance matrix ; most typical distance functions are:</p>
<p><span class="math display">\[
dist(x,z) = (\sum_{x =1}^d |(x_{\perp}))
\]</span>
* Euclidean distance (Natural Choice)</p>
<ul>
<li><p>Mahalanobis distance</p></li>
<li><p>Minkowski distance</p></li>
</ul>
<p>*Diabetes Predcition on kNN algorithm</p>
<p>Berif Description:</p>
<p><span class="math display">\[
\hat{C}_k(x) =  \underset{g}{\mathrm{argmax}} \ \ \hat{p}_{kg}(x)
\]</span></p>
</div>
</div>
<div id="variable-explaination" class="section level2">
<h2>Variable Explaination</h2>
<p>[1] npreg : number of pregnancies.</p>
<p>[2] glu : plasma glucose concentration in an oral glucose tolerance test.</p>
<p>[3] bp : diastolic blood pressure (mm Hg).</p>
<p>[4] skin : triceps skin fold thickness (mm).</p>
<p>[5] bmi : body mass index (weight in kg/(height in m)^2).</p>
<p>[6] ped : diabetes pedigree function.</p>
<p>[7] age : age in years</p>
<p>[8] type : Yes or No, for diabetic according to WHO criteria.</p>
</div>
<div id="feature-engineering-ideas-or" class="section level2">
<h2>Feature Engineering IDeas or</h2>
<p>Biological Age: f(BMI)</p>
<pre class="r"><code>setwd(&quot;/Users/seunghyunsung/Desktop/rdata/ML_basic/SVM_RF/diabetes_prediction&quot;)

suppressMessages(library(class))
suppressMessages(library(kknn))
suppressMessages(library(randomForest))
suppressMessages(library(e1071))
suppressMessages(library(caret))
suppressMessages(library(reshape2))
suppressMessages(library(ggplot2))
suppressMessages(library(kernlab))
suppressMessages(library(MASS))
suppressMessages(library(skimr))

library(ggplot2)
library(GGally)</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;GGally&#39;:
##   method from   
##   +.gg   ggplot2</code></pre>
<pre class="r"><code># conditions (options &amp; set.seed)
options(digits = 3, scipen = 100)

set.seed(42)
# load the data
data(Pima.tr)
data(Pima.te)

dim(Pima.tr) # [1] 200   8</code></pre>
<pre><code>## [1] 200   8</code></pre>
<pre class="r"><code>dim(Pima.te) # [1] 332   8</code></pre>
<pre><code>## [1] 332   8</code></pre>
<div id="combine-train-and-teat-data-set" class="section level3">
<h3>Combine Train and Teat Data Set</h3>
<ul>
<li>Algorithm cannot get a glimpse of the future (Test data for this case)</li>
</ul>
<pre class="r"><code># Train dataset 
Pima.tr$type &lt;- ifelse(Pima.tr$type == &quot;No&quot;, 0, 1)
prop.table(table(Pima.tr$type))</code></pre>
<pre><code>## 
##    0    1 
## 0.66 0.34</code></pre>
<pre class="r"><code># Test dataset 
Pima.te$type &lt;- ifelse(Pima.te$type == &quot;No&quot;, 0, 1)
prop.table(table(Pima.te$type))</code></pre>
<pre><code>## 
##     0     1 
## 0.672 0.328</code></pre>
<pre class="r"><code># Combine  
Pima = rbind(Pima.tr, Pima.te)</code></pre>
</div>
</div>
<div id="eda" class="section level2">
<h2>EDA</h2>
<p>K-NN classifier algorithm</p>
<p>To do:</p>
<ul>
<li><p>Feature Scaling: K-NN and many ML algorithm relies on the distance (norm) of the matrix.</p></li>
<li><p>Outlier detection: K-NN algorithm is highly sensitive to outliers</p></li>
</ul>
<pre class="r"><code>partition(skim(Pima))</code></pre>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">npreg</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3.52</td>
<td align="right">3.31</td>
<td align="right">0.00</td>
<td align="right">1.00</td>
<td align="right">2.00</td>
<td align="right">5.00</td>
<td align="right">17.00</td>
<td align="left">▇▂▂▁▁</td>
</tr>
<tr class="even">
<td align="left">glu</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">121.03</td>
<td align="right">31.00</td>
<td align="right">56.00</td>
<td align="right">98.75</td>
<td align="right">115.00</td>
<td align="right">141.25</td>
<td align="right">199.00</td>
<td align="left">▂▇▅▃▂</td>
</tr>
<tr class="odd">
<td align="left">bp</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">71.51</td>
<td align="right">12.31</td>
<td align="right">24.00</td>
<td align="right">64.00</td>
<td align="right">72.00</td>
<td align="right">80.00</td>
<td align="right">110.00</td>
<td align="left">▁▂▇▆▁</td>
</tr>
<tr class="even">
<td align="left">skin</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">29.18</td>
<td align="right">10.52</td>
<td align="right">7.00</td>
<td align="right">22.00</td>
<td align="right">29.00</td>
<td align="right">36.00</td>
<td align="right">99.00</td>
<td align="left">▅▇▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">bmi</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">32.89</td>
<td align="right">6.88</td>
<td align="right">18.20</td>
<td align="right">27.87</td>
<td align="right">32.80</td>
<td align="right">36.90</td>
<td align="right">67.10</td>
<td align="left">▃▇▃▁▁</td>
</tr>
<tr class="even">
<td align="left">ped</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.50</td>
<td align="right">0.34</td>
<td align="right">0.09</td>
<td align="right">0.26</td>
<td align="right">0.42</td>
<td align="right">0.66</td>
<td align="right">2.42</td>
<td align="left">▇▃▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">age</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">31.61</td>
<td align="right">10.76</td>
<td align="right">21.00</td>
<td align="right">23.00</td>
<td align="right">28.00</td>
<td align="right">38.00</td>
<td align="right">81.00</td>
<td align="left">▇▂▁▁▁</td>
</tr>
<tr class="even">
<td align="left">type</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.33</td>
<td align="right">0.47</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="left">▇▁▁▁▃</td>
</tr>
</tbody>
</table>
<pre class="r"><code>ggpairs(Pima[ ,c(1:8)], aes(color = type %&gt;% as.factor(), alpha =0.75), lower = list(continuous = &quot;smooth&quot;)) + theme_bw() + 
  labs(title = &quot;Diabetes&quot;) +
  theme(plot.title = element_text(face =&#39;bold&#39;, color =&#39;black&#39;, hjust=0.5, size =12)) </code></pre>
<pre><code>## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>Pima_melt &lt;- reshape2::melt(Pima, id.var =&quot;type&quot;)

ggplot2::ggplot(data = Pima_melt,
                aes(x = type, y = value, group = type)) +
  geom_boxplot() +
  facet_wrap(~variable, ncol=2) </code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<div id="eucliden-distance-calculation" class="section level3">
<h3>Eucliden Distance Calculation</h3>
<pre class="r"><code>euclideanDist &lt;- function(a, b){
  d = 0
  for(i in c(1:(length(a)-1) ))
  {
    d = d + (a[[i]]-b[[i]])^2
  }
  d = sqrt(d)
  return(d)
}</code></pre>
</div>
<div id="knn-prediction-function" class="section level3">
<h3>KNN Prediction Function</h3>
<pre class="r"><code>knn_predict &lt;- function(test_data, train_data, k_value){
  pred &lt;- c()  #empty pred vector 
  #LOOP-1
  for(i in c(1:nrow(test_data))){   #looping over each record of test data
    eu_dist =c()          #eu_dist &amp; eu_char empty  vector
    eu_char = c()
    good = 0              #good &amp; bad variable initialization with 0 value
    bad = 0
    
    #LOOP-2-looping over train data 
    for(j in c(1:nrow(train_data))){
 
      #adding euclidean distance b/w test data point and train data to eu_dist vector
      eu_dist &lt;- c(eu_dist, euclideanDist(test_data[i,], train_data[j,]))
 
      #adding class variable of training data in eu_char
      eu_char &lt;- c(eu_char, as.character(train_data[j,][[6]]))
    }
    
    eu &lt;- data.frame(eu_char, eu_dist) #eu dataframe created with eu_char &amp; eu_dist columns
 
    eu &lt;- eu[order(eu$eu_dist),]       #sorting eu dataframe to gettop K neighbors
    eu &lt;- eu[1:k_value,]               #eu dataframe with top K neighbors
 
    #Loop 3: loops over eu and counts classes of neibhors.
    for(k in c(1:nrow(eu))){
      if(as.character(eu[k,&quot;eu_char&quot;]) == &quot;g&quot;){
        good = good + 1
      }
      else
        bad = bad + 1
    }
 
    # Compares the no. of neighbors with class label good or bad
    if(good &gt; bad){          #if majority of neighbors are good then put &quot;g&quot; in pred vector
 
      pred &lt;- c(pred, &quot;g&quot;)
    }
    else if(good &lt; bad){
                   #if majority of neighbors are bad then put &quot;b&quot; in pred vector
      pred &lt;- c(pred, &quot;b&quot;)
    }
    
  }
  return(pred) #return pred vector
}</code></pre>
</div>
<div id="accuracy-calculation" class="section level3">
<h3>Accuracy Calculation</h3>
<pre class="r"><code>accuracy &lt;- function(test_data){
  correct = 0
  for(i in c(1:nrow(test_data))){
    if(test_data[i,6] == test_data[i,7]){ 
      correct = correct+1
    }
  }
  accu = correct/nrow(test_data) * 100  
  return(accu)
}</code></pre>
<p>confusion of correlation with the cause
Just seeing the correlation by not the cause (dependency) -&gt; leads to false interpretation (mistake)</p>
<p>string togehter these events into narratives</p>
</div>
</div>
