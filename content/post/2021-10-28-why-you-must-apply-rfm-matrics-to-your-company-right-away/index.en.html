---
title: K-Nearest Neighbour Diabetes Prediction 
author: Package Build
date: '2021-10-26'
slug: K-Nearest Neighbour Diabetes Prediction
categories:
  - R
tags:
  - Classification 
  - Health Care
subtitle: ''
summary: ''
authors: []
lastmod: '2021-10-26T03:10:43+09:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
draft: FALSE
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<p>Contents:</p>
<p>Study</p>
<ul>
<li>What is K-Nearest Neighbour function
<ul>
<li>What are K-NN assumptions?</li>
<li>Parameters</li>
<li>Variety of distance criteria</li>
<li>Optimal Number of Neighbors</li>
</ul></li>
<li>Cons
<ul>
<li>Homogeneous feature</li>
<li>Curse of Dimensionality</li>
<li>Imbalanced data problems</li>
<li>Outlier Sensitivity</li>
</ul></li>
<li>Advanced topics
<ul>
<li>Kd tree algorithm</li>
</ul></li>
</ul>
<div id="section" class="section level2">
<h2></h2>
<p><span class="math display">\[
\hat{C}_k(x) =  \underset{g}{\mathrm{argmax}} \ \ \hat{p}_{kg}(x)
\]</span></p>
</div>
<div id="variable-explaination" class="section level2">
<h2>Variable Explaination</h2>
<p>[1] npreg : number of pregnancies.</p>
<p>[2] glu : plasma glucose concentration in an oral glucose tolerance test.</p>
<p>[3] bp : diastolic blood pressure (mm Hg).</p>
<p>[4] skin : triceps skin fold thickness (mm).</p>
<p>[5] bmi : body mass index (weight in kg/(height in m)^2).</p>
<p>[6] ped : diabetes pedigree function.</p>
<p>[7] age : age in years</p>
<p>[8] type : Yes or No, for diabetic according to WHO criteria.</p>
</div>
<div id="feature-engineering-ideas-or" class="section level2">
<h2>Feature Engineering IDeas or</h2>
<p>Biological Age: f(BMI)</p>
<pre class="r"><code>setwd(&quot;/Users/seunghyunsung/Desktop/rdata/ML_basic/SVM_RF/diabetes_prediction&quot;)

suppressMessages(library(class))
suppressMessages(library(kknn))
suppressMessages(library(randomForest))
suppressMessages(library(e1071))
suppressMessages(library(caret))
suppressMessages(library(reshape2))
suppressMessages(library(ggplot2))
suppressMessages(library(kernlab))
suppressMessages(library(MASS))
suppressMessages(library(skimr))

library(ggplot2)
library(GGally)</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;GGally&#39;:
##   method from   
##   +.gg   ggplot2</code></pre>
<pre class="r"><code># conditions (options &amp; set.seed)
options(digits = 3, scipen = 100)

set.seed(42)
# load the data
data(Pima.tr)
data(Pima.te)

dim(Pima.tr) # [1] 200   8</code></pre>
<pre><code>## [1] 200   8</code></pre>
<pre class="r"><code>dim(Pima.te) # [1] 332   8</code></pre>
<pre><code>## [1] 332   8</code></pre>
<div id="combine-train-and-teat-data-set" class="section level3">
<h3>Combine Train and Teat Data Set</h3>
<pre class="r"><code># Train dataset 
Pima.tr$type &lt;- ifelse(Pima.tr$type == &quot;No&quot;, 0, 1)
prop.table(table(Pima.tr$type))</code></pre>
<pre><code>## 
##    0    1 
## 0.66 0.34</code></pre>
<pre class="r"><code># Test dataset 
Pima.te$type &lt;- ifelse(Pima.te$type == &quot;No&quot;, 0, 1)
prop.table(table(Pima.te$type))</code></pre>
<pre><code>## 
##     0     1 
## 0.672 0.328</code></pre>
<pre class="r"><code># Combine  
Pima = rbind(Pima.tr, Pima.te)</code></pre>
</div>
</div>
<div id="eda" class="section level2">
<h2>EDA</h2>
<p>K-NN classifier algorithm</p>
<p>To do:</p>
<ul>
<li><p>Feature Scaling: K-NN and many ML algorithm relies on the distance (norm) of the matrix.</p></li>
<li><p>Outlier detection: K-NN algorithm is highly sensitive to outliers</p></li>
</ul>
<pre class="r"><code>partition(skim(Pima))</code></pre>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">npreg</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3.52</td>
<td align="right">3.31</td>
<td align="right">0.00</td>
<td align="right">1.00</td>
<td align="right">2.00</td>
<td align="right">5.00</td>
<td align="right">17.00</td>
<td align="left">▇▂▂▁▁</td>
</tr>
<tr class="even">
<td align="left">glu</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">121.03</td>
<td align="right">31.00</td>
<td align="right">56.00</td>
<td align="right">98.75</td>
<td align="right">115.00</td>
<td align="right">141.25</td>
<td align="right">199.00</td>
<td align="left">▂▇▅▃▂</td>
</tr>
<tr class="odd">
<td align="left">bp</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">71.51</td>
<td align="right">12.31</td>
<td align="right">24.00</td>
<td align="right">64.00</td>
<td align="right">72.00</td>
<td align="right">80.00</td>
<td align="right">110.00</td>
<td align="left">▁▂▇▆▁</td>
</tr>
<tr class="even">
<td align="left">skin</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">29.18</td>
<td align="right">10.52</td>
<td align="right">7.00</td>
<td align="right">22.00</td>
<td align="right">29.00</td>
<td align="right">36.00</td>
<td align="right">99.00</td>
<td align="left">▅▇▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">bmi</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">32.89</td>
<td align="right">6.88</td>
<td align="right">18.20</td>
<td align="right">27.87</td>
<td align="right">32.80</td>
<td align="right">36.90</td>
<td align="right">67.10</td>
<td align="left">▃▇▃▁▁</td>
</tr>
<tr class="even">
<td align="left">ped</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.50</td>
<td align="right">0.34</td>
<td align="right">0.09</td>
<td align="right">0.26</td>
<td align="right">0.42</td>
<td align="right">0.66</td>
<td align="right">2.42</td>
<td align="left">▇▃▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">age</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">31.61</td>
<td align="right">10.76</td>
<td align="right">21.00</td>
<td align="right">23.00</td>
<td align="right">28.00</td>
<td align="right">38.00</td>
<td align="right">81.00</td>
<td align="left">▇▂▁▁▁</td>
</tr>
<tr class="even">
<td align="left">type</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.33</td>
<td align="right">0.47</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="left">▇▁▁▁▃</td>
</tr>
</tbody>
</table>
<pre class="r"><code>ggpairs(Pima[ ,c(1:8)], aes(color = type %&gt;% as.factor(), alpha =0.75), lower = list(continuous = &quot;smooth&quot;)) + theme_bw() + 
  labs(title = &quot;Diabetes&quot;) +
  theme(plot.title = element_text(face =&#39;bold&#39;, color =&#39;black&#39;, hjust=0.5, size =12)) </code></pre>
<pre><code>## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero

## Warning in cor(x, y): the standard deviation is zero</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>Pima_melt &lt;- reshape2::melt(Pima, id.var =&quot;type&quot;)

ggplot2::ggplot(data = Pima_melt,
                aes(x = type, y = value, group = type)) +
  geom_boxplot() +
  facet_wrap(~variable, ncol=2) </code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<div id="eucliden-distance-calculation" class="section level3">
<h3>Eucliden Distance Calculation</h3>
<pre class="r"><code>euclideanDist &lt;- function(a, b){
  d = 0
  for(i in c(1:(length(a)-1) ))
  {
    d = d + (a[[i]]-b[[i]])^2
  }
  d = sqrt(d)
  return(d)
}</code></pre>
</div>
<div id="knn-prediction-function" class="section level3">
<h3>KNN Prediction Function</h3>
<pre class="r"><code>knn_predict &lt;- function(test_data, train_data, k_value){
  pred &lt;- c()  #empty pred vector 
  #LOOP-1
  for(i in c(1:nrow(test_data))){   #looping over each record of test data
    eu_dist =c()          #eu_dist &amp; eu_char empty  vector
    eu_char = c()
    good = 0              #good &amp; bad variable initialization with 0 value
    bad = 0
    
    #LOOP-2-looping over train data 
    for(j in c(1:nrow(train_data))){
 
      #adding euclidean distance b/w test data point and train data to eu_dist vector
      eu_dist &lt;- c(eu_dist, euclideanDist(test_data[i,], train_data[j,]))
 
      #adding class variable of training data in eu_char
      eu_char &lt;- c(eu_char, as.character(train_data[j,][[6]]))
    }
    
    eu &lt;- data.frame(eu_char, eu_dist) #eu dataframe created with eu_char &amp; eu_dist columns
 
    eu &lt;- eu[order(eu$eu_dist),]       #sorting eu dataframe to gettop K neighbors
    eu &lt;- eu[1:k_value,]               #eu dataframe with top K neighbors
 
    #Loop 3: loops over eu and counts classes of neibhors.
    for(k in c(1:nrow(eu))){
      if(as.character(eu[k,&quot;eu_char&quot;]) == &quot;g&quot;){
        good = good + 1
      }
      else
        bad = bad + 1
    }
 
    # Compares the no. of neighbors with class label good or bad
    if(good &gt; bad){          #if majority of neighbors are good then put &quot;g&quot; in pred vector
 
      pred &lt;- c(pred, &quot;g&quot;)
    }
    else if(good &lt; bad){
                   #if majority of neighbors are bad then put &quot;b&quot; in pred vector
      pred &lt;- c(pred, &quot;b&quot;)
    }
    
  }
  return(pred) #return pred vector
}</code></pre>
</div>
<div id="accuracy-calculation" class="section level3">
<h3>Accuracy Calculation</h3>
<pre class="r"><code>accuracy &lt;- function(test_data){
  correct = 0
  for(i in c(1:nrow(test_data))){
    if(test_data[i,6] == test_data[i,7]){ 
      correct = correct+1
    }
  }
  accu = correct/nrow(test_data) * 100  
  return(accu)
}</code></pre>
</div>
</div>
