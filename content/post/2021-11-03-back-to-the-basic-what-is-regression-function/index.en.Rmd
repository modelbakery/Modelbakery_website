---
title: The Practical Approach of Multiple Linear Regression Model
author: Package Build
date: '2021-11-03'
slug: back-to-the-basic-what-is-regression-function
categories:
  - R
tags:
  - Academic
  - Regression
  - Model Interpretation
  - R
subtitle: 'A general class of regression models and how to interpret these models'
summary: ''
authors: []
lastmod: '2021-11-03T13:30:59+09:00'
featured: no 
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
draft: FALSE
---

As we fit regression models, we might need to make a choice between linear and nonlinear regression models. Often, in the field of statistics, the region between these forms are explained by two folds: the strength of the assumption and interpretability of variables' relationship their weights used to make the prediction. 

"Will my model actually be used?"- the one of the first questions as analyst or researcher must ask them selves (Frank E 2008$^1$). If anything, why can we not simply trust the model have been established with high performance? 

In this post, we will get the pulse of "correct" model criteria and the assumptions associated with linear regression model.


## Agenda 

[1] The assumptions associated with linear regression model and where have they oriented from?

[2] Variable __asymmetric reduction__: variable transformation & Parallel Coordinates Plot (PCP) 

[3] __Outlier detection__: Cook's Distance Outlier Dignostic and Treatment 

[4] Understand Forward & Backward __Stepwise Regression__



## Why linear regression?

The biggest advantage of linear regression models is linearity: Due to its strong assumption it makes the estimation procedure simple and easy to understand the interpretation on the relationship and weights of the individual features. For example, in medical field, despite the fact that having R-squared above 0.7 is extremely rare, linear regression is yet favourable approach. This is because it is not only important to predict the clinical outcome, but also to quantify the influence of the drug and other features into account in an interpretable way. 

The application of linear regression is endless and it is often this generalised and interpretable statistical learning to be our benchmark on how extent we need to flex our assumption. 

>“If I had an hour to solve a problem and my life depended on the solution, I would spend the first 55 minutes determining the proper question to ask … for once I know the proper question, I could solve the problem in less than five minutes.”
>—Albert Einstein


## Why linear regression?


## Load Libraries

```{r, message=FALSE, warning=FALSE}
library(MASS)
library(ISLR)

library(tidyverse)
library(psych)
library(skimr)
library(car)
library(janitor)
library(tidyquant)
library(lares)
library(leaps)
library(broom)

library(caret)
library(tidymodels)
library(vip) 

library(hrbrthemes)
library(GGally)
library(viridis)
library(kableExtra)
library(gridExtra)
```
 

```{r}
data(Boston)

describe(Boston) %>% kbl() %>% kable_minimal()
```

## Understanding Data: Boston Housing Data 

#### Aim of the Analysis 

The Boston Housing data set was analysed by Harrison and Rubinfeld, who wanted to find out whether "clean air" had an influence on house prices. 
 
#### Segmenting Variables: Macro to Micro 

[1] Macro{External Influence} 

__chas__: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

__age__: proportion of owner-occupied units built prior to 1940.


[2] Industry/land/district 

__zn__: proportion of residential land zoned for lots over 25,000 sq.ft.

__indus__: proportion of non-retail business acres per town.

__dis__: weighted mean of distances to five Boston employment centres.

__rad__: index of accessibility to radial highways.


[3] Population 

__lstat__: lower status of the population (percent).

__black__: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.

__ptratio__: pupil-teacher ratio by town.


[3] Cause/result

__nox__: nitrogen oxides concentration (parts per 10 million)

__tax__: full-value property-tax rate per \$10,000.

__crim__: per capita crime rate by town.


[4] Direct influence 

__rm__: average number of rooms per dwelling.


[5] Target variable

__medv__: median value of owner-occupied homes in \$1000s.



## Distribution of Target Variable 

```{r}
# Data preparation: logarithm target medv 
lBoston <- Boston %>% 
  mutate(log_medv = log(medv))
  
# Distribution of target medv 
P_medv <- ggplot(Boston, aes(x = medv)) +
  geom_density(aes(x = medv, y = ..density..)) +
    geom_density(color="black", lwd =1.5, fill = viridisLite::viridis(n = 20)[3], alpha = 0.8)  +
    stat_function(fun=dnorm, 
                args = list(mean=mean(Boston$medv),
                            sd=sd(Boston$medv)),
                color=viridisLite::viridis(n = 20)[20], lwd = 1) +
  theme_minimal()

# Distribution of logarithm target medv 
P_lmedv <- lBoston %>% 
  ggplot(aes(x = log_medv)) +
  geom_density(aes(x = log_medv, y = ..density..)) +
    geom_density(color="black", lwd =1.5, fill = viridisLite::viridis(n = 20)[3], alpha = 0.8)  +
    stat_function(fun=dnorm, 
                args = list(mean=mean(lBoston$log_medv),
                            sd=sd(lBoston$log_medv)),
                color=viridisLite::viridis(n = 20)[20], lwd = 1) +
  theme_minimal()

# Shapiro-Wilk normality test: Both rejects the Null Hypothesis stating that the target data does not fit the normal distribution with 95% confidence. Significant departure from normality was found. 
shapiro.test(Boston$medv)$p.value
shapiro.test(lBoston$log_medv)$p.value

par(mfrow = c(1,2))
qqnorm(Boston$medv); qqline(Boston$medv, col =2)
qqnorm(lBoston$log_medv); qqline(lBoston$log_medv, col =2)

grid.arrange(P_medv, P_lmedv)
```
By looking at the p-values, the log transform departure less significant from normality hence I will carry logarithm medv. The R squared estimatation will be computed after inversion using exponent function. <Must Remember>


```{r, echo = FALSE}
generator_hist_facet <- function(data, bins = 20, ncol = 5,
                                 fct_reorder = FALSE, 
                                 fct_rev = FALSE,
                                 fill = palette_light()[[3]],
                                 color = "white", scale = "free") {
      data_factored <- data %>% 
        gather(key = key, value = value, factor_key = TRUE) 
      
    if(fct_reorder) {
      data_factored <- data_factored %>% 
        # Trick to get variables in alphabetical order 
        mutate(key = as.character(key) %>% as.factor())
    }
      
    if(fct_rev) {
      data_factored <- data_factored %>% 
        mutate(key = fct_rev(key))
    }
      
    g <- data_factored %>% 
      ggplot(aes(x = value, group = key)) +
      geom_histogram(bins = bins, fill = fill, color = color) +
      facet_wrap(~key, ncol = ncol, scale = scale) +
      theme_tq()
    
    return(g)
}
```


## Visualisation: Boxplot & Histogram 

* Scaling or transformation should be considered 

* Outlier treatment should be considered 

```{r}
# Quick boxplot:
boxplot(Boston)

# Customised facet histogram: can be found on my GitHub :)  
generator_hist_facet(lBoston, bins = 15, fill = viridisLite::viridis(n = 20)[1])
```


## GGally Visualisation 

```{r, message=FALSE, warning=FALSE}
# Data preparation for GGally visualisation with observation split at meidan Y value. 
Boston_medv_median <- Boston %>% 
  mutate(median_medv_cutoff = ifelse(medv > median(medv), 1, 0),
         median_medv_cutoff = as.factor(median_medv_cutoff)) 

# GGally ggpairs plot for variables upto age 
GGally::ggpairs(Boston_medv_median[ ,c(14,1:7, 15)],
                aes(color=median_medv_cutoff, alpha = 0.75),
                lower = list(continous = "smooth")) +
  theme_bw() +
  scale_color_viridis_d(alpha = 0.7) +
  scale_fill_viridis_d(alpha = 0.7) +
  labs(
    title    = "Correlation Matrix Plot for Boston Housing: crim ~ age",
    subtitle = "Promising Separation of High & Low target Subgroups by rm & indus Predictor Vars",
    caption  = "medv > median(medv): Yellow
                medv ≤ meidan(medv): Purple",
  ) + 
  theme(plot.title = element_text(face = 'bold',
                                  colour = 'black',
                                  hjust = 0.5, size = 12)) 

# GGally ggpairs plot for variables from dis ~ lstat 
GGally::ggpairs(Boston_medv_median[ ,c(14,8:13,15)],
                aes(colour=median_medv_cutoff, alpha = 0.75),
                lower = list(continous = "smooth")) +
  theme_bw() +
  labs(
    title    = "Correlation Matrix Plot for Boston Housing: dis ~ lstat",
    subtitle = "Promising Separation of High & Low target Subgroups by black & lstat Predictor Vars",
    caption  = "medv > median(medv): Yellow
                medv ≤ meidan(medv): Purple",
  ) +
  scale_color_viridis_d(alpha = 0.7) +
  scale_fill_viridis_d(alpha = 0.7) +
  theme(plot.title = element_text(face = 'bold',
                                  colour = 'black',
                                  hjust = 0.5, size = 12))

# Gally parallel coordinates plot {PCP}
Boston_medv_median %>%
  arrange(desc(median_medv_cutoff)) %>%
  ggparcoord(
    columns = 1:14, groupColumn = 15,
    scale="uniminmax",
    showPoints = FALSE,
    alphaLines = 1
    ) + 
  scale_color_viridis(discrete=TRUE)+
    labs(
    title = "Parallel Coordinates Plot for Boston House Price: Minmax Standardised",
    subtitle = "Strong negative connection observed between lstat & medv",
    caption = "medv > median(medv): Yellow
               medv ≤ meidan(medv): Purple",
    y = "Minmax scaled medv",
    x = "") +
  theme_ipsum()+
  theme(
    legend.position="default",
    plot.title = element_text(size=15)
  )

```

In order to highlight the relation of medv (target Y) to the remaining 13 variables, I have split the observations with medv > median(medv) as __yellow__ lines and rest as the __purple__ line. Some of the variables seem to be strongly related. The most obvious relation is the negative dependence between lstat and medv.For better graphical representations, the variables have been minmax scaled over the interval [0,1]. There are variables where the observations are highly concnetrated at the low region, close to zero. By means, it makes sense to consider transformation of the original data. 

* crim -> lcrim (logarithm)
  + Taking the logarithm makes the variable's distribution more symmetric. Its median and the mean have moved closer to each other then they were for the original crim. 

* zn -> zn (binning)
  + There is a large number of zero values. 
  + There is a noticable non-linear negative relationship with crim, noticed on PCP. Almost all the observations for which zn = 0, have a high percapita crime rate, and vice versa. 
  + On the concern of house price prediction, the correlation says that there seems to be no clear linear relationship with medv. However, the residential land over its common size would obviously lead to higher property price. 
  
* indus -> lindus (logarithm)
  + Negative correlation with target medv observed. The relationship between the logarithms of both variables seems to be almost linear. The negative relation might result from the noise non-retail business sometimes generates and other pollution aspects. 
  + Strong linear correlation with nox. Greater the proportion of non-retail business, more likely to cause a pollution hence the high nox concentration. 
  
* chas 
  + There are some doubt that Charles River influences the house price. That said, the districts close to the Charles River would likely to influence other factors such as the pupil/teacher ratio or the proportion of non-retail business acres. Hence, their relation may be pure coincidence or some form of hierchical interaction. 
  
* rm -> lrm (logarithm)
  + The number of rooms per dwelling is a direct measure of the size of the houses. Thus, I suspect rm to be strongly correlated with the target medv.  
  + Outlier treatment needed. 
  
* age -> age^2.5/10000
  + There is no clear sign of rlationship with the house price. On top of that, usually the time, date, age factor has connection with multiple variables, which likely to inflate the VIF factor and rise multicollinearity issue. 
  + Left skewed 
  
* dis -> ldis (logarithm)
  + The scatter plot showed hardly any linear relationship with the target, house price. However, there is a noticable non-linear relation and its logarithm does seem to preserve distance influence on the house price.
  
* rad -> lrad (logarithm)
  + The one obvious thing one can observe is the subgroups of districts containing rad value, which are lose to the respective group's mean. The boxplot from ggpair reveals that the mean value of these subgroups stays relatively the same for both low and high price of the house. 
  + Its correlation with tax exceeds 0.9 

* tax -> ltax (logarithm)
  + Likewise to rad, there is a noticable subgroups on the distribution of tax value. The scatter plot shows downward curve with increase in the tax rate. There are presence of outliers in lower value subgroup but the mean difference within these groups seems to be significant to retain its information.   

* ptratio -> exp(0.4*ptratio)/1000
  + The ggpair boxplot indicates negative relation with medv. 
  + The mean values of its subgroups depart significantly by high and low regime of medv.
  + The kurtosis of of its red distribution {medv < median(medv)} is highly noticeable hence likely be potential predictor variable on medv.  
  
* black -> black/100
  + https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8
  
* lstat -> lstat^0.5
  + Of all the variables, lstat exhibits the strongest negative relation with medv (PCP & ggpairs). 
  + Taking the square root removes all the outliers and gains better relation with log(mdv)
  


## Feature Transformation & Asymmetry reduction 

```{r}
# feature transformation/scaling 
Boston_transform <- Boston %>% 
  summarise(lcrim    = log(crim + 1),
            zn       = zn/10,
            lindus   = log(indus),
            chas     = chas,
            lnox     = log(nox + 1),
            lrm      = log(rm),
            sqage    = (age^2.5)/10000,
            ldis     = log(dis),
            lrad     = log(rad),
            ltax     = log(tax),
            eptratio = exp(0.4*ptratio)/1000,
            black    = black/100,
            srlstat  = lstat^0.5,
            lmedv    = log(medv))

# Boxplots for all of the transformed variables + scaling 
Boston_transform %>% 
  summarise_all(funs(x = scale(., center = TRUE, scale = TRUE))) %>% 
  setNames(c("crim", "zn", "indus", "chas", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "black", "lstat", "medv")) %>% 
  gather(key = key, value = value, factor_key = TRUE) %>% 
  ggplot(aes(key, value)) +
  geom_boxplot(fill = viridisLite::viridis(n = 20)[1], colour = "yellow") +
  labs(
    title = "Transformed Boston Housing Data",
    x = "",
    y = "Scaled Index"
  ) +
  theme_dark() 

```

Since most of the variables exhibit an asymmetry with a higher density on the left-hand side (right skewed), the logarithm transformations are proposed on most of the variables. Taking the logarithm helps to reduce the asymmetry, pushing lower values to move further away from each other whereas the distance between greater values is reduced. 

In comparison to original data, these transformed variables are more asymmetric and where logarithm was applied, the less outliers were shown. The question is: is the upper and/or extreme always an outlier? I believe there is no straight answer hence I will carry out this transformed data set and outlier treated data set as I go along the analysis.  


#### Correlation: lares package 

```{r}
# Target variable correlation plot
corr_plot <- corr_var(Boston_transform, 
  lmedv 
); corr_plot

# X's cross correlation plot: display only significant correlations (at 5% level)
corr_cross_plot <- corr_cross(Boston_transform %>% dplyr::select(-lmedv), 
  max_pvalue = 0.05,
  rm.na = FALSE
); corr_cross_plot

# Capturing data into table: 
# @ TBD {Bind with variable coefficient statistics tbl}
corr_tbl <- corr_plot$data %>% dplyr::select(variables, corr)
corr_cross_tbl <-corr_cross_plot$data %>% dplyr::select(key, label, corr)
```

#### Linear Model: Original VS Transformed Data Set

```{r}
# linear model on all variables 
lm_fit_00edit <- lm(medv~., data = Boston); summary(lm_fit_00edit)

# linear model on all transformed variables 
lm_fit_01edit <- lm(lmedv~., data = Boston_transform); summary(lm_fit_01edit)

glance(lm_fit_00edit) %>% kbl %>% kable_material()
glance(lm_fit_01edit) %>% kbl %>% kable_material()
```
From the summary statistics computed with glance function, it is clear that asymmetric reduced data having positive influence on the overall accuracy of the linear model and OLS approach. 

* Higher R-squared & adj.R-squared, and lower AIC & BIC 

* Less number of statistically significant variables depicted (t_value > 2) on the model. Optimal parameters   


Depending on the nature of task (target variable: Boston House Price) the predictors will vary in weight of importance between the interpretation and the accuracy. Having skewed data will have tendency to hide these importance and its information have on the House Price. By default, it is our challenge to align these distributions in respect to, ideally normally distributed, Y in order to protect loss on valuable information. 

Understanding that skewness and scale are different is important, it can be easily seen from its result on t-statistics and correlation coefficients. It can be seem non-ordinary to split the numeric target variable into two subgroups during the data interpretation. However, as seen above, it adds more visual interpretation than having correlation matrix alone. I believe this approach can also be powerful in recognising hierarchical relationships between the variable X's. 


## Summary table for variable coefficient & correlation 

One of the aspect in regression is the multi-colinearity issue. Although a thorough investigation in variable correlation can be useful but it merely highlights on confounders and its effect sizes. I like to imagine multi-colinearity as the entaglements of features in given dimensional space, and without the any knowelgde on the subject, it holds risk of miss-interpreting the what's in behind the scene, as I like to phrase ultimate "route cause". 

Multi-colinearity and robust regression is one of my keen interest and would like to discover further and share. Hopefully in the next post :)  

```{r}
var_names <- colnames(Boston_transform %>% dplyr::select(lmedv, everything()))

summary_relationship_tbl <- summary(lm_fit_00edit)$coefficients[,c(1,3:4)] %>% 
  as_tibble() %>% 
  cbind(var_names) %>% 
  dplyr::select(var_names, everything()) %>% 
  left_join(corr_tbl, by = c("var_names" = "variables")) %>% 
  left_join(corr_cross_tbl, by = c("var_names" = "key")) %>% 
  setNames(c("var_names", "Estimate", "t_stats", "p_value", "corr", "label", "cross_corr")) %>% 
  arrange(cross_corr)

summary_relationship_tbl %>% kbl() %>% kable_material()
```



## Outlier Detection: On Fluential Observation by Cook's Distance 

The family of regression in nature sensitive to abnormal data points and there are many techniques to remove the outliers from the data set. In linear approach, the estimated Y's are predicted by conditional average of X's coordinates in individual instances. Hence it is a voting mechanism of all the points to estimate the line of the best fit. I like to imagine it as every data points tries to pull the line towards it selves, and some does more successfully than others: leverage. 

```{r, echo=FALSE}
outlier_treatment <- function(x, na.rm = T) {
  x[x > quantile(x,.95,na.rm = na.rm)]<- quantile(x, .95,na.rm = na.rm)
  x[x < quantile(x,.05,na.rm = na.rm)]<- quantile(x, .05,na.rm = na.rm)
  return(x)
}
```

```{r, echo=FALSE, result=TRUE, message=FALSE}
Boston_example <- Boston_transform

P_example1 <- Boston_example %>% 
  ggplot(aes(lcrim, lmedv)) +
  geom_point(colour = viridisLite::viridis(n = 20)[1]) +
  geom_smooth(method = "lm", se = FALSE, colour = "yellow") +
  theme_tq_dark()

P_example2 <- Boston_example %>% 
  mutate(lcrim_outlier_treat_at95 = lcrim %>% outlier_treatment()) %>% 
  ggplot(aes(lcrim_outlier_treat_at95, lmedv)) +
  geom_point(colour = viridisLite::viridis(n = 20)[1]) +
  geom_smooth(method = "lm", se = FALSE, colour = "yellow") +
  theme_tq_dark()
  
gridExtra::grid.arrange(P_example1, P_example2, ncol = 2)
```

In other words, if there are presence of extreme outlier situated far from the mean and rest of the coordinates, this leverage is likely to cause regression line distortion and thus the coefficient estimates. 

A general notation for Cook's Distance is given below:

$$
D_i = \frac{\sum_{j=1}^n(\hat{y_j} - \hat{y_{j(i)}})^2}{constant}  
$$

The equation denotes, how much influential effect does i observation have and it is computed by the difference of $\ \hat{y_j}$ (with i) and $\ \hat{y_{j(i)}}$ (without i) at all the $\ y$ values. If the parameter estimate change a great deal upon suspect outlier (leverage) deletion from the least squares estimate calculation, the point is said to be influential. 


```{r}
# plot Cook's distance
cooksd_outlier_detect <- function(lm_fit){
  cooksd <- cooks.distance(lm_fit)
  influential <- as.numeric(names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))])
  return(influential)
}
cooksd_outlier_detect(lm_fit_01edit)

cooksd_outlier_plot <- function(lm_fit){
  cooksd <- cooks.distance(lm_fit)
  plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance") +
    # I have relaxed the Cook's distance assumption to retain as much information as possible. 
    abline(h = 4*mean(cooksd, na.rm=T), col="red") +
    text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")
}
cooksd_outlier_plot(lm_fit_01edit)
```

A general rule of thumb is to investigate any point that is more than 3 times the mean of all the distances. However, the log(y) (House Price) contain noticable outliers and there is a possibility that these target value upon deletion could be the reason for parameter estimate change. If the influential observation happen to be the Y actual value contribution, we would need to pull this set of observations separatelty and investigate further while making the prediction without this influential detected set.

```{r}
# Vectorise outliers NA converter 
detect_outliers_to_na <- function(x) {
  if (missing(x)) stop("The argument x needs a vector.")
  if (!is.numeric(x)) stop("The argument x must be numeric")
  data_tbl <- tibble(data = x)
  limits_tbl <- data_tbl %>% 
    summarise(
      quantile_lo = quantile(data, probs = 0.25, na.rm = TRUE),
      quantile_hi = quantile(data, probs = 0.75, na.rm = TRUE),
      iqr = IQR(data, na.rm = TRUE),
      limit_lo = quantile_lo - 1.5 * iqr,
      limit_hi = quantile_hi + 1.5 * iqr
    )
    output_tbl <- data_tbl %>% 
    mutate(outlier = ifelse(data < limits_tbl$limit_lo, NA,
                            ifelse(data > limits_tbl$limit_hi, NA,
                                   data))
           )
  return(output_tbl$outlier)
}
```

```{r, echo=FALSE}
detect_outliers_logic <- function(x) {
  if (missing(x)) stop("The argument x needs a vector.")
  if (!is.numeric(x)) stop("The argument x must be numeric")
  data_tbl <- tibble(data = x)
  limits_tbl <- data_tbl %>% 
    summarise(
      quantile_lo = quantile(data, probs = 0.25, na.rm = TRUE),
      quantile_hi = quantile(data, probs = 0.75, na.rm = TRUE),
      iqr = IQR(data, na.rm = TRUE),
      limit_lo = quantile_lo - 1.5 * iqr,
      limit_hi = quantile_hi + 1.5 * iqr
    )
  output_tbl <- data_tbl %>% 
    mutate(outlier = case_when(
      data < limits_tbl$limit_lo ~ TRUE,
      data > limits_tbl$limit_hi ~ TRUE,
      TRUE ~ FALSE
    ))
  return(output_tbl$outlier)
}

outlier_treatment <- function(x, na.rm = T) {
  x[x > quantile(x,.95,na.rm = na.rm)]<- quantile(x, .95,na.rm = na.rm)
  x[x < quantile(x,.05,na.rm = na.rm)]<- quantile(x, .05,na.rm = na.rm)
  return(x)
}
```

### Outlier Treatment #1: Predictor Varaibles IQR out-of-bound Outliers Treatment at Detected Influential Observation

```{r}
# Add Row Id 
Boston_row_tbl <- Boston_transform %>% 
  mutate(row_id = row_number()) %>% 
  select(row_id, everything())

# Add flag to the Cook's Distance Diagnosed Observation
Boston_outlier_1tbl <- Boston_row_tbl %>% 
  mutate(row_id = row_number(),
         cooksd_flag = case_when(row_id %in% cooksd_outlier_detect(lm_fit_01edit) ~ 1,
                                TRUE ~ 0)) %>% 
  # filter these observation set 
  filter(cooksd_flag == 1) %>%  
  # Detect IQR out-of-bound outliers and convert to NA values  
  mutate_at(vars(lcrim:srlstat), ~ detect_outliers_to_na(.)) %>% 
  # Replace NA to mean value 
  mutate_at(vars(lcrim:srlstat), function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))) %>% 
  # bind with rest of the observation 
  bind_rows(Boston_row_tbl[-cooksd_outlier_detect(lm_fit_01edit),]) %>% 
  arrange(row_id) %>% 
  select(-c(cooksd_flag)) 

# 02Edit linear model: Outlier Detection + Treatment Iteration 
lm_fit_02edit <- lm(lmedv~.-row_id, data = Boston_outlier_1tbl); summary(lm_fit_02edit)

# Treated 3 observation out of 24: the 21 observations are influential data points attributed by the Y values. 
cooksd_outlier_detect(lm_fit_02edit)
cooksd_outlier_plot(lm_fit_02edit)
plot(lm_fit_02edit)


# Remove thes Cook's distance diagnosed outliers 
Boston_outlier_2tbl <- Boston_outlier_1tbl %>% 
  filter(!row_id %in% cooksd_outlier_detect(lm_fit_02edit)) 

```


```{r}
# 03Edit linear model: Cook's Distance Outiler Deletion 
lm_fit_03edit <- lm(lmedv~.-row_id, data = Boston_outlier_2tbl); summary(lm_fit_03edit)
cooksd_outlier_plot(lm_fit_03edit)
plot(lm_fit_03edit)

glance(lm_fit_03edit) %>% kbl %>% kable_material()
```

## Data Partitioning: train:test (7:3)

```{r}
Boston_prepared <- Boston_outlier_2tbl %>% select(-row_id)

set.seed(42)
tidy_split <- initial_split(Boston_prepared, prop = .7,
                            strata = lmedv)
Boston_tr <- training(tidy_split)
Boston_te <- testing(tidy_split)
```


## Variable Selection: Best Subsets Selection & Optimal Dimension

Optimal number of variables: consideration process by computation of best model with each subsets. 

Finding: 

[1] Method: "Forward" and "Backward"
Minimum Sum of error and both maxmium adjusted R squared and R squared achieved at total number of variables (13). However at minimal BIC, penalised AIC, suggests using 10 variables is optimal goodness of fit. 

[2] Method: "Sequential Replacement" (stepwise)
Suggests the variable importance of srlstat and black have on the estmimate parameters. 

```{r, message=FALSE, warning=FALSE}
regsubfit_full <- regsubsets(lmedv~.,
                        data= Boston_tr, 
                        intercept = T,
                        method = c("exhaustive"),
                        nvmax = 13)

regfit_summary <- summary(regsubfit_full)

regfit_summary$outmat %>% kbl %>% kable_material()
```

```{r}
opt_adjr2 <- which.max(regfit_summary$adjr2)
opt_bic <- which.min(regfit_summary$bic)

plot(regfit_summary$adjr2, xlab="No. of Predictor Variables", ylab="adj.RSquare", type="l") +points(opt_adjr2, regfit_summary$adjr2[opt_adjr2], col = "red", cex=2, pch=20)

plot(regfit_summary$bic, xlab="No. of Predictor Variables", ylab="BIC", type="l") +
  points(opt_bic, regfit_summary$bic[opt_bic], col = "red", cex=2, pch=20)

plot(regfit_summary$rss, xlab="No. of Predictor Variables", ylab="RSS", type="l") 
```
#### VIP: Variable of Importance 

```{r}
summary_relationship_tbl %>% filter(label %>% str_detect("lnox"))
```

```{r}
# Stepwise function 
lmfit <- lm(lmedv~., Boston_tr)
forward  <- stats::step(lmfit, direction  = "forward", trace = 0)
backward <- stats::step(lmfit, direction  = "backward", trace = 0)
stepwise <- stats::step(lmfit, direction  = "both", trace = 0)

# Backward and stepwise outputs the same prediction result 
method <- c("forward", "backward", "stepwise")
cbind(method, rbind(
  forward  %>% glance(),
  backward %>% glance(),
  stepwise %>% glance())) %>% kbl %>% kable_material()

# Step Selection method: VIP visualisation 
p1_forw <- vip(forward, num_features = length(coef(backward)), 
          geom = "point", horizontal = FALSE, 
          mapping = aes_string(color = "Sign", size = 5)) + 
  scale_color_viridis_d() +
    theme(legend.position="default") 

p2_back <- backward %>% vip(num_features = length(coef(backward)), 
          geom = "point", horizontal = FALSE,
          mapping = aes_string(color = "Sign", size = 5)) + 
  scale_color_viridis_d() +
    theme(legend.position="default") 

grid.arrange(p1_forw + labs(caption  = "VIP: Forward Selection Algorithm by AIC | Positive/Negative interaction (Yellow/Purple)"), 
             p2_back + labs(caption  = "VIP: Backward Selection Algorithm by AIC | Positive/Negative interaction (Yellow/Purple)"))
```

## Linear Model Predcition 

Steps 

[1] Split Data: 7:3 (Done)

[2] Feature Engineering: (Done)

[3] Specify a Model: Linear Regression 

[4] Recipe (Refer from regsubset)

* Forward stepwise: All 13 variables  

* Backward stepwise: 11 variables 

* Interaction TBD (next post)

```{r}
# Recipes 
forw_recipe <- recipe(lmedv~., data = Boston_tr)

backw_recipe <- recipe(backward$call$formula, data = Boston_tr) 

poly_recipe <- recipe(lmedv ~ lrm + lrad + lcrim + chas + lnox + ldis + ltax + 
    eptratio + black + srlstat, data = Boston_tr) %>%
  step_poly(lrm, lrad) %>% 
  prep()


# Specify a Model: Linear Regression 
lm_model <- linear_reg() %>% 
            set_engine('lm') %>% 
            set_mode('regression') 

# Work flow 
forw_fit <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(forw_recipe) %>% 
  last_fit(split = tidy_split)

backw_fit <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(backw_recipe) %>% 
  last_fit(split = tidy_split)

poly_fit <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(poly_recipe) %>% 
  last_fit(split = tidy_split)


names_fit <- c("forw_fit", "forw_fit", "backw_fit", "backw_fit", "poly_fit", "poly_fit")
cbind(names_fit, rbind(
  forw_fit %>% collect_metrics(),
  backw_fit %>% collect_metrics(),
  poly_fit %>% collect_metrics()
)) %>% arrange(.metric) %>% kbl() %>% kable_material()

```


[3] Prediction 

```{r}
forw_results <- forw_fit %>% 
                 collect_predictions() %>% 
  left_join(forw_recipe %>% prep() %>% bake(new_data = Boston_te), by = "lmedv") %>% 
  mutate(residue = lmedv - .pred) %>% 
  select(-c(id, .config))

backw_results <- backw_fit %>% 
                 collect_predictions() %>% 
  left_join(backw_recipe %>% prep() %>% bake(new_data = Boston_te), by = "lmedv") %>% 
  mutate(residue = lmedv - .pred) %>% 
  select(-c(id, .config))

poly_results <- poly_fit %>% 
                 collect_predictions() %>% 
  left_join(poly_recipe %>% prep() %>% bake(new_data = Boston_te), by = "lmedv") %>% 
  mutate(residue = lmedv - .pred) %>% 
  select(-c(id, .config))

forw_results %>% 
  ggplot(mapping = aes(x = .pred, y = lmedv)) +
  geom_point(color = '#006EA1') +
  geom_abline(intercept = 0, slope = 1, color = 'orange') +
  labs(title = 'Multiple Linear Regression Results: Boston House Price Prediction',
       x = 'Predicted Price',
       y = 'Actual Price') +
  theme_tq()

forw_results %>% 
  ggplot(mapping = aes(x = .pred, y = residue)) +
  geom_point(color = '#006EA1') +
  geom_smooth(method = "lm", se = FALSE, color = 'orange') +
  labs(title = 'Residue Plot: Error Variance Check',
       x = 'Predicted Price',
       y = 'Error') +
  theme_tq()

backw_results %>% 
  ggplot(mapping = aes(x = .pred, y = lmedv)) +
  geom_point(color = '#006EA1') +
  geom_abline(intercept = 0, slope = 1, color = 'orange') +
  labs(title = 'Multiple Linear Regression Results: Boston House Price Prediction',
       x = 'Predicted Price',
       y = 'Actual Price') +
  theme_tq()

backw_results %>% 
  ggplot(mapping = aes(x = .pred, y = residue)) +
  geom_point(color = '#006EA1') +
  geom_smooth(method = "lm", se = FALSE, color = 'orange')  +
  labs(title = 'Residue Plot: Error Variance Check',
       x = 'Predicted Price',
       y = 'Error') +
  theme_tq()

poly_results %>% 
  ggplot(mapping = aes(x = .pred, y = lmedv)) +
  geom_point(color = '#006EA1') +
  geom_abline(intercept = 0, slope = 1, color = 'orange') +
  labs(title = 'Multiple Linear Regression Results: Boston House Price Prediction',
       x = 'Predicted Price',
       y = 'Actual Price') +
  theme_tq()

poly_results %>% 
  ggplot(mapping = aes(x = .pred, y = residue)) +
  geom_point(color = '#006EA1') +
  geom_smooth(method = "lm", se = FALSE, color = 'orange')  +
  labs(title = 'Residue Plot: Error Variance Check',
       x = 'Predicted Price',
       y = 'Error') +
  theme_tq()
```
```{r}
sum(forw_results$residue^2); mean(forw_results$residue)

sum(backw_results$residue^2); mean(backw_results$residue)

sum(poly_results$residue^2); mean(poly_results$residue)

```



Some Problems with R-squared

Previously, I demonstrated that you cannot use R-squared to conclude whether your model is biased. To check for this bias, you need to check your residual plots. Unfortunately, there are yet more problems with R-squared that we need to address.

Problem 1: R-squared increases every time you add an independent variable to the model. The R-squared never decreases, not even when it’s just a chance correlation between variables. A regression model that contains more independent variables than another model can look like it provides a better fit merely because it contains more variables.


Problem 2: When a model contains an excessive number of independent variables and polynomial terms, it becomes overly customized to fit the peculiarities and random noise in your sample rather than reflecting the entire population. Statisticians call this overfitting the model, and it produces deceptively high R-squared values and a decreased capability for precise predictions.

Fortunately for us, adjusted R-squared and predicted R-squared address both of these problems.

```{r}
Boston_final_lm_fit <- lm_model %>% 
                fit(lmedv~., data = Boston_tr)

cof <- round(Boston_final_lm_fit$fit$coefficients, 3)
n <- names(cof)
y = paste('Final Model:', 'y=',
          cof[1], '*', n[1], "+",
          cof[2], '*', n[2], '+',
          cof[3], '*', n[3], '+',
          cof[4], '*', n[4], '+',
          cof[5], '*', n[5], '+',
          cof[6], '*', n[6], '+',
          cof[7], '*', n[7], '+',
          cof[8], '*', n[8], '+',
          cof[9], '*', n[9], '+',
          cof[10], '*', n[10], '+',
          cof[11], '*', n[11], '+',
          cof[12], '*', n[12], '+',
          cof[13], '*', n[13], '+',
          cof[14], '*', n[14])
y
```

```{r, echo = FALSE}
generator_univar_importance_tbl <- function(dataframe, response) {
  
  if (sum(sapply(dataframe, function(x) {is.numeric(x) | is.factor(x)})) < ncol(dataframe)) {
    stop("Make sure that all variables are of class numeric/factor!")
  }
  
  # pre-allocate vectors
  varname <- c()
  vartype <- c()
  R2 <- c()
  R2_log <- c()
  R2_quad <- c()
  AIC <- c()
  AIC_log <- c()
  AIC_quad <- c()
  y <- dataframe[, response]
  # # # # # NUMERIC RESPONSE # # # # #
  if (is.numeric(y)) {
    
    for (i in 1:ncol(dataframe)) {
      
      x <- dataframe[ ,i]
      varname[i] <- names(dataframe)[i]
      
      if (class(x) %in% c("numeric", "integer")) {
        vartype[i] <- "numeric"
      } else {
        vartype[i] <- "categorical"
      }
      
      if (!identical(y, x)) {
        
        # linear: y ~ x
        R2[i] <- summary(lm(y ~ x))$r.squared 
        
        # log-transform: y ~ log(x)
        if (is.numeric(x)) { 
          if (min(x) <= 0) { # if y ~ log(x) for min(x) <= 0, do y ~ log(x + abs(min(x)) + 1)
            R2_log[i] <- summary(lm(y ~ log(x + abs(min(x)) + 1)))$r.squared
          } else {
            R2_log[i] <- summary(lm(y ~ log(x)))$r.squared
          }
        } else {
          R2_log[i] <- NA
        }
        
        # quadratic: y ~ x + x^2
        if (is.numeric(x)) { 
          R2_quad[i] <- summary(lm(y ~ x + I(x^2)))$r.squared
        } else {
          R2_quad[i] <- NA
        }
        
      } else {
        R2[i] <- NA
        R2_log[i] <- NA
        R2_quad[i] <- NA
      }
    }
    
    print(paste("Response variable:", response))
    
    data.frame(varname, 
               vartype, 
               R2 = round(R2, 3), 
               R2_log = round(R2_log, 3), 
               R2_quad = round(R2_quad, 3)) %>%
      mutate(max_R2 = pmax(R2, R2_log, R2_quad, na.rm = T)) %>%
      arrange(desc(max_R2))
    
    
    # # # # # CATEGORICAL RESPONSE # # # # #
  } else {
    
    for (i in 1:ncol(dataframe)) {
      
      x <- dataframe[ ,i]
      varname[i] <- names(dataframe)[i]
      
      if (class(x) %in% c("numeric", "integer")) {
        vartype[i] <- "numeric"
      } else {
        vartype[i] <- "categorical"
      }
      
      if (!identical(y, x)) {
        # linear: y ~ x
        AIC[i] <- summary(glm(y ~ x, family = "binomial"))$aic 
        
        # log-transform: y ~ log(x)
        if (is.numeric(x)) { 
          if (min(x) <= 0) { # if y ~ log(x) for min(x) <= 0, do y ~ log(x + abs(min(x)) + 1)
            AIC_log[i] <- summary(glm(y ~ log(x + abs(min(x)) + 1), family = "binomial"))$aic
          } else {
            AIC_log[i] <- summary(glm(y ~ log(x), family = "binomial"))$aic
          }
        } else {
          AIC_log[i] <- NA
        }
        
        # quadratic: y ~ x + x^2
        if (is.numeric(x)) { 
          AIC_quad[i] <- summary(glm(y ~ x + I(x^2), family = "binomial"))$aic
        } else {
          AIC_quad[i] <- NA
        }
        
      } else {
        AIC[i] <- NA
        AIC_log[i] <- NA
        AIC_quad[i] <- NA
      }
    }
    
    print(paste("Response variable:", response))
    
    data.frame(varname, 
               vartype, 
               AIC = round(AIC, 3), 
               AIC_log = round(AIC_log, 3), 
               AIC_quad = round(AIC_quad, 3)) %>%
      mutate(min_AIC = pmin(AIC, AIC_log, AIC_quad, na.rm = T)) %>%
      arrange(min_AIC)
  } 
}

```

```{r}
generator_univar_importance_tbl(Boston_tr, "lmedv")
```

# Reference 
Regression Modeling Strategies with Applications to Linear Models, Logistics Regression, and Survial Analysis (pg 4)