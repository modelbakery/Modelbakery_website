---
title: 'The Practical Approach of Multiple Linear Regression Model'
author: Package Build
date: '2021-11-03'
slug: back-to-the-basic-what-is-regression-function
categories:
  - R
tags:
  - Academic
  - Regression
  - Model Interpretation
  - R
subtitle: 'A general class of regression models and how to interpret these models'
summary: ''
authors: []
lastmod: '2021-11-03T13:30:59+09:00'
featured: no 
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
draft: TRUE
---

The concept I first learnt about multiple linear regression was too good to be true, to be applied in the real world data. This view I once had on linear regression differ greatly from one I have now. Simplicity is the best, but the work behind?- seeking deeply into the what's called "root cause" is simply a time investment. That said, the better we are with regression, the better we will understand the hidden problems and not be betrayed by algorithms.

In this post, I will be focusing on variable transformation, multicollinearity, VIF, adjusted R-squared, and variable selection to come up with acceptable accuracy without relaxing the assumption of linearity. 


Contents:

Study 

* What is multiple linear regression function 
  + Why linear regression?
  + What are its assumptions and difference with linear regression?
  + Estimation of the parameters by least squares
  + Assessing the accuracy of the Coefficient Estimates
  + Hypothesis Testing (t-test)
  + Assessing the Overall Accuracy of the Model 
  + Variable Selection 
  + Model Selection 

* Business Case Study 
  + Variable Interaction (Synergy Effect: Feature Engineering & Business Strategy)


## General notation for 

Its general notation:

f(x) = E(Y|X =x) 

Depending on the complexity of the function, we may be able to understand how each components Xj affects Y, and therefore the interpretable level of parameters with respect to independent X variables influencing on Y. 

Thus models, even within Regression, have many uses and those amongst them. Some of its models would be easier to interpret than others but stronger the assumptions and its limitations to follow. I mean there is really no "free-lunch" right? 

Before I begin explore the assumptions and algorithms of different regression models. Lets try to understand the general scheme of ideal function - explained by ...

* Conditional average 
So is there an ideal f(X)? 


## Why linear regression?

I always had this wonder. How do we know in what situation, the linear approximation is reasonable assumption to be made? - Well, most of the times never. Does that make linear regression any less vulable? Absolutley not, right? 

>“If I had an hour to solve a problem and my life depended on the solution, I would spend the first 55 minutes determining the proper question to ask … for once I know the proper question, I could solve the problem in less than five minutes.”
>—Albert Einstein

I believe that the more complicated the problem is, the more important its interpretation becomes. 
In other words, we should always consider the possibility of simpler way to approach any form of problems. 


## Load Libraries

```{r, message=FALSE, warning=FALSE}
library(MASS)
library(ISLR)

library(tidyverse)
library(psych)
library(skimr)
library(car)
library(janitor)
library(tidyquant)
library(lares)
library(leaps)
library(broom)

library(hrbrthemes)
library(GGally)
library(viridis)
library(kableExtra)
library(gridExtra)
```
 

```{r}
data(Boston)

describe(Boston) %>% kbl() %>% kable_minimal()
```

## Understanding Data: Boston Housing Data 

#### Aim of the Analysis 

The Boston Housing data set was analysed by Harrison and Rubinfeld, who wanted to find out whether "clean air" had an influence on house prices. 
 
#### Segmenting Variables: Macro to Micro 

[1] Macro{External Influence} 

__chas__: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

__age__: proportion of owner-occupied units built prior to 1940.


[2] Industry/land/district 

__zn__: proportion of residential land zoned for lots over 25,000 sq.ft.

__indus__: proportion of non-retail business acres per town.

__dis__: weighted mean of distances to five Boston employment centres.

__rad__: index of accessibility to radial highways.


[3] Population 

__lstat__: lower status of the population (percent).

__black__: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.

__ptratio__: pupil-teacher ratio by town.


[3] Cause/result

__nox__: nitrogen oxides concentration (parts per 10 million)

__tax__: full-value property-tax rate per \$10,000.

__crim__: per capita crime rate by town.


[4] Direct influence 

__rm__: average number of rooms per dwelling.


[5] Target variable

__medv__: median value of owner-occupied homes in \$1000s.



## Distribution of Target Variable 

```{r}
# Data preparation: logarithm target medv 
lBoston <- Boston %>% 
  mutate(log_medv = log(medv))
  
# Distribution of target medv 
P_medv <- ggplot(Boston, aes(x = medv)) +
  geom_density(aes(x = medv, y = ..density..)) +
    geom_density(color="black", lwd =1.5, fill = palette_light()[1])  +
    stat_function(fun=dnorm, 
                args = list(mean=mean(Boston$medv),
                            sd=sd(Boston$medv)),
                color="red", lwd = 1) +
  theme_minimal()

# Distribution of logarithm target medv 
P_lmedv <- lBoston %>% 
  ggplot(aes(x = log_medv)) +
  geom_density(aes(x = log_medv, y = ..density..)) +
    geom_density(color="black", lwd =1.5, fill = palette_light()[1])  +
    stat_function(fun=dnorm, 
                args = list(mean=mean(lBoston$log_medv),
                            sd=sd(lBoston$log_medv)),
                color="red", lwd = 1) +
  theme_minimal()

# Shapiro-Wilk normality test: Both rejects the Null Hypothesis stating that the target data does not fit the normal distribution with 95% confidence. Significant departure from normality was found. 
shapiro.test(Boston$medv)$p.value
shapiro.test(lBoston$log_medv)$p.value

par(mfrow = c(1,2))
qqnorm(Boston$medv); qqline(Boston$medv, col =2)
qqnorm(lBoston$log_medv); qqline(lBoston$log_medv, col =2)

grid.arrange(P_medv, P_lmedv)
```
By looking at the p-values, the log transform departure less significant from normality hence I will carry logarithm medv. The R squared estimatation will be computed after inversion using exponent function. <Must Remember>


```{r, echo = FALSE}
generator_hist_facet <- function(data, bins = 20, ncol = 5,
                                 fct_reorder = FALSE, 
                                 fct_rev = FALSE,
                                 fill = palette_light()[[3]],
                                 color = "white", scale = "free") {
      data_factored <- data %>% 
        gather(key = key, value = value, factor_key = TRUE) 
      
    if(fct_reorder) {
      data_factored <- data_factored %>% 
        # Trick to get variables in alphabetical order 
        mutate(key = as.character(key) %>% as.factor())
    }
      
    if(fct_rev) {
      data_factored <- data_factored %>% 
        mutate(key = fct_rev(key))
    }
      
    g <- data_factored %>% 
      ggplot(aes(x = value, group = key)) +
      geom_histogram(bins = bins, fill = fill, color = color) +
      facet_wrap(~key, ncol = ncol, scale = scale) +
      theme_tq()
    
    return(g)
}
```


## Visualisation: Boxplot & Histogram 

* Scaling or transformation should be considered 

* Outlier treatment should be considered 

```{r}
# Quick boxplot:
boxplot(Boston)

# Customised facet histogram: can be found on my GitHub :)  
generator_hist_facet(lBoston)
```


## GGally Visualisation 
```{r, message=FALSE, warning=FALSE}
# Data preparation for GGally visualisation with observation split at meidan Y value. 
Boston_medv_median <- Boston %>% 
  mutate(median_medv_cutoff = ifelse(medv > median(medv), 1, 0),
         median_medv_cutoff = as.factor(median_medv_cutoff)) 

# GGally ggpairs plot for variables upto age 
GGally::ggpairs(Boston_medv_median[ ,c(14,1:7, 15)],
                aes(color=median_medv_cutoff, alpha = 0.75),
                lower = list(continous = "smooth")) +
  theme_bw() +
  labs(title = "prostate cancer") +
  theme(plot.title = element_text(face = 'bold',
                                  colour = 'black',
                                  hjust = 0.5, size = 12))

# GGally ggpairs plot for variables from dis ~ lstat 
GGally::ggpairs(Boston_medv_median[ ,c(14,8:15)],
                aes(color=median_medv_cutoff, alpha = 0.75),
                lower = list(continous = "smooth")) +
  theme_bw() +
  labs(title = "prostate cancer") +
  theme(plot.title = element_text(face = 'bold',
                                  colour = 'black',
                                  hjust = 0.5, size = 12))

# Gally parallel coordinates plot {PCP}
Boston_medv_median %>%
  arrange(desc(median_medv_cutoff)) %>%
  ggparcoord(
    columns = 1:14, groupColumn = 15,
    scale="uniminmax",
    showPoints = FALSE,
    alphaLines = 1
    ) + 
  scale_color_viridis(discrete=TRUE)+
    labs(
    title = "Parallel Coordinates Plot for Boston House Price: Minmax Standardised",
    subtitle = "Strong negative connection observed between lstat & medv",
    caption = "medv > median(medv): Yellow
               medv ≤ meidan(medv): Purple",
    y = "Minmax scaled medv",
    x = "") +
  theme_ipsum()+
  theme(
    legend.position="default",
    plot.title = element_text(size=15)
  )

```

In order to highlight the relation of medv (target Y) to the remaining 13 variables, I have split the observations with medv > median(medv) as __yellow__ lines and rest as the __purple__ line. Some of the variables seem to be strongly related. The most obvious relation is the negative dependence between lstat and medv.For better graphical representations, the variables have been minmax scaled over the interval [0,1]. There are variables where the observations are highly concnetrated at the low region, close to zero. By means, it makes sense to consider transformation of the original data. 

* crim -> lcrim (logarithm)
  + Taking the logarithm makes the variable's distribution more symmetric. Its median and the mean have moved closer to each other then they were for the original crim. 

* zn -> zn (binning)
  + There is a large number of zero values. 
  + There is a noticable non-linear negative relationship with crim, noticed on PCP. Almost all the observations for which zn = 0, have a high percapita crime rate, and vice versa. 
  + On the concern of house price prediction, the correlation says that there seems to be no clear linear relationship with medv. However, the residential land over its common size would obviously lead to higher property price. 
  
* indus -> lindus (logarithm)
  + Negative correlation with target medv observed. The relationship between the logarithms of both variables seems to be almost linear. The negative relation might result from the noise non-retail business sometimes generates and other pollution aspects. 
  + Strong linear correlation with nox. Greater the proportion of non-retail business, more likely to cause a pollution hence the high nox concentration. 
  
* chas 
  + There are some doubt that Charles River influences the house price. That said, the districts close to the Charles River would likely to influence other factors such as the pupil/teacher ratio or the proportion of non-retail business acres. Hence, their relation may be pure coincidence or some form of hierchical interaction. 
  
* rm -> lrm (logarithm)
  + The number of rooms per dwelling is a direct measure of the size of the houses. Thus, I suspect rm to be strongly correlated with the target medv.  
  + Outlier treatment needed. 
  
* age -> age^2.5/10000
  + There is no clear sign of rlationship with the house price. On top of that, usually the time, date, age factor has connection with multiple variables, which likely to inflate the VIF factor and rise multicollinearity issue. 
  + Left skewed 
  
* dis -> ldis (logarithm)
  + The scatter plot showed hardly any linear relationship with the target, house price. However, there is a noticable non-linear relation and its logarithm does seem to preserve distance influence on the house price.
  
* rad -> lrad (logarithm)
  + The one obvious thing one can observe is the subgroups of districts containing rad value, which are lose to the respective group's mean. The boxplot from ggpair reveals that the mean value of these subgroups stays relatively the same for both low and high price of the house. 
  + Its correlation with tax exceeds 0.9 

* tax -> ltax (logarithm)
  + Likewise to rad, there is a noticable subgroups on the distribution of tax value. The scatter plot shows downward curve with increase in the tax rate. There are presence of outliers in lower value subgroup but the mean difference within these groups seems to be significant to retain its information.   

* ptratio -> exp(0.4*ptratio)/1000
  + The ggpair boxplot indicates negative relation with medv. 
  + The mean values of its subgroups depart significantly by high and low regime of medv.
  + The kurtosis of of its red distribution {medv < median(medv)} is highly noticeable hence likely be potential predictor variable on medv.  
  
* black -> black/100
  + https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8
  
* lstat -> lstat^0.5
  + Of all the variables, lstat exhibits the strongest negative relation with medv (PCP & ggpairs). 
  + Taking the square root removes all the outliers and gains better relation with log(mdv)
  


## Feature Transformation & Asymmetry reduction 

```{r}
# feature transformation/scaling 
Boston_transform <- Boston %>% 
  summarise(lcrim    = log(crim),
            zn       = zn/10,
            lindus   = log(indus),
            chas     = chas,
            lnox     = log(nox),
            lrm      = log(rm),
            sqage    = (age^2.5)/10000,
            ldis     = log(dis),
            lrad     = log(rad),
            ltax     = log(tax),
            eptratio = exp(0.4*ptratio)/1000,
            black    = black/100,
            srlstat  = lstat^0.5,
            lmedv    = log(medv))

# Boxplots for all of the transformed variables + scaling 
Boston_transform %>% 
  summarise_all(funs(x = scale(., center = TRUE, scale = TRUE))) %>% 
  setNames(c("crim", "zn", "indus", "chas", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "black", "lstat", "medv")) %>% 
  gather(key = key, value = value, factor_key = TRUE) %>% 
  ggplot(aes(key, value)) +
  geom_boxplot(fill = viridisLite::viridis(n = 20)[1], colour = "yellow") +
  labs(
    title = "Transformed Boston Housing Data",
    x = "",
    y = "Scaled Index"
  ) +
  theme_dark() 

```

Since most of the variables exhibit an asymmetry with a higher density on the left-hand side (right skewed), the logarithm transformations are proposed on most of the variables. Taking the logarithm helps to reduce the asymmetry, pushing lower values to move further away from each other whereas the distance between greater values is reduced. 

In comparison to original data, these transformed variables are more asymmetric and where logarithm was applied, the less outliers were shown. The question is: is the upper and/or extreme always an outlier? I believe there is no straight answer hence I will carry out this transformed data set and outlier treated data set as I go along the analysis.  

#### Correlation: lares package 

```{r}
# Target variable correlation plot
corr_info <- corr_var(Boston_transform, 
  lmedv 
); corr_info

# X's cross correlation plot: display only significant correlations (at 5% level)
corr_cross_info <- corr_cross(Boston_transform %>% dplyr::select(-lmedv), 
  max_pvalue = 0.05,
  rm.na = FALSE
); corr_cross_info

# Capturing data into table 
corr_tbl <- corr_info$data %>% dplyr::select(variables, corr)
corr_cross_tbl <-corr_cross_info$data %>% dplyr::select(key, label, corr)
```

#### Linear Model: Original VS Transformed Data Set

```{r}
# linear model on all variables 
lm_fit_00edit <- lm(medv~., data = Boston); summary(lm_fit_00edit)

# linear model on all transformed variables 
lm_fit_01edit <- lm(lmedv~., data = Boston_transform); summary(lm_fit_01edit)
summary_01edit <- summary(lm_fit_01edit)

```
We can clearly notice here the positive influence of asymmetry reduction has on the overall accuracy of the linear model. 

* Higher R-squared (+0.0244) & adj.R-squared (+0.025) 

* Less number of statistically significant variables depicted (t_value > 2) on the model. Optimal parameters   


```{r}
glance(lm_fit_00edit) %>% kbl %>% kable_material()

glance(lm_fit_01edit) %>% kbl %>% kable_material()
```

  


```{r}
var_names <- colnames(Boston_transform %>% dplyr::select(lmedv, everything()))

# Summary table for variable coefficient & correlation 
summary_relationship_tbl <- summary(lm_fit_00edit)$coefficients[,c(1,3:4)] %>% 
  as_tibble() %>% 
  cbind(var_names) %>% 
  dplyr::select(var_names, everything()) %>% 
  left_join(corr_tbl, by = c("var_names" = "variables")) %>% 
  left_join(corr_cross_tbl, by = c("var_names" = "key")) %>% 
  setNames(c("var_names", "Estimate", "t_stats", "p_value", "corr", "label", "cross_corr")) %>% 
  arrange(cross_corr)
```


```{r}
# Multicollinearity suspect: Promising variables (Severe)
summary_relationship_tbl %>% 
  filter(str_detect(label, pattern = "srlstat"))

summary_relationship_tbl %>% 
  filter(str_detect(label, pattern = "lrm"))

# No sign of significant collinearity :) 
summary_relationship_tbl %>% 
  filter(str_detect(label, pattern = "ltax"))
```

Multicollinearity suspect: High entanglment in multiple dimension (variable removal candidate)

indus variable is the most problematic 

```{r}
summary_relationship_tbl %>% 
  filter(str_detect(label, pattern = "indus"))

summary_relationship_tbl %>% 
  filter(str_detect(label, pattern = "age"))

summary_relationship_tbl %>% 
  filter(str_detect(label, pattern = "nox"))
```

The regsubsets() function (part of the leaps library) performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The syntax is the same as for lm(). The summary() command outputs the best set of variables for each model size.

An asterisk ("*") indicates that a given variable is included in the corresponding model. For instance, this output indicates that the best two-variable model contains only Hits and CRBI. By default, regsubsets() only reports results up to the best eight-variable model. But the nvmax option can be used in order to return as many variables as are desired. Here we fit up to a 19-variable model:

```{r}
lm_fit_00edit <- lm(medv~., data = Boston); summary(lm_fit_00edit)

summary(lm_fit_00edit)[1:10]
summary(lm_fit_00edit)$r.squared # 0.7406427
summary(lm_fit_00edit)$adj.r.squared # 0.7337897
summary(lm_fit_00edit)$fstatistic # 108.0767

lm_fit_01edit <- lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + black + lstat +indus, data = Boston); summary(lm_fit_01edit)
# Multiple R-squared:  0.7406,	Adjusted R-squared:  0.7348 
# F-statistic: 128.2 on 11 and 494 DF,  p-value: < 2.2e-16

vif(lm_fit_01edit) %>% 
  as_tibble() %>% 
  bind_cols(names(vif(lm_fit_01edit))) %>% 
  setNames(c("vif", "var_name")) %>% 
  arrange(desc(vif))
```
http://www.science.smith.edu/~jcrouser/SDS293/labs/lab8-r.html

```{r, echo = FALSE}
generator_univar_importance_tbl <- function(dataframe, response) {
  
  if (sum(sapply(dataframe, function(x) {is.numeric(x) | is.factor(x)})) < ncol(dataframe)) {
    stop("Make sure that all variables are of class numeric/factor!")
  }
  
  # pre-allocate vectors
  varname <- c()
  vartype <- c()
  R2 <- c()
  R2_log <- c()
  R2_quad <- c()
  AIC <- c()
  AIC_log <- c()
  AIC_quad <- c()
  y <- dataframe[, response]
  # # # # # NUMERIC RESPONSE # # # # #
  if (is.numeric(y)) {
    
    for (i in 1:ncol(dataframe)) {
      
      x <- dataframe[ ,i]
      varname[i] <- names(dataframe)[i]
      
      if (class(x) %in% c("numeric", "integer")) {
        vartype[i] <- "numeric"
      } else {
        vartype[i] <- "categorical"
      }
      
      if (!identical(y, x)) {
        
        # linear: y ~ x
        R2[i] <- summary(lm(y ~ x))$r.squared 
        
        # log-transform: y ~ log(x)
        if (is.numeric(x)) { 
          if (min(x) <= 0) { # if y ~ log(x) for min(x) <= 0, do y ~ log(x + abs(min(x)) + 1)
             R2_log[i] <- summary(lm(y ~ log(x + abs(min(x)) + 1)))$r.squared
          } else {
            R2_log[i] <- summary(lm(y ~ log(x)))$r.squared
          }
        } else {
          R2_log[i] <- NA
        }
        
        # quadratic: y ~ x + x^2
        if (is.numeric(x)) { 
          R2_quad[i] <- summary(lm(y ~ x + I(x^2)))$r.squared
        } else {
          R2_quad[i] <- NA
        }
        
      } else {
        R2[i] <- NA
        R2_log[i] <- NA
        R2_quad[i] <- NA
      }
    }
    
    print(paste("Response variable:", response))
    
    data.frame(varname, 
                vartype, 
               R2 = round(R2, 3), 
               R2_log = round(R2_log, 3), 
               R2_quad = round(R2_quad, 3)) %>%
      mutate(max_R2 = pmax(R2, R2_log, R2_quad, na.rm = T)) %>%
      arrange(desc(max_R2))
    
    
    # # # # # CATEGORICAL RESPONSE # # # # #
  } else {
    
    for (i in 1:ncol(dataframe)) {
      
      x <- dataframe[ ,i]
      varname[i] <- names(dataframe)[i]
      
      if (class(x) %in% c("numeric", "integer")) {
        vartype[i] <- "numeric"
      } else {
        vartype[i] <- "categorical"
      }
      
      if (!identical(y, x)) {
         # linear: y ~ x
        AIC[i] <- summary(glm(y ~ x, family = "binomial"))$aic 
        
        # log-transform: y ~ log(x)
        if (is.numeric(x)) { 
          if (min(x) <= 0) { # if y ~ log(x) for min(x) <= 0, do y ~ log(x + abs(min(x)) + 1)
            AIC_log[i] <- summary(glm(y ~ log(x + abs(min(x)) + 1), family = "binomial"))$aic
          } else {
            AIC_log[i] <- summary(glm(y ~ log(x), family = "binomial"))$aic
          }
        } else {
          AIC_log[i] <- NA
        }
        
        # quadratic: y ~ x + x^2
        if (is.numeric(x)) { 
          AIC_quad[i] <- summary(glm(y ~ x + I(x^2), family = "binomial"))$aic
        } else {
          AIC_quad[i] <- NA
        }
        
      } else {
        AIC[i] <- NA
        AIC_log[i] <- NA
        AIC_quad[i] <- NA
      }
    }
    
    print(paste("Response variable:", response))
    
    data.frame(varname, 
               vartype, 
               AIC = round(AIC, 3), 
               AIC_log = round(AIC_log, 3), 
               AIC_quad = round(AIC_quad, 3)) %>%
      mutate(min_AIC = pmin(AIC, AIC_log, AIC_quad, na.rm = T)) %>%
      arrange(min_AIC)
  } 
}
```


```{r}
generator_univar_importance_tbl(Boston, "medv")
            
```

```{r}
summary_relationship_tbl %>% 
  filter(str_detect(label, pattern = "rad"))

regsubfit <- regsubsets(medv~.-tax,
                        data= Boston, 
                        intercept = T,
                        method = c("exhaustive", "backward", "forward", "seqrep"),
                        nvmax = 19)
reg_summary <- summary(regsubfit)
summary(regsubfit)[1:10]

par(mfrow = c(2,2))
plot(reg_summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg_summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
```


```{r}
outlier_treatment <- function(x, na.rm = T) {
  x[x > quantile(x,.95,na.rm = na.rm)]<- quantile(x, .95,na.rm = na.rm)
  x[x < quantile(x,.05,na.rm = na.rm)]<- quantile(x, .05,na.rm = na.rm)
}
```

```{r}

lm_fit_dir <- lm(medv~rm + age, data = Boston); summary(lm_fit_dir)
plot(lm_fit_dir)
boxplot(Boston$medv)
boxplot(Boston$rm)
boxplot(Boston$age)

Boston %>% 
  mutate(rm_outlierT = rm %>% outlier_treatment()) %>% 
  ggplot(aes(medv, rm)) +
  geom_point()

Boston_medv_max <- Boston %>% filter(medv >= 50)
bind_rows(sapply(Boston_medv_max, mean), sapply(Boston, mean))
# lstat, chas
```

```{r}

```

