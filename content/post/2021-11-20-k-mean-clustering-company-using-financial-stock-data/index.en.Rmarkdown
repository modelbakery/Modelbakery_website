---
title: "Stock Market Clustering"
subtitle: "k-Means Clustering: Live SP500 Stock Market Data from Yahoo Finance"
author: "Package Build"
date: '2021-11-20'
slug: K-Mean Clustering Company using Financial Stock Data
categories: R
tags:
- Academic
- Clustering
- Distance Matrix
summary: ''
authors: []
lastmod: '2021-11-20T19:35:42+09:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



# Purpose: 

B2B sales organisation wants to know which companies are similar to each other to help in identifying potential customers in various segments of the market. There are many techniques for market research but in here we will try to penetrate various market segments using companies stock price data. 

When it comes to market research or investment, looking at the returns of dozens of companies will be a tricky analysis to draw various insights. The cluster analysis process of dividing a common set of companies into groups (by range and scope) provides a more holistic perspective on the internal and external relationship of the relevant companies. Since stocks have a tendency to fluctuate together, grouping by the similarity in return trend seems an applicable clustering recipe in practice. 

# Reference:

The code workflow here is been devloped from University of Business Science 101 course & UC Business analytics R programming guide. 


# Agenda: 

#### [1] Pull Stock Price Data from Yahoo Finance using getSymbols  

#### [2] Data Preparation: Preparing data for clustering (User-Item or User-date format) + Normalisation 

#### [3] Determine Optimal Clusters: Identify the optimal number of clusters from various statistical approaches 

#### [4] Visualisation aid by UMAP: 2-Dimensional feature reduction + plotly 



$$ 
return_{daily} = \frac{price_{i}-price_{i-1}}{price_{i-1}}
$$

# The Concept of Clustering 

The task of clustering analysis is to divide the set of objects into homogeneous groups. The ideal grouping is where the arbitrary objects belonging to the same group are more similar to each other than arbitrary objects belonging different groups. 

There are two questions we must find before applying cluster analysis$^1$:

* How to define the similarity between the object - the distance metric that we use to identify the common traits carries some kind of semantic meaning of similarity?

* In what manner should one make use of the thus defined similarity in the process of grouping - does clustering provide any new insight into the data? 

### Why Would One Undergo Cluster Analysis 

One may have collected large data set at different experimental sites and resources. Data may stem from subjects related to one and another, and there are further possibilities of hidden variables, e.g. macro external influence or special events,  which in a way could expand the variance and much uninterpretable noise in the data. If one is unaware of such a hierarchical relationship or is absent in this domain of knowledge, it is advised to apply a clustering method prior to further analysis. 

For an example of its interesting application, cluster analysis on both the customers and their typical and the atypical product items helps to uncover hidden needs of the customer database and verify the hypotheses concerning these common user-item relationships: play an important role in the development of the recommendation system. 


# Methods for measuring distances 

The choice of distance matrix is a critical step in cluster. It determines how the similarity of two elements (x, y) is calculated and it will directly influence the shape of the cluster. 

#### Euclidean Distance: 

* Most commonly used and often a default setting in many distance matrix algorithm in R

$$
d_{euc}(x, y) = \sqrt{\sum_{i=1}^n(x_i-y_i)^2}
$$

#### Manhattan Distance: 

$$
d_{man}(x, y) = \sum_{i=1}^n|(x_i-y_i)|
$$

#### Correlation-based distance: 

* Pearson correlation: Measures the degree of a linear relationship between two profiles. It is a parametric correlation which depends on the distribution of the data. 

* Kendall & Spearman correlation correlation are non-parametric and they are used to perform rank-based correlation analysis. (Outlier sensitivity can be mitigated by using Spearman's correlation instead of Pearson's correlation)

* Considers two objects to be similar if their features are highly correlated, even though the observed values may be far apart in terms of Euclidean distance. 

* Correlation-based distance often preferred over two above matrix in case for dealing with high dimensional data set, due to curse of dimensionality. 

#### What type of distance measure should we choose?

Despite the choice of distance measure has a strong influence on the clustering result, it is often arbitrary. But we must note two things: 

[1] If we want to identify clusters of observations with the small overall profiles regardless of their magnitudes, we should go with __correlation-based distance__ as a dissimilarity measure. In marketing, we can imagine a case where organisation like to identify group of shoppers with the same preference in terms of items, regardless of the volume of items they bought. 

[2] If Euclidean distance is chosen, then observations with high values of features will be clustered together. The same holds true for observations with low values of features. 


# K-means Aglorithm 

Kmeans algorithm is an iterative algorithm that tries to partition the dataset into pre-defined k distinct non-overlapping subgroups (clusters) where each data point belongs to only one group. It tries to make the intra-cluster data points as similar as possible while also keeping the clusters as different (far) as possible. It assigns data points to a cluster such that the sum of the squared distance between the data points and the clusterâ€™s centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster.

#### Basic Idea 

The way that kmeans algorithm operates are as follow: 

[1] Specify the number of clusters (K) to be created.

[2] Select random k objects from the data set as the initial cluster centers.

[3] Assigns each observation to their closest centroid, based on the Euclidean distance between the object and the centroid.

[4] For each of the k clusters __update the cluster centroid__ by calculating the new mean values of all the data points in the cluster. The centroid of a $\ K_{th}$ cluster is a vector of length p containing the means of all variables for the observations in the $\ K_{th}$ cluster; p is the number of variables.

[5] Iteratively minimize the total within sum of squares. That is, iterate steps 3 and 4 until the cluster assignments stop changing or the maximum number of iterations is reached. Hence, there is a tendency of retaining reliability of the accuracy with greater number of iteration. 


The approach kmeans follows to solve the problem is called Expectation-Maximization. The E-step is assigning the data points to the closest cluster. The M-step is computing the centroid of each cluster. Below is a break down of how we can solve it mathematically (feel free to skip it).


 
 
Sum of the squares deviation from the mean or the centroid. 

$$
\sum_{i=1}^n(x_i-\overline{x})^2 \\

\sum_{i=1}^n|(x_i-\overline{x})| \\


\frac{SS_B}{df_B}   \frac{SS_W}{df_W}
$$

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidyquant)
library(quantmod)
library(broom)
library(umap)
library(plotly)
library(factoextra)
library(cluster)
library(kableExtra)

```


```{r}
get_stock_list <- 
  function(stock_index = "SP500") {
        tq_index(stock_index) %>%
        select(symbol, company, sector) %>%
        arrange(symbol)
  }

get_stock <- 
  function(stock_symbol,
           from = today() - lubridate::days(730),
           to = today()){
  stock_symbol %>%
    tq_get(get = "stock.prices", from = from, to = to) 
  }

```


```{r}
SP500_prices_tbl <- read_rds("/Users/seunghyunsung/Documents/GitHub/Modelbakery_backup/post3/sp500stock/sp_500_prices_tbl_2019.11_2021.11.rds")

SP500_index_tbl <- read_rds("/Users/seunghyunsung/Documents/GitHub/Modelbakery_backup/post3/sp500stock/SP500_index_list_tbl.rds")

```

```{r}
SP500_prices_tbl %>% head(20) %>% kbl() %>% kable_material()
```

```{r}
SP500_index_tbl %>% head(20) %>% kbl() %>% kable_material()
```


```{r}
SP500_daily_returns_tbl <- SP500_prices_tbl %>% 
    select(symbol, date, adjusted) %>% 
    group_by(symbol) %>% 
    mutate(lag = lag(adjusted, order_by = date, n = 1)) %>% 
    filter(!is.na(lag)) %>% 
    mutate(diff = adjusted - lag) %>% 
    mutate(pct_return = diff/lag)  %>% ungroup() %>% 
    select(symbol, date, pct_return) 
```


## Convert to User-Item Format: here it would be Company-Return Format

```{r}
SP500_date_matrix_tbl <- SP500_daily_returns_tbl %>% 
  spread(date, pct_return, fill = 0)
```

```{r}
distance <- get_dist(SP500_date_matrix_tbl %>% select(-symbol))
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```


```{r}
set.seed(42)

kmeans_obj_mapper <- function(center = 1){
   SP500_date_matrix_tbl %>% 
    select(-symbol) %>% 
    kmeans(centers =center, nstart = 30)
}

kmeans_overview <- tibble(center = 1:10) %>% 
  mutate(
    kmeans_obj_map = center %>% map(kmeans_obj_mapper),
    glance = kmeans_obj_map %>% map(broom::glance)
    )

kmeans_overview %>% unnest(glance) %>% 
  mutate(
    perc_tot.withiness = 
      (tot.withinss/totss) %>% scales::percent()
    ) %>% 
  select(-kmeans_obj_map) %>% 
  kbl() %>% kable_material()
```


## Step 4 - Find the optimal value of K


#### Direct Methods: Consists of optimizing a criterion, such as the within  cluster sums of squares or the avergae silhouette.


#### Elbow Method

There are several k-means algorithms available. The basic idea behind the Elbow method aims to to minimize the Euclidean distances of all points with their nearest cluster centers, by minimizing total intra-cluster variation (within-cluster sum of squares, WSS).

$$
Total\ Variation = Variation\ Within + Variation\ Between \\
$$
$$
TSS = WSS + BSS\      where\  SS\ = Sum\ of\ Squares  
$$

```{r, warning=FALSE}
set.seed(42)
kmeans_mapper <- function(center = 3) {
    SP500_date_matrix_tbl %>%
        select(-symbol) %>%
        kmeans(centers = center, nstart = 20)
}

cluster_tbl <- tibble(centers = 1:20)

k_means_mapped_tbl <- cluster_tbl %>% 
  mutate(k_means = centers %>% map(kmeans_mapper),
         glance  = k_means %>% map(glance))

```

The Elbow method looks at the total within sum of squares as a function of the number of clusters. With ever increasing the number of clusters (max at number of observations) the WSS will continuously descend in a linear manner. One should choose a number of clusters one before constant rate of WSS change is reached, that is adding another cluster does not significantly influence total WSS. 

To note, the elbow method is sometimes ambiguous. In our SP500 return data set, it is difficult to spot optimal cluster k, with our naked eye. Hence, I got the pulse of local region 5~10 to investigate further using alternative clustering approach. 

```{r}
k_means_mapped_tbl %>% 
  unnest(glance) %>% 
  ggplot(aes(x = centers, y = tot.withinss)) +
  geom_point(colour = palette_light()[1], size = 3) +
  geom_line(colour = palette_light()[1], size = 1) +
  ggrepel::geom_label_repel(aes(label = centers, alpha = 0.7), colour = palette_light()[1]) +
  theme_tq() +
  labs(
        x = "Number of Clusters K",
        y = "Total Within Sum of Square",
        title = "Scree Plot",
        subtitle = "Purpoe: Minimise difference in distance between the centers of clusters and each of the companies.
Scree Plot determines the optimal number of cluster for K-means",
        caption = "Scree Plot becomes linear (constant rate of change) between 5 and 10 centers for K; 
        hence we select 5~10 clusters to segement the customer base."
    ) +
  theme(legend.position = "none")

```

#### Average Silhouette Method

* Measures the quality of a clustering. It determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering. 

```{r}
# function to compute average silhouette for k clusters
silhouette_mapper <- function(k) {
    df <- SP500_date_matrix_tbl %>% select(-symbol)
    
    k_means <- df %>%
        kmeans(centers = k, nstart = 20)
    
    ss <- silhouette(k_means$cluster, dist(df))
    if(k == 1){
      return(NA) 
    } else{
    mean(ss[, 3])
    }
}

k_means_silhouette_mapped_tbl <- cluster_tbl %>% 
  mutate(k_means = centers %>% map(silhouette_mapper)) %>% unnest(k_means)
```

#### [2] Silhouette Method: Visualisation

```{r}
k_means_silhouette_mapped_tbl[-1,] %>% 
  ggplot(aes(centers, k_means)) +
  geom_point(colour = palette_light()[1], size = 3) +
  geom_line(colour = palette_light()[1], size = 1) +
  ggrepel::geom_label_repel(aes(label = centers, alpha = 0.7), colour = palette_light()[1]) +
  theme_tq() +
  labs(
        x = "Number of Clusters K",
        y = "Average Silhouette Width",
        title = "Scree Plot",
        subtitle = "Purpoe: Minimise difference in distance between the centers of clusters and each of the companies.
Scree Plot determines the optimal number of cluster for K-means",
        caption = "Scree Plot becomes linear (constant rate of change) between 5 and 10 centers for K; 
        hence we select 5~10 clusters to segement the customer base."
    ) +
  theme(legend.position = "none")
```


#### Statistical Testing Methods: consists of comparing evidence against null hypothesis. An example is the gap analysis. 

#### Gap Statistic Method

The gap analysis compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximise the gap statistics. This means that the clustering strucutre is far away from the random uniform distribution of points. 


#### Conclusion:

* In order to retain the hierachical relationship with industry sector 


## Visualising k-means clusters 

#### Use UMAP for 2-D Projection 

Once K-Means Clustering is performed, we can use UMPA (or PCA) to help us visualise each data point according to its cluster assignment. The problem that these observations (companies) are high dimension in space can be solved by applying a dimensionality reduction algorithm to output two most influencial variables respect to the originial variables. 

```{r, warning=FALSE}
umap_obj <- SP500_date_matrix_tbl %>% 
    select(-symbol) %>% 
    umap()

umap_result_tbl <- umap_obj$layout %>% 
    as_tibble() %>% 
    setNames(c("x", "y")) %>% 
    bind_cols(SP500_date_matrix_tbl %>% select(symbol))
```


```{r, warning=FALSE}
# Get the k_means_obj from the 6th center
k_means_obj <- k_means_mapped_tbl %>% 
    filter(centers == 6) %>% 
    pull(k_means) %>%  pluck(1) 

umap_kmeans_SP500_result_tbl <- k_means_obj %>% 
    broom::augment(SP500_date_matrix_tbl %>% select(symbol)) %>%
    left_join(umap_result_tbl, by = "symbol") %>% 
    left_join(SP500_index_tbl %>% select(symbol, company, sector), by = "symbol")

```


```{r, warning=FALSE}
indus_mapper <- function(cluster =1){
  foo <- umap_kmeans_SP500_result_tbl %>% 
  select(-c(x,y)) %>% 
  filter(.cluster == cluster) %>% 
  count(sector) %>% 
  mutate(indus_prop = n/sum(n),
         indus_prop_text = indus_prop %>% scales::percent()) %>%
  arrange(desc(indus_prop)) %>% 
  select(sector, indus_prop, indus_prop_text) %>% head(3) %>% 
  mutate(sector_prop = str_c(sector, str_glue("({indus_prop_text})"))) 
  if(is.na(foo[2,]) == TRUE) {
  foo %>% mutate(sector_text = str_glue("{sector_prop[1]}")) %>% 
  select(sector_text) %>% distinct(sector_text) 
  } else if(is.na(foo[3,]) == TRUE) {
  foo %>% mutate(sector_text = str_glue("{sector_prop[1]}
                                {sector_prop[2]}")) %>% 
  select(sector_text) %>% distinct(sector_text)   
  } else{
   foo %>% mutate(sector_text = str_glue("{sector_prop[1]}
                                {sector_prop[2]}
                                {sector_prop[3]}")) %>% 
  select(sector_text) %>% distinct(sector_text)   
  }
}

cluster_indus <-  tibble(.cluster = 1:6)

industry_text_tbl <- cluster_indus %>% 
  mutate(indus = .cluster %>% map(indus_mapper)) %>% unnest(indus) %>% 
  mutate(.cluster = as.factor(.cluster))

```


```{r}
umap_kmeans_SP500_result_tbl %>% 
  left_join(industry_text_tbl, by = ".cluster") %>% 
    ggplot(aes(x, y, colour = .cluster)) +
    geom_point(alpha = 0.5) +
    ggrepel::geom_label_repel(data  = . %>% 
                              mutate(label = ifelse(symbol %in% c("APA", "AEE", "VTRS", "DFS", "TER", "DAL"), sector_text, "")),
                                     aes(label = label), 
                              size = 3,
                              min.segment.length =  1,
                              max.overlaps = getOption("ggrepel.max.overlaps", default = 100), show.legend = FALSE, alpha = 0.7) + 
    theme_tq() +
    scale_colour_tq() 
```


```{r}
get_kmeans <- function(k = 3) {
    
    k_means_obj <- k_means_mapped_tbl %>%
        filter(centers == k) %>%
        pull(k_means) %>%
        pluck(1)
    
    umap_kmeans_results_tbl <- k_means_obj %>% 
        augment(SP500_date_matrix_tbl) %>%
        select(symbol, .cluster) %>%
    left_join(umap_result_tbl, by = "symbol") %>% 
    left_join(SP500_index_tbl %>% select(symbol, company, sector), by = "symbol")
    
    return(umap_kmeans_results_tbl)
}

plot_cluster <- function(k = 3) {
    
    g <- get_kmeans(k) %>%
        
        mutate(label_text = str_glue("Stock: {symbol}
                                     Company: {company}
                                     Sector: {sector}")) %>%
        
        ggplot(aes(x, y, color = .cluster, text = label_text)) +
        geom_point(alpha = 0.5) +
        theme_tq() +
        scale_color_tq()
    
    g %>%
        ggplotly(tooltip = "text")
    
}
```

We can plot the clusters interactively. 

```{r}
plot_cluster(k = 6)
```






