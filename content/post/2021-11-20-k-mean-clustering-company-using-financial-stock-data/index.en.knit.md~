---
title: "Stock Market Clustering"
subtitle: "k-Means Clustering: Live SP500 Stock Market Data from Yahoo Finance"
author: "Package Build"
date: '2021-11-20'
slug: K-Mean Clustering Company using Financial Stock Data
categories: R
tags:
- Academic
- Clustering
- Distance Matrix
summary: ''
authors: []
lastmod: '2021-11-20T19:35:42+09:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



# Purpose: 

B2B sales organisation wants to know which companies are similar to each other to help in identifying potential customers in various segments of the market. There are many techniques for market research but in here we will try to penetrate various market segments using companies stock price data. 

When it comes to market research or investment, looking at the returns of dozens of companies will be a tricky analysis to draw various insights. The cluster analysis process of dividing a common set of companies into groups (by range and scope) provides a more holistic perspective on the internal and external relationship of the relevant companies. Since stocks have a tendency to fluctuate together, grouping by the similarity in return trend seems an applicable clustering recipe in practice. 

# Reference:

The code workflow here is been devloped from University of Business Science 101 course & UC Business analytics R programming guide. 


# Agenda: 

#### [1] Pull Stock Price Data from Yahoo Finance using getSymbols  

#### [2] Data Preparation: Preparing data for clustering (User-Item or User-date format) + Normalisation 

#### [3] Determine Optimal Clusters: Identify the optimal number of clusters from various statistical approaches 

#### [4] Visualisation aid by UMAP: 2-Dimensional feature reduction + plotly 



$$ 
return_{daily} = \frac{price_{i}-price_{i-1}}{price_{i-1}}
$$

# The Concept of Clustering 

The task of clustering analysis is to divide the set of objects into homogeneous groups. The ideal grouping is where the arbitrary objects belonging to the same group are more similar to each other than arbitrary objects belonging different groups. 

There are two questions we must find before applying cluster analysis$^1$:

* How to define the similarity between the object - the distance metric that we use to identify the common traits carries some kind of semantic meaning of similarity?

* In what manner should one make use of the thus defined similarity in the process of grouping - does clustering provide any new insight into the data? 

### Why Would One Undergo Cluster Analysis 

One may have collected large data set at different experimental sites and resources. Data may stem from subjects related to one and another, and there are further possibilities of hidden variables, e.g. macro external influence or special events,  which in a way could expand the variance and much uninterpretable noise in the data. If one is unaware of such a hierarchical relationship or is absent in this domain of knowledge, it is advised to apply a clustering method prior to further analysis. 

For an example of its interesting application, cluster analysis on both the customers and their typical and the atypical product items helps to uncover hidden needs of the customer database and verify the hypotheses concerning these common user-item relationships: play an important role in the development of the recommendation system. 


# Methods for measuring distances 

The choice of distance matrix is a critical step in cluster. It determines how the similarity of two elements (x, y) is calculated and it will directly influence the shape of the cluster. 

#### Euclidean Distance: 

* Most commonly used and often a default setting in many distance matrix algorithm in R

$$
d_{euc}(x, y) = \sqrt{\sum_{i=1}^n(x_i-y_i)^2}
$$

#### Manhattan Distance: 

$$
d_{man}(x, y) = \sum_{i=1}^n|(x_i-y_i)|
$$

#### Correlation-based distance: 

* Pearson correlation: Measures the degree of a linear relationship between two profiles. It is a parametric correlation which depends on the distribution of the data. 

* Kendall & Spearman correlation correlation are non-parametric and they are used to perform rank-based correlation analysis. (Outlier sensitivity can be mitigated by using Spearman's correlation instead of Pearson's correlation)

* Considers two objects to be similar if their features are highly correlated, even though the observed values may be far apart in terms of Euclidean distance. 

* Correlation-based distance often preferred over two above matrix in case for dealing with high dimensional data set, due to curse of dimensionality. 

#### What type of distance measure should we choose?

Despite the choice of distance measure has a strong influence on the clustering result, it is often arbitrary. But we must note two things: 

[1] If we want to identify clusters of observations with the small overall profiles regardless of their magnitudes, we should go with __correlation-based distance__ as a dissimilarity measure. In marketing, we can imagine a case where organisation like to identify group of shoppers with the same preference in terms of items, regardless of the volume of items they bought. 

[2] If Euclidean distance is chosen, then observations with high values of features will be clustered together. The same holds true for observations with low values of features. 


# K-means Aglorithm 

Kmeans algorithm is an iterative algorithm that tries to partition the dataset into pre-defined k distinct non-overlapping subgroups (clusters) where each data point belongs to only one group. It tries to make the intra-cluster data points as similar as possible while also keeping the clusters as different (far) as possible. It assigns data points to a cluster such that the sum of the squared distance between the data points and the cluster¡¯s centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster.

#### Basic Idea 

The way that kmeans algorithm operates are as follow: 

[1] Specify the number of clusters (K) to be created.

[2] Select random k objects from the data set as the initial cluster centers.

[3] Assigns each observation to their closest centroid, based on the Euclidean distance between the object and the centroid.

[4] For each of the k clusters __update the cluster centroid__ by calculating the new mean values of all the data points in the cluster. The centroid of a $\ K_{th}$ cluster is a vector of length p containing the means of all variables for the observations in the $\ K_{th}$ cluster; p is the number of variables.

[5] Iteratively minimize the total within sum of squares. That is, iterate steps 3 and 4 until the cluster assignments stop changing or the maximum number of iterations is reached. Hence, there is a tendency of retaining reliability of the accuracy with greater number of iteration. 


The approach kmeans follows to solve the problem is called Expectation-Maximization. The E-step is assigning the data points to the closest cluster. The M-step is computing the centroid of each cluster. Below is a break down of how we can solve it mathematically (feel free to skip it).


 
 
Sum of the squares deviation from the mean or the centroid. 

$$
\sum_{i=1}^n(x_i-\overline{x})^2 \\

\sum_{i=1}^n|(x_i-\overline{x})| \\


\frac{SS_B}{df_B}   \frac{SS_W}{df_W}
$$


```r
library(tidyverse)
library(tidyquant)
library(quantmod)
library(broom)
library(umap)
library(plotly)
library(factoextra)
library(cluster)
library(kableExtra)
```



```r
get_stock_list <- 
  function(stock_index = "SP500") {
        tq_index(stock_index) %>%
        select(symbol, company, sector) %>%
        arrange(symbol)
  }

get_stock <- 
  function(stock_symbol,
           from = today() - lubridate::days(730),
           to = today()){
  stock_symbol %>%
    tq_get(get = "stock.prices", from = from, to = to) 
  }
```




































